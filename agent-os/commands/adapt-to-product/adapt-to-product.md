You are helping to adapt an existing codebase into product documentation by analyzing the codebase and gathering product information from multiple sources.  This will include:

- **Setup and Information Gathering**: Create inheritance folder, check existing product folder, gather information from user, documents, links, web search (optional), and codebase traversal
- **Codebase Analysis**: Analyze existing codebase structure, dependencies, documentation, and code organization
- **Mission Document**: Take all gathered information and create a concise mission document
- **Roadmap**: Create a phased development plan with prioritized features
- **Tech stack**: Establish the technical stack used for all aspects of this product's codebase
- **Review and Combine**: Analyze all product documents together, verify consistency, and create summary
- **Product-Focused Cleanup**: Simplify and enhance agent-os files based on detected product scope
- **Navigate to Next**: Provide summary of accomplishments and guide to next command

Carefully read and execute the instructions in the following files IN SEQUENCE, following their numbered file names.  Only proceed to the next numbered instruction file once the previous numbered instruction has been executed.

Instructions to follow in sequence:

# PHASE 1: Setup And Information Gathering

This begins a multi-step process for adapting an existing codebase into product documentation.

## Step 0: Automatic Project Detection & Research (NEW)

Before gathering product information, automatically detect project characteristics and enrich with web research:

```bash
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  ADAPTIVE PROJECT DETECTION"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
```

### 0.1: Run Project Detection

Automatically analyze the codebase to detect tech stack, commands, architecture, and security level:

```bash
# Workflow: Detect Project Profile

## Purpose

Main orchestrator that calls all detection functions, aggregates results into a unified project profile, calculates overall confidence score, and handles fallbacks for failed detections.

## Inputs

- Project root directory (current working directory)
- Optional: Existing `agent-os/config/project-profile.yml` (for incremental updates)

## Outputs

- `agent-os/config/project-profile.yml` - Unified project profile
- `agent-os/config/detected-profile.yml` - Raw detection results (cache)

---

## Workflow

### Step 1: Initialize Detection Environment

```bash
echo "ðŸ” Starting project detection..."

# Initialize variables
DETECTION_CONFIDENCE=0
DETECTIONS_RUN=0
DETECTIONS_SUCCESS=0

# Create config directory if needed
mkdir -p agent-os/config

# Initialize raw detection results
cat > agent-os/config/detected-profile.yml << 'INIT_EOF'
# Auto-detected project profile
# Generated by detect-project-profile workflow

detected_at: $(date -Iseconds)
INIT_EOF
```

### Step 2: Run Detection Workflows

Execute each detection workflow in sequence:

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  PROJECT DETECTION"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Track detection results
declare -A DETECTION_RESULTS
declare -A DETECTION_CONFIDENCE

# 1. Detect Tech Stack
echo "ðŸ”§ Detecting tech stack..."
# Workflow: Detect Tech Stack

## Purpose

Parse project configuration files to detect languages, frameworks, and databases. Supports Node.js, Rust, Go, and Python projects.

## Outputs

Sets the following variables:
- `DETECTED_LANGUAGE` - Primary language (javascript, typescript, rust, python, go)
- `DETECTED_FRAMEWORK` - Web framework if detected (react, vue, express, fastapi, etc.)
- `DETECTED_BACKEND` - Backend runtime (nodejs, rust, python, go)
- `DETECTED_DATABASE` - Database if detected (postgresql, mongodb, mysql, redis)
- `TECH_STACK_CONFIDENCE` - Confidence score (0.0 - 1.0)

---

## Detection Logic

### Step 1: Detect from package.json (Node.js/JavaScript/TypeScript)

```bash
if [ -f "package.json" ]; then
    echo "   Found package.json - analyzing..."
    
    # Base language
    DETECTED_LANGUAGE="javascript"
    DETECTED_BACKEND="nodejs"
    
    # Check for TypeScript
    if [ -f "tsconfig.json" ] || grep -q '"typescript"' package.json 2>/dev/null; then
        DETECTED_LANGUAGE="typescript"
    fi
    
    # Detect frameworks from dependencies
    DEPS=$(cat package.json)
    
    # Frontend frameworks
    if echo "$DEPS" | grep -q '"react"'; then
        DETECTED_FRAMEWORK="react"
    elif echo "$DEPS" | grep -q '"vue"'; then
        DETECTED_FRAMEWORK="vue"
    elif echo "$DEPS" | grep -q '"angular"'; then
        DETECTED_FRAMEWORK="angular"
    elif echo "$DEPS" | grep -q '"svelte"'; then
        DETECTED_FRAMEWORK="svelte"
    fi
    
    # Meta-frameworks (override if found)
    if echo "$DEPS" | grep -q '"next"'; then
        DETECTED_FRAMEWORK="nextjs"
    elif echo "$DEPS" | grep -q '"nuxt"'; then
        DETECTED_FRAMEWORK="nuxt"
    elif echo "$DEPS" | grep -q '"remix"'; then
        DETECTED_FRAMEWORK="remix"
    fi
    
    # Backend frameworks
    if echo "$DEPS" | grep -q '"express"'; then
        DETECTED_BACKEND="nodejs-express"
    elif echo "$DEPS" | grep -q '"fastify"'; then
        DETECTED_BACKEND="nodejs-fastify"
    elif echo "$DEPS" | grep -q '"koa"'; then
        DETECTED_BACKEND="nodejs-koa"
    elif echo "$DEPS" | grep -q '"hono"'; then
        DETECTED_BACKEND="nodejs-hono"
    fi
    
    # Database detection
    if echo "$DEPS" | grep -qE '"(pg|postgres|postgresql)"'; then
        DETECTED_DATABASE="postgresql"
    elif echo "$DEPS" | grep -qE '"(mysql|mysql2)"'; then
        DETECTED_DATABASE="mysql"
    elif echo "$DEPS" | grep -qE '"(mongodb|mongoose)"'; then
        DETECTED_DATABASE="mongodb"
    elif echo "$DEPS" | grep -q '"redis"'; then
        DETECTED_DATABASE="redis"
    elif echo "$DEPS" | grep -qE '"(prisma|@prisma)"'; then
        # Prisma - check schema for db type
        if [ -f "prisma/schema.prisma" ]; then
            if grep -q 'provider = "postgresql"' prisma/schema.prisma 2>/dev/null; then
                DETECTED_DATABASE="postgresql"
            elif grep -q 'provider = "mysql"' prisma/schema.prisma 2>/dev/null; then
                DETECTED_DATABASE="mysql"
            elif grep -q 'provider = "mongodb"' prisma/schema.prisma 2>/dev/null; then
                DETECTED_DATABASE="mongodb"
            fi
        fi
    fi
    
    TECH_STACK_CONFIDENCE="0.95"
fi
```

### Step 2: Detect from Cargo.toml (Rust)

```bash
if [ -f "Cargo.toml" ]; then
    echo "   Found Cargo.toml - analyzing..."
    
    DETECTED_LANGUAGE="rust"
    DETECTED_BACKEND="rust"
    
    CARGO=$(cat Cargo.toml)
    
    # Web frameworks
    if echo "$CARGO" | grep -q 'actix-web'; then
        DETECTED_FRAMEWORK="actix-web"
    elif echo "$CARGO" | grep -q 'axum'; then
        DETECTED_FRAMEWORK="axum"
    elif echo "$CARGO" | grep -q 'rocket'; then
        DETECTED_FRAMEWORK="rocket"
    elif echo "$CARGO" | grep -q 'warp'; then
        DETECTED_FRAMEWORK="warp"
    fi
    
    # Database
    if echo "$CARGO" | grep -qE '(sqlx|postgres)'; then
        DETECTED_DATABASE="postgresql"
    elif echo "$CARGO" | grep -q 'mysql'; then
        DETECTED_DATABASE="mysql"
    elif echo "$CARGO" | grep -q 'mongodb'; then
        DETECTED_DATABASE="mongodb"
    fi
    
    TECH_STACK_CONFIDENCE="0.95"
fi
```

### Step 3: Detect from go.mod (Go)

```bash
if [ -f "go.mod" ]; then
    echo "   Found go.mod - analyzing..."
    
    DETECTED_LANGUAGE="go"
    DETECTED_BACKEND="go"
    
    GOMOD=$(cat go.mod)
    
    # Web frameworks
    if echo "$GOMOD" | grep -q 'github.com/gin-gonic/gin'; then
        DETECTED_FRAMEWORK="gin"
    elif echo "$GOMOD" | grep -q 'github.com/labstack/echo'; then
        DETECTED_FRAMEWORK="echo"
    elif echo "$GOMOD" | grep -q 'github.com/gofiber/fiber'; then
        DETECTED_FRAMEWORK="fiber"
    elif echo "$GOMOD" | grep -q 'github.com/gorilla/mux'; then
        DETECTED_FRAMEWORK="gorilla-mux"
    fi
    
    # Database
    if echo "$GOMOD" | grep -qE '(lib/pq|jackc/pgx)'; then
        DETECTED_DATABASE="postgresql"
    elif echo "$GOMOD" | grep -q 'go-sql-driver/mysql'; then
        DETECTED_DATABASE="mysql"
    elif echo "$GOMOD" | grep -q 'mongo-driver'; then
        DETECTED_DATABASE="mongodb"
    fi
    
    TECH_STACK_CONFIDENCE="0.95"
fi
```

### Step 4: Detect from Python files

```bash
if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ] || [ -f "setup.py" ]; then
    echo "   Found Python project files - analyzing..."
    
    DETECTED_LANGUAGE="python"
    DETECTED_BACKEND="python"
    
    # Combine all dependency sources
    PYDEPS=""
    [ -f "requirements.txt" ] && PYDEPS="$PYDEPS $(cat requirements.txt)"
    [ -f "pyproject.toml" ] && PYDEPS="$PYDEPS $(cat pyproject.toml)"
    [ -f "setup.py" ] && PYDEPS="$PYDEPS $(cat setup.py)"
    
    # Web frameworks
    if echo "$PYDEPS" | grep -qi 'fastapi'; then
        DETECTED_FRAMEWORK="fastapi"
    elif echo "$PYDEPS" | grep -qi 'django'; then
        DETECTED_FRAMEWORK="django"
    elif echo "$PYDEPS" | grep -qi 'flask'; then
        DETECTED_FRAMEWORK="flask"
    elif echo "$PYDEPS" | grep -qi 'starlette'; then
        DETECTED_FRAMEWORK="starlette"
    fi
    
    # Database
    if echo "$PYDEPS" | grep -qiE '(psycopg|asyncpg|postgres)'; then
        DETECTED_DATABASE="postgresql"
    elif echo "$PYDEPS" | grep -qi 'mysql'; then
        DETECTED_DATABASE="mysql"
    elif echo "$PYDEPS" | grep -qiE '(pymongo|motor)'; then
        DETECTED_DATABASE="mongodb"
    elif echo "$PYDEPS" | grep -qi 'redis'; then
        DETECTED_DATABASE="redis"
    fi
    
    TECH_STACK_CONFIDENCE="0.90"
fi
```

### Step 5: Fallback - Detect from file extensions

```bash
if [ -z "$DETECTED_LANGUAGE" ]; then
    echo "   No config files found - detecting from file extensions..."
    
    # Count files by extension
    TS_COUNT=$(find . -name "*.ts" -o -name "*.tsx" 2>/dev/null | wc -l)
    JS_COUNT=$(find . -name "*.js" -o -name "*.jsx" 2>/dev/null | wc -l)
    RS_COUNT=$(find . -name "*.rs" 2>/dev/null | wc -l)
    GO_COUNT=$(find . -name "*.go" 2>/dev/null | wc -l)
    PY_COUNT=$(find . -name "*.py" 2>/dev/null | wc -l)
    
    # Pick the most common
    MAX_COUNT=0
    if [ "$TS_COUNT" -gt "$MAX_COUNT" ] 2>/dev/null; then
        MAX_COUNT=$TS_COUNT
        DETECTED_LANGUAGE="typescript"
        DETECTED_BACKEND="nodejs"
    fi
    if [ "$JS_COUNT" -gt "$MAX_COUNT" ] 2>/dev/null; then
        MAX_COUNT=$JS_COUNT
        DETECTED_LANGUAGE="javascript"
        DETECTED_BACKEND="nodejs"
    fi
    if [ "$RS_COUNT" -gt "$MAX_COUNT" ] 2>/dev/null; then
        MAX_COUNT=$RS_COUNT
        DETECTED_LANGUAGE="rust"
        DETECTED_BACKEND="rust"
    fi
    if [ "$GO_COUNT" -gt "$MAX_COUNT" ] 2>/dev/null; then
        MAX_COUNT=$GO_COUNT
        DETECTED_LANGUAGE="go"
        DETECTED_BACKEND="go"
    fi
    if [ "$PY_COUNT" -gt "$MAX_COUNT" ] 2>/dev/null; then
        MAX_COUNT=$PY_COUNT
        DETECTED_LANGUAGE="python"
        DETECTED_BACKEND="python"
    fi
    
    TECH_STACK_CONFIDENCE="0.60"
fi
```

---

## Important Constraints

- Must not fail if config files are missing
- Must handle malformed JSON/TOML gracefully
- Should detect the MOST SPECIFIC framework possible
- Confidence score reflects detection certainty

DETECTIONS_RUN=$((DETECTIONS_RUN + 1))
if [ -n "$DETECTED_LANGUAGE" ]; then
    DETECTIONS_SUCCESS=$((DETECTIONS_SUCCESS + 1))
    echo "   âœ“ Language: $DETECTED_LANGUAGE"
    [ -n "$DETECTED_FRAMEWORK" ] && echo "   âœ“ Framework: $DETECTED_FRAMEWORK"
    [ -n "$DETECTED_DATABASE" ] && echo "   âœ“ Database: $DETECTED_DATABASE"
fi

# 2. Detect Commands
echo "âš™ï¸  Detecting build/test/lint commands..."
# Workflow: Detect Commands

## Purpose

Extract build, test, and lint commands from project configuration files (package.json scripts, Makefile targets, CI configs). Falls back to language-specific defaults when not found.

## Outputs

Sets the following variables:
- `DETECTED_BUILD_CMD` - Build/compile command
- `DETECTED_TEST_CMD` - Test command
- `DETECTED_LINT_CMD` - Lint/static analysis command
- `DETECTED_TYPECHECK_CMD` - Type checking command (if applicable)
- `COMMANDS_CONFIDENCE` - Confidence score (0.0 - 1.0)

---

## Detection Logic

### Step 1: Detect from package.json scripts

```bash
if [ -f "package.json" ]; then
    echo "   Checking package.json scripts..."
    
    # Extract scripts section
    SCRIPTS=$(cat package.json | grep -A 100 '"scripts"' | grep -B 100 -m 1 '^  }' 2>/dev/null || cat package.json)
    
    # Build command
    if echo "$SCRIPTS" | grep -q '"build"'; then
        DETECTED_BUILD_CMD="npm run build"
    elif echo "$SCRIPTS" | grep -q '"compile"'; then
        DETECTED_BUILD_CMD="npm run compile"
    fi
    
    # Test command
    if echo "$SCRIPTS" | grep -q '"test"'; then
        DETECTED_TEST_CMD="npm test"
    elif echo "$SCRIPTS" | grep -q '"test:unit"'; then
        DETECTED_TEST_CMD="npm run test:unit"
    fi
    
    # Lint command
    if echo "$SCRIPTS" | grep -q '"lint"'; then
        DETECTED_LINT_CMD="npm run lint"
    elif echo "$SCRIPTS" | grep -q '"eslint"'; then
        DETECTED_LINT_CMD="npm run eslint"
    fi
    
    # Type check command
    if echo "$SCRIPTS" | grep -q '"typecheck"'; then
        DETECTED_TYPECHECK_CMD="npm run typecheck"
    elif echo "$SCRIPTS" | grep -q '"type-check"'; then
        DETECTED_TYPECHECK_CMD="npm run type-check"
    elif [ -f "tsconfig.json" ]; then
        DETECTED_TYPECHECK_CMD="tsc --noEmit"
    fi
    
    COMMANDS_CONFIDENCE="0.95"
fi
```

### Step 2: Detect from Makefile

```bash
if [ -f "Makefile" ]; then
    echo "   Checking Makefile targets..."
    
    # Build target
    if grep -q "^build:" Makefile; then
        [ -z "$DETECTED_BUILD_CMD" ] && DETECTED_BUILD_CMD="make build"
    fi
    
    # Test target
    if grep -q "^test:" Makefile; then
        [ -z "$DETECTED_TEST_CMD" ] && DETECTED_TEST_CMD="make test"
    fi
    
    # Lint target
    if grep -q "^lint:" Makefile; then
        [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="make lint"
    fi
    
    # Check target (often type-check or full validation)
    if grep -q "^check:" Makefile; then
        [ -z "$DETECTED_TYPECHECK_CMD" ] && DETECTED_TYPECHECK_CMD="make check"
    fi
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.90"
fi
```

### Step 3: Detect from CI config files

```bash
# GitHub Actions
if [ -d ".github/workflows" ]; then
    echo "   Checking GitHub Actions workflows..."
    
    for workflow in .github/workflows/*.yml .github/workflows/*.yaml; do
        [ -f "$workflow" ] || continue
        
        WORKFLOW_CONTENT=$(cat "$workflow" 2>/dev/null)
        
        # Look for common CI steps
        if echo "$WORKFLOW_CONTENT" | grep -q "npm run build" && [ -z "$DETECTED_BUILD_CMD" ]; then
            DETECTED_BUILD_CMD="npm run build"
        fi
        if echo "$WORKFLOW_CONTENT" | grep -q "npm test" && [ -z "$DETECTED_TEST_CMD" ]; then
            DETECTED_TEST_CMD="npm test"
        fi
        if echo "$WORKFLOW_CONTENT" | grep -q "cargo test" && [ -z "$DETECTED_TEST_CMD" ]; then
            DETECTED_TEST_CMD="cargo test"
        fi
        if echo "$WORKFLOW_CONTENT" | grep -q "pytest" && [ -z "$DETECTED_TEST_CMD" ]; then
            DETECTED_TEST_CMD="pytest"
        fi
        if echo "$WORKFLOW_CONTENT" | grep -q "go test" && [ -z "$DETECTED_TEST_CMD" ]; then
            DETECTED_TEST_CMD="go test ./..."
        fi
    done
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.80"
fi

# GitLab CI
if [ -f ".gitlab-ci.yml" ]; then
    echo "   Checking GitLab CI config..."
    
    GITLAB_CI=$(cat .gitlab-ci.yml 2>/dev/null)
    
    if echo "$GITLAB_CI" | grep -q "npm run build" && [ -z "$DETECTED_BUILD_CMD" ]; then
        DETECTED_BUILD_CMD="npm run build"
    fi
    if echo "$GITLAB_CI" | grep -q "npm test" && [ -z "$DETECTED_TEST_CMD" ]; then
        DETECTED_TEST_CMD="npm test"
    fi
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.80"
fi
```

### Step 4: Detect from Cargo.toml (Rust)

```bash
if [ -f "Cargo.toml" ]; then
    echo "   Detecting Rust commands..."
    
    # Rust has standard commands
    [ -z "$DETECTED_BUILD_CMD" ] && DETECTED_BUILD_CMD="cargo build"
    [ -z "$DETECTED_TEST_CMD" ] && DETECTED_TEST_CMD="cargo test"
    [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="cargo clippy"
    [ -z "$DETECTED_TYPECHECK_CMD" ] && DETECTED_TYPECHECK_CMD="cargo check"
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.95"
fi
```

### Step 5: Detect from go.mod (Go)

```bash
if [ -f "go.mod" ]; then
    echo "   Detecting Go commands..."
    
    [ -z "$DETECTED_BUILD_CMD" ] && DETECTED_BUILD_CMD="go build ./..."
    [ -z "$DETECTED_TEST_CMD" ] && DETECTED_TEST_CMD="go test ./..."
    [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="golangci-lint run ./..."
    [ -z "$DETECTED_TYPECHECK_CMD" ] && DETECTED_TYPECHECK_CMD="go vet ./..."
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.95"
fi
```

### Step 6: Detect from Python files

```bash
if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ]; then
    echo "   Detecting Python commands..."
    
    # Python typically doesn't have a build step
    
    # Test framework detection
    if [ -f "pytest.ini" ] || [ -f "pyproject.toml" ] && grep -q "pytest" pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_TEST_CMD" ] && DETECTED_TEST_CMD="pytest"
    elif [ -f "setup.py" ]; then
        [ -z "$DETECTED_TEST_CMD" ] && DETECTED_TEST_CMD="python -m pytest"
    fi
    
    # Lint detection
    if grep -q "flake8" requirements.txt pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="flake8"
    elif grep -q "ruff" requirements.txt pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="ruff check"
    elif grep -q "pylint" requirements.txt pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="pylint"
    fi
    
    # Type checking
    if grep -q "mypy" requirements.txt pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_TYPECHECK_CMD" ] && DETECTED_TYPECHECK_CMD="mypy ."
    elif grep -q "pyright" requirements.txt pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_TYPECHECK_CMD" ] && DETECTED_TYPECHECK_CMD="pyright"
    fi
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.85"
fi
```

### Step 7: Output Detection Summary

```bash
# Set default confidence if nothing was detected
[ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.50"

# Log what was found
if [ -n "$DETECTED_BUILD_CMD" ] || [ -n "$DETECTED_TEST_CMD" ] || [ -n "$DETECTED_LINT_CMD" ]; then
    echo "   Commands detected with confidence: $COMMANDS_CONFIDENCE"
else
    echo "   âš ï¸ No commands detected - will use language defaults"
fi
```

---

## Important Constraints

- Must not fail if config files are missing
- Should prefer explicit configs (package.json scripts) over inferred
- CI configs are lower confidence than direct project configs
- Must handle both YAML and JSON gracefully

DETECTIONS_RUN=$((DETECTIONS_RUN + 1))
if [ -n "$DETECTED_BUILD_CMD" ] || [ -n "$DETECTED_TEST_CMD" ]; then
    DETECTIONS_SUCCESS=$((DETECTIONS_SUCCESS + 1))
    [ -n "$DETECTED_BUILD_CMD" ] && echo "   âœ“ Build: $DETECTED_BUILD_CMD"
    [ -n "$DETECTED_TEST_CMD" ] && echo "   âœ“ Test: $DETECTED_TEST_CMD"
    [ -n "$DETECTED_LINT_CMD" ] && echo "   âœ“ Lint: $DETECTED_LINT_CMD"
fi

# 3. Detect Architecture
echo "ðŸ—ï¸  Analyzing project architecture..."
# Workflow: Detect Architecture

## Purpose

Analyze directory structure to detect project type, architecture patterns (monolith vs microservices), module boundaries, and project size metrics.

## Outputs

Sets the following variables:
- `DETECTED_PROJECT_TYPE` - web_app, cli, api, library, monorepo
- `DETECTED_ARCHITECTURE` - monolith, microservices, modular
- `DETECTED_FILE_COUNT` - Total number of source files
- `DETECTED_LINE_COUNT` - Approximate total lines of code
- `DETECTED_MODULE_COUNT` - Number of detected modules
- `DETECTED_MODULES` - Comma-separated list of module names
- `ARCHITECTURE_CONFIDENCE` - Confidence score (0.0 - 1.0)

---

## Detection Logic

### Step 1: Count Files and Lines

```bash
echo "   Counting files and lines..."

# Count source files (excluding node_modules, vendor, etc.)
DETECTED_FILE_COUNT=$(find . \
    -type f \
    \( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" \
       -o -name "*.rs" -o -name "*.go" -o -name "*.py" \
       -o -name "*.java" -o -name "*.kt" -o -name "*.swift" \
       -o -name "*.c" -o -name "*.cpp" -o -name "*.h" \) \
    ! -path "*/node_modules/*" \
    ! -path "*/vendor/*" \
    ! -path "*/.git/*" \
    ! -path "*/dist/*" \
    ! -path "*/build/*" \
    ! -path "*/target/*" \
    ! -path "*/__pycache__/*" \
    2>/dev/null | wc -l | tr -d ' ')

# Estimate line count (faster than wc -l on all files)
if [ "$DETECTED_FILE_COUNT" -gt 0 ] 2>/dev/null; then
    # Sample up to 50 files and extrapolate
    SAMPLE_LINES=$(find . \
        -type f \
        \( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" \
           -o -name "*.rs" -o -name "*.go" -o -name "*.py" \) \
        ! -path "*/node_modules/*" \
        ! -path "*/vendor/*" \
        ! -path "*/.git/*" \
        2>/dev/null | head -50 | xargs wc -l 2>/dev/null | tail -1 | awk '{print $1}')
    
    SAMPLE_COUNT=$(find . \
        -type f \
        \( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" \
           -o -name "*.rs" -o -name "*.go" -o -name "*.py" \) \
        ! -path "*/node_modules/*" \
        ! -path "*/vendor/*" \
        ! -path "*/.git/*" \
        2>/dev/null | head -50 | wc -l | tr -d ' ')
    
    if [ "$SAMPLE_COUNT" -gt 0 ] 2>/dev/null; then
        AVG_LINES_PER_FILE=$((SAMPLE_LINES / SAMPLE_COUNT))
        DETECTED_LINE_COUNT=$((DETECTED_FILE_COUNT * AVG_LINES_PER_FILE))
    else
        DETECTED_LINE_COUNT=0
    fi
else
    DETECTED_LINE_COUNT=0
fi

echo "   Found $DETECTED_FILE_COUNT files, ~$DETECTED_LINE_COUNT lines"
```

### Step 2: Detect Project Type

```bash
echo "   Detecting project type..."

DETECTED_PROJECT_TYPE="unknown"
ARCHITECTURE_CONFIDENCE="0.50"

# Web Application indicators
if [ -d "src/components" ] || [ -d "app/components" ] || [ -d "components" ]; then
    DETECTED_PROJECT_TYPE="web_app"
    ARCHITECTURE_CONFIDENCE="0.90"
elif [ -d "pages" ] || [ -d "src/pages" ] || [ -d "app" ]; then
    # Could be Next.js, Nuxt, etc.
    if [ -f "next.config.js" ] || [ -f "next.config.mjs" ] || [ -f "nuxt.config.ts" ]; then
        DETECTED_PROJECT_TYPE="web_app"
        ARCHITECTURE_CONFIDENCE="0.95"
    fi
fi

# CLI Tool indicators
if [ -d "bin" ] || [ -f "cli.js" ] || [ -f "cli.ts" ]; then
    DETECTED_PROJECT_TYPE="cli"
    ARCHITECTURE_CONFIDENCE="0.85"
elif grep -q '"bin":' package.json 2>/dev/null; then
    DETECTED_PROJECT_TYPE="cli"
    ARCHITECTURE_CONFIDENCE="0.90"
fi

# API/Backend Service indicators
if [ -d "src/routes" ] || [ -d "src/api" ] || [ -d "routes" ] || [ -d "api" ]; then
    DETECTED_PROJECT_TYPE="api"
    ARCHITECTURE_CONFIDENCE="0.85"
elif [ -d "src/controllers" ] || [ -d "controllers" ]; then
    DETECTED_PROJECT_TYPE="api"
    ARCHITECTURE_CONFIDENCE="0.85"
fi

# Library indicators
if [ -f "src/index.ts" ] || [ -f "src/index.js" ] || [ -f "src/lib.rs" ]; then
    if [ ! -d "src/components" ] && [ ! -d "src/pages" ] && [ ! -d "src/routes" ]; then
        DETECTED_PROJECT_TYPE="library"
        ARCHITECTURE_CONFIDENCE="0.80"
    fi
fi

# Monorepo indicators
if [ -d "packages" ] || [ -f "lerna.json" ] || [ -f "pnpm-workspace.yaml" ]; then
    DETECTED_PROJECT_TYPE="monorepo"
    ARCHITECTURE_CONFIDENCE="0.95"
elif [ -f "turbo.json" ] || [ -f "nx.json" ]; then
    DETECTED_PROJECT_TYPE="monorepo"
    ARCHITECTURE_CONFIDENCE="0.95"
fi

echo "   Project type: $DETECTED_PROJECT_TYPE (confidence: $ARCHITECTURE_CONFIDENCE)"
```

### Step 3: Detect Architecture Pattern

```bash
echo "   Detecting architecture pattern..."

DETECTED_ARCHITECTURE="monolith"

# Check for microservices indicators
if [ -d "services" ] && [ $(ls -d services/*/ 2>/dev/null | wc -l) -gt 1 ]; then
    DETECTED_ARCHITECTURE="microservices"
elif [ -f "docker-compose.yml" ] || [ -f "docker-compose.yaml" ]; then
    # Check if docker-compose defines multiple services with different build contexts
    SERVICE_COUNT=$(grep -c "build:" docker-compose.yml docker-compose.yaml 2>/dev/null || echo 0)
    if [ "$SERVICE_COUNT" -gt 2 ]; then
        DETECTED_ARCHITECTURE="microservices"
    fi
fi

# Check for modular architecture
if [ -d "src/modules" ] || [ -d "modules" ] || [ -d "src/features" ]; then
    DETECTED_ARCHITECTURE="modular"
fi

# Monorepo is a form of modular
if [ "$DETECTED_PROJECT_TYPE" = "monorepo" ]; then
    DETECTED_ARCHITECTURE="modular"
fi

echo "   Architecture: $DETECTED_ARCHITECTURE"
```

### Step 4: Detect Module Boundaries

```bash
echo "   Detecting module boundaries..."

DETECTED_MODULE_COUNT=0
DETECTED_MODULES=""

# For monorepos, packages are modules
if [ -d "packages" ]; then
    DETECTED_MODULES=$(ls -d packages/*/ 2>/dev/null | xargs -n1 basename | tr '\n' ',' | sed 's/,$//')
    DETECTED_MODULE_COUNT=$(echo "$DETECTED_MODULES" | tr ',' '\n' | wc -l | tr -d ' ')
fi

# For services-based
if [ -d "services" ]; then
    SERVICES=$(ls -d services/*/ 2>/dev/null | xargs -n1 basename | tr '\n' ',' | sed 's/,$//')
    if [ -n "$SERVICES" ]; then
        DETECTED_MODULES="$SERVICES"
        DETECTED_MODULE_COUNT=$(echo "$DETECTED_MODULES" | tr ',' '\n' | wc -l | tr -d ' ')
    fi
fi

# For src/modules or src/features structure
if [ -d "src/modules" ]; then
    MODULES=$(ls -d src/modules/*/ 2>/dev/null | xargs -n1 basename | tr '\n' ',' | sed 's/,$//')
    if [ -n "$MODULES" ]; then
        DETECTED_MODULES="$MODULES"
        DETECTED_MODULE_COUNT=$(echo "$DETECTED_MODULES" | tr ',' '\n' | wc -l | tr -d ' ')
    fi
elif [ -d "src/features" ]; then
    FEATURES=$(ls -d src/features/*/ 2>/dev/null | xargs -n1 basename | tr '\n' ',' | sed 's/,$//')
    if [ -n "$FEATURES" ]; then
        DETECTED_MODULES="$FEATURES"
        DETECTED_MODULE_COUNT=$(echo "$DETECTED_MODULES" | tr ',' '\n' | wc -l | tr -d ' ')
    fi
fi

# Fallback: count top-level src directories
if [ "$DETECTED_MODULE_COUNT" -eq 0 ] && [ -d "src" ]; then
    SRC_DIRS=$(ls -d src/*/ 2>/dev/null | xargs -n1 basename | grep -v "test" | grep -v "__" | tr '\n' ',' | sed 's/,$//')
    if [ -n "$SRC_DIRS" ]; then
        DETECTED_MODULES="$SRC_DIRS"
        DETECTED_MODULE_COUNT=$(echo "$DETECTED_MODULES" | tr ',' '\n' | wc -l | tr -d ' ')
    fi
fi

echo "   Modules: $DETECTED_MODULE_COUNT ($DETECTED_MODULES)"
```

### Step 5: Output Summary

```bash
echo ""
echo "   Architecture Detection Summary:"
echo "   - Type: $DETECTED_PROJECT_TYPE"
echo "   - Pattern: $DETECTED_ARCHITECTURE"
echo "   - Files: $DETECTED_FILE_COUNT"
echo "   - Lines: ~$DETECTED_LINE_COUNT"
echo "   - Modules: $DETECTED_MODULE_COUNT"
echo "   - Confidence: $ARCHITECTURE_CONFIDENCE"
```

---

## Important Constraints

- Must handle empty directories gracefully
- Should not count files in ignored directories (node_modules, vendor, etc.)
- Line count is an estimate, not exact
- Module detection is heuristic-based

DETECTIONS_RUN=$((DETECTIONS_RUN + 1))
if [ -n "$DETECTED_PROJECT_TYPE" ]; then
    DETECTIONS_SUCCESS=$((DETECTIONS_SUCCESS + 1))
    echo "   âœ“ Type: $DETECTED_PROJECT_TYPE"
    echo "   âœ“ Size: $DETECTED_FILE_COUNT files, ~$DETECTED_LINE_COUNT lines"
    [ -n "$DETECTED_MODULES" ] && echo "   âœ“ Modules: $DETECTED_MODULES"
fi

# 4. Detect Security Level
echo "ðŸ”’ Checking security indicators..."
# Workflow: Detect Security Level

## Purpose

Check for security-related dependencies, secrets management patterns, and authentication configurations to determine the project's security level (low, moderate, high).

## Outputs

Sets the following variables:
- `DETECTED_SECURITY_LEVEL` - low, moderate, or high
- `SECURITY_INDICATORS` - Comma-separated list of detected security features
- `HAS_AUTH` - true/false
- `HAS_SECRETS_MANAGEMENT` - true/false
- `SECURITY_CONFIDENCE` - Confidence score (0.0 - 1.0)

---

## Detection Logic

### Step 1: Initialize

```bash
SECURITY_SCORE=0
SECURITY_INDICATORS=""
HAS_AUTH="false"
HAS_SECRETS_MANAGEMENT="false"
SECURITY_CONFIDENCE="0.70"
```

### Step 2: Check for Authentication Dependencies

```bash
echo "   Checking for authentication..."

# Node.js auth libraries
if [ -f "package.json" ]; then
    DEPS=$(cat package.json 2>/dev/null)
    
    # Auth libraries
    if echo "$DEPS" | grep -qE '"(passport|@auth0|firebase-admin|next-auth|lucia|clerk)"'; then
        HAS_AUTH="true"
        SECURITY_SCORE=$((SECURITY_SCORE + 3))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}auth-library,"
        echo "   âœ“ Authentication library detected"
    fi
    
    # Password hashing
    if echo "$DEPS" | grep -qE '"(bcrypt|argon2|scrypt)"'; then
        SECURITY_SCORE=$((SECURITY_SCORE + 2))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}password-hashing,"
        echo "   âœ“ Password hashing library detected"
    fi
    
    # JWT/Session
    if echo "$DEPS" | grep -qE '"(jsonwebtoken|jose|iron-session)"'; then
        SECURITY_SCORE=$((SECURITY_SCORE + 1))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}jwt-sessions,"
    fi
    
    # OAuth
    if echo "$DEPS" | grep -qE '"(oauth|passport-oauth|passport-google|passport-github)"'; then
        HAS_AUTH="true"
        SECURITY_SCORE=$((SECURITY_SCORE + 2))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}oauth,"
        echo "   âœ“ OAuth integration detected"
    fi
fi

# Rust auth
if [ -f "Cargo.toml" ]; then
    CARGO=$(cat Cargo.toml 2>/dev/null)
    
    if echo "$CARGO" | grep -qE '(argon2|bcrypt|jsonwebtoken|oauth2)'; then
        HAS_AUTH="true"
        SECURITY_SCORE=$((SECURITY_SCORE + 3))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}rust-auth,"
    fi
fi

# Python auth
if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ]; then
    PYDEPS=""
    [ -f "requirements.txt" ] && PYDEPS="$PYDEPS $(cat requirements.txt)"
    [ -f "pyproject.toml" ] && PYDEPS="$PYDEPS $(cat pyproject.toml)"
    
    if echo "$PYDEPS" | grep -qiE '(passlib|bcrypt|python-jose|authlib|django-allauth)'; then
        HAS_AUTH="true"
        SECURITY_SCORE=$((SECURITY_SCORE + 3))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}python-auth,"
    fi
fi
```

### Step 3: Check for Secrets Management

```bash
echo "   Checking for secrets management..."

# Environment files
if [ -f ".env.example" ] || [ -f ".env.sample" ] || [ -f ".env.template" ]; then
    HAS_SECRETS_MANAGEMENT="true"
    SECURITY_SCORE=$((SECURITY_SCORE + 1))
    SECURITY_INDICATORS="${SECURITY_INDICATORS}env-files,"
    echo "   âœ“ Environment file template detected"
fi

# Secrets in docker-compose
if [ -f "docker-compose.yml" ] || [ -f "docker-compose.yaml" ]; then
    DC_CONTENT=$(cat docker-compose.yml docker-compose.yaml 2>/dev/null)
    
    if echo "$DC_CONTENT" | grep -qE '(secrets:|vault)'; then
        HAS_SECRETS_MANAGEMENT="true"
        SECURITY_SCORE=$((SECURITY_SCORE + 2))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}docker-secrets,"
        echo "   âœ“ Docker secrets detected"
    fi
fi

# Vault integration
if grep -rq "vault" . --include="*.yml" --include="*.yaml" --include="*.json" 2>/dev/null | head -1 | grep -q vault; then
    HAS_SECRETS_MANAGEMENT="true"
    SECURITY_SCORE=$((SECURITY_SCORE + 3))
    SECURITY_INDICATORS="${SECURITY_INDICATORS}vault,"
    echo "   âœ“ HashiCorp Vault detected"
fi

# AWS Secrets Manager
if grep -rq "secretsmanager\|SecretsManager" . --include="*.ts" --include="*.js" --include="*.py" 2>/dev/null | head -1 | grep -q secret; then
    HAS_SECRETS_MANAGEMENT="true"
    SECURITY_SCORE=$((SECURITY_SCORE + 3))
    SECURITY_INDICATORS="${SECURITY_INDICATORS}aws-secrets,"
    echo "   âœ“ AWS Secrets Manager detected"
fi
```

### Step 4: Check for Security Headers/Middleware

```bash
echo "   Checking for security middleware..."

# Helmet (Node.js security headers)
if [ -f "package.json" ]; then
    if grep -q '"helmet"' package.json 2>/dev/null; then
        SECURITY_SCORE=$((SECURITY_SCORE + 1))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}helmet,"
        echo "   âœ“ Helmet security headers detected"
    fi
    
    # Rate limiting
    if grep -qE '"(express-rate-limit|rate-limiter-flexible)"' package.json 2>/dev/null; then
        SECURITY_SCORE=$((SECURITY_SCORE + 1))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}rate-limiting,"
        echo "   âœ“ Rate limiting detected"
    fi
    
    # CORS
    if grep -q '"cors"' package.json 2>/dev/null; then
        SECURITY_INDICATORS="${SECURITY_INDICATORS}cors-config,"
    fi
fi
```

### Step 5: Check for Encryption

```bash
echo "   Checking for encryption..."

# Crypto libraries
if [ -f "package.json" ]; then
    if grep -qE '"(crypto-js|node-forge|tweetnacl)"' package.json 2>/dev/null; then
        SECURITY_SCORE=$((SECURITY_SCORE + 2))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}encryption,"
        echo "   âœ“ Encryption library detected"
    fi
fi

# TLS/SSL configs
if [ -f "nginx.conf" ] || [ -d "ssl" ] || [ -d "certs" ]; then
    SECURITY_SCORE=$((SECURITY_SCORE + 1))
    SECURITY_INDICATORS="${SECURITY_INDICATORS}ssl-config,"
    echo "   âœ“ SSL/TLS configuration detected"
fi
```

### Step 6: Check for Open Source Indicators (Lower Security Needs)

```bash
# Open source projects may have lower security needs
if [ -f "LICENSE" ]; then
    LICENSE_TYPE=$(head -5 LICENSE 2>/dev/null)
    
    if echo "$LICENSE_TYPE" | grep -qiE '(MIT|Apache|GPL|BSD|ISC)'; then
        # Open source - reduce security expectation slightly
        SECURITY_SCORE=$((SECURITY_SCORE - 1))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}open-source,"
    fi
fi
```

### Step 7: Calculate Security Level

```bash
# Normalize score
[ $SECURITY_SCORE -lt 0 ] && SECURITY_SCORE=0

# Determine level
if [ $SECURITY_SCORE -ge 5 ]; then
    DETECTED_SECURITY_LEVEL="high"
    SECURITY_CONFIDENCE="0.90"
elif [ $SECURITY_SCORE -ge 2 ]; then
    DETECTED_SECURITY_LEVEL="moderate"
    SECURITY_CONFIDENCE="0.80"
else
    DETECTED_SECURITY_LEVEL="low"
    SECURITY_CONFIDENCE="0.70"
fi

# Clean up indicators string
SECURITY_INDICATORS=$(echo "$SECURITY_INDICATORS" | sed 's/,$//')

echo ""
echo "   Security Detection Summary:"
echo "   - Level: $DETECTED_SECURITY_LEVEL"
echo "   - Score: $SECURITY_SCORE"
echo "   - Has Auth: $HAS_AUTH"
echo "   - Has Secrets Management: $HAS_SECRETS_MANAGEMENT"
echo "   - Indicators: $SECURITY_INDICATORS"
```

---

## Security Level Guidelines

| Level | Score | Indicators |
|-------|-------|------------|
| **High** | 5+ | Auth + secrets management + encryption |
| **Moderate** | 2-4 | Some auth or env file management |
| **Low** | 0-1 | No security indicators, likely open source |

---

## Important Constraints

- Must not fail if files are missing
- Should not scan inside node_modules, vendor, etc.
- Open source projects default to lower security expectations
- Confidence increases with more indicators found

DETECTIONS_RUN=$((DETECTIONS_RUN + 1))
if [ -n "$DETECTED_SECURITY_LEVEL" ]; then
    DETECTIONS_SUCCESS=$((DETECTIONS_SUCCESS + 1))
    echo "   âœ“ Security Level: $DETECTED_SECURITY_LEVEL"
fi
```

### Step 3: Calculate Overall Confidence

```bash
# Calculate confidence score (0.0 - 1.0)
if [ $DETECTIONS_RUN -gt 0 ]; then
    DETECTION_CONFIDENCE=$(echo "scale=2; $DETECTIONS_SUCCESS / $DETECTIONS_RUN" | bc)
else
    DETECTION_CONFIDENCE="0.00"
fi

echo ""
echo "ðŸ“Š Detection confidence: ${DETECTION_CONFIDENCE} ($DETECTIONS_SUCCESS/$DETECTIONS_RUN successful)"
```

### Step 4: Apply Fallbacks for Failed Detections

```bash
# Fallback: Unknown language
if [ -z "$DETECTED_LANGUAGE" ]; then
    echo "âš ï¸  Could not detect language - will ask user"
    DETECTED_LANGUAGE="unknown"
    NEEDS_USER_INPUT_LANGUAGE=true
fi

# Fallback: Unknown project type
if [ -z "$DETECTED_PROJECT_TYPE" ]; then
    echo "âš ï¸  Could not detect project type - will ask user"
    DETECTED_PROJECT_TYPE="unknown"
    NEEDS_USER_INPUT_TYPE=true
fi

# Fallback: Commands not found
if [ -z "$DETECTED_BUILD_CMD" ] && [ -z "$DETECTED_TEST_CMD" ]; then
    echo "âš ï¸  Could not detect build/test commands - using language defaults"
    case "$DETECTED_LANGUAGE" in
        "javascript"|"typescript") 
            DETECTED_BUILD_CMD="npm run build"
            DETECTED_TEST_CMD="npm test"
            DETECTED_LINT_CMD="npm run lint"
            ;;
        "rust")
            DETECTED_BUILD_CMD="cargo build"
            DETECTED_TEST_CMD="cargo test"
            DETECTED_LINT_CMD="cargo clippy"
            ;;
        "python")
            DETECTED_TEST_CMD="pytest"
            DETECTED_LINT_CMD="flake8"
            ;;
        "go")
            DETECTED_BUILD_CMD="go build ./..."
            DETECTED_TEST_CMD="go test ./..."
            DETECTED_LINT_CMD="golangci-lint run"
            ;;
    esac
fi

# Fallback: Security level
if [ -z "$DETECTED_SECURITY_LEVEL" ]; then
    DETECTED_SECURITY_LEVEL="moderate"
fi
```

### Step 5: Generate Unified Profile

```bash
# Infer complexity from size
INFERRED_COMPLEXITY="simple"
if [ "$DETECTED_FILE_COUNT" -gt 500 ] 2>/dev/null; then
    INFERRED_COMPLEXITY="complex"
elif [ "$DETECTED_FILE_COUNT" -gt 100 ] 2>/dev/null; then
    INFERRED_COMPLEXITY="moderate"
fi

# Generate the profile YAML
cat > agent-os/config/project-profile.yml << PROFILE_EOF
# Project Profile
# Auto-generated by detect-project-profile workflow
# Review and modify as needed

gathered:
  project_type: ${DETECTED_PROJECT_TYPE:-unknown}
  tech_stack:
    language: ${DETECTED_LANGUAGE:-unknown}
    framework: ${DETECTED_FRAMEWORK:-}
    backend: ${DETECTED_BACKEND:-}
    database: ${DETECTED_DATABASE:-}
  size:
    lines: ${DETECTED_LINE_COUNT:-0}
    files: ${DETECTED_FILE_COUNT:-0}
    modules: ${DETECTED_MODULE_COUNT:-0}
  commands:
    build: "${DETECTED_BUILD_CMD:-}"
    test: "${DETECTED_TEST_CMD:-}"
    lint: "${DETECTED_LINT_CMD:-}"

inferred:
  security_level: ${DETECTED_SECURITY_LEVEL:-moderate}
  complexity: ${INFERRED_COMPLEXITY}

user_specified:
  compliance: []
  human_review_level: moderate

_meta:
  detected_at: $(date -Iseconds)
  detection_confidence: ${DETECTION_CONFIDENCE}
  questions_asked: 0
  questions_auto_answered: $DETECTIONS_SUCCESS
  needs_user_input:
    language: ${NEEDS_USER_INPUT_LANGUAGE:-false}
    project_type: ${NEEDS_USER_INPUT_TYPE:-false}

PROFILE_EOF

echo ""
echo "âœ… Project profile saved to: agent-os/config/project-profile.yml"
```

### Step 6: Output Detection Summary

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  DETECTION COMPLETE"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Detected Configuration:"
echo ""
echo "  Project Type:    ${DETECTED_PROJECT_TYPE:-unknown}"
echo "  Language:        ${DETECTED_LANGUAGE:-unknown}"
echo "  Framework:       ${DETECTED_FRAMEWORK:-(none detected)}"
echo "  Database:        ${DETECTED_DATABASE:-(none detected)}"
echo "  Size:            ${DETECTED_FILE_COUNT:-?} files, ~${DETECTED_LINE_COUNT:-?} lines"
echo "  Security Level:  ${DETECTED_SECURITY_LEVEL:-moderate}"
echo "  Complexity:      ${INFERRED_COMPLEXITY}"
echo ""
echo "  Build Command:   ${DETECTED_BUILD_CMD:-(not set)}"
echo "  Test Command:    ${DETECTED_TEST_CMD:-(not set)}"
echo "  Lint Command:    ${DETECTED_LINT_CMD:-(not set)}"
echo ""
echo "  Confidence:      ${DETECTION_CONFIDENCE}"
echo ""
```

---

## Integration

This workflow is called by:
- `adapt-to-product/1-setup-and-information-gathering.md`
- `create-basepoints/1-validate-prerequisites.md`
- `deploy-agents/1-validate-prerequisites.md`

After this workflow completes, call:
- `# Workflow: Present and Confirm

## Purpose

Format detected values for display, present a confirmation prompt to the user, handle user overrides, and output the final confirmed profile.

## Inputs

Expects these variables to be set (from prior detection workflows):
- `DETECTED_PROJECT_TYPE`
- `DETECTED_LANGUAGE`
- `DETECTED_FRAMEWORK`
- `DETECTED_DATABASE`
- `DETECTED_BUILD_CMD`
- `DETECTED_TEST_CMD`
- `DETECTED_LINT_CMD`
- `DETECTED_SECURITY_LEVEL`
- `DETECTED_FILE_COUNT`
- `DETECTED_LINE_COUNT`
- `DETECTION_CONFIDENCE`

## Outputs

- Updates `agent-os/config/project-profile.yml` with confirmed values
- Sets `USER_CONFIRMED=true` when user accepts

---

## Workflow

### Step 1: Display Detected Configuration

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "                      DETECTED PROJECT CONFIGURATION                         "
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Project Profile Section
echo "ðŸ“¦ PROJECT PROFILE"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Type:" "${DETECTED_PROJECT_TYPE:-unknown} âœ“"
printf "   %-20s %s\n" "Size:" "${DETECTED_FILE_COUNT:-?} files, ~${DETECTED_LINE_COUNT:-?} lines âœ“"
printf "   %-20s %s\n" "Maturity:" "$([ -d '.git' ] && echo 'Version controlled' || echo 'Unknown') âœ“"
echo ""

# Tech Stack Section
echo "ðŸ”§ TECH STACK"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Language:" "${DETECTED_LANGUAGE:-unknown} âœ“"
[ -n "$DETECTED_FRAMEWORK" ] && printf "   %-20s %s\n" "Framework:" "$DETECTED_FRAMEWORK âœ“"
[ -n "$DETECTED_BACKEND" ] && printf "   %-20s %s\n" "Backend:" "$DETECTED_BACKEND âœ“"
[ -n "$DETECTED_DATABASE" ] && printf "   %-20s %s\n" "Database:" "$DETECTED_DATABASE âœ“"
echo ""

# Commands Section
echo "âš™ï¸  DETECTED COMMANDS"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Build:" "${DETECTED_BUILD_CMD:-(not detected)} âœ“"
printf "   %-20s %s\n" "Test:" "${DETECTED_TEST_CMD:-(not detected)} âœ“"
printf "   %-20s %s\n" "Lint:" "${DETECTED_LINT_CMD:-(not detected)} âœ“"
echo ""

# Inferred Settings Section
echo "ðŸ”’ INFERRED SETTINGS"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Security Level:" "${DETECTED_SECURITY_LEVEL:-moderate} âœ“"
printf "   %-20s %s\n" "Complexity:" "${INFERRED_COMPLEXITY:-moderate} âœ“"
echo ""

# Confidence
echo "ðŸ“Š DETECTION CONFIDENCE: ${DETECTION_CONFIDENCE:-0.80}"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

### Step 2: Present Minimal Questions

Only ask questions for things that cannot be detected:

```bash
echo ""
echo "âš ï¸  QUESTIONS REQUIRING YOUR INPUT"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "These items cannot be determined automatically from your codebase:"
echo ""

# Question 1: Compliance Requirements
echo "1. What compliance requirements apply to this project?"
echo ""
echo "   [ ] None (default)"
echo "   [ ] SOC 2"
echo "   [ ] HIPAA"
echo "   [ ] GDPR"
echo "   [ ] PCI-DSS"
echo "   [ ] Other"
echo ""
echo "   Enter your choice (e.g., 'none', 'gdpr', 'soc2,hipaa'): "

# In non-interactive mode or if user presses Enter, use default
USER_COMPLIANCE="${USER_COMPLIANCE:-none}"

echo ""

# Question 2: Human Review Level
echo "2. How much human oversight do you want for AI-generated changes?"
echo ""
echo "   [ ] minimal  - Trust AI, review only critical changes"
echo "   [ ] moderate - Review architectural decisions (default)"
echo "   [ ] high     - Review all significant changes"
echo ""
echo "   Enter your choice (minimal/moderate/high): "

# Default to moderate
USER_HUMAN_REVIEW="${USER_HUMAN_REVIEW:-moderate}"

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

### Step 3: Confirmation Prompt

```bash
echo ""
echo "ðŸ“‹ CONFIRMATION"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "Press Enter to accept these detected values, or type a section name to modify:"
echo ""
echo "   â€¢ 'type'      - Change project type"
echo "   â€¢ 'language'  - Change language/framework"
echo "   â€¢ 'commands'  - Change build/test/lint commands"
echo "   â€¢ 'security'  - Change security level"
echo "   â€¢ 'all'       - Review all settings interactively"
echo ""
echo "Your choice (Enter to accept): "

# For non-interactive execution, assume acceptance
USER_CHOICE="${USER_CHOICE:-accept}"

if [ "$USER_CHOICE" = "" ] || [ "$USER_CHOICE" = "accept" ]; then
    echo ""
    echo "âœ… Configuration confirmed!"
    USER_CONFIRMED="true"
fi
```

### Step 4: Handle Overrides (if requested)

```bash
# Handle override requests
case "$USER_CHOICE" in
    "type")
        echo "Enter new project type (web_app, cli, api, library, monorepo):"
        read -r NEW_TYPE
        [ -n "$NEW_TYPE" ] && DETECTED_PROJECT_TYPE="$NEW_TYPE"
        ;;
    "language")
        echo "Enter primary language (typescript, javascript, rust, python, go):"
        read -r NEW_LANG
        [ -n "$NEW_LANG" ] && DETECTED_LANGUAGE="$NEW_LANG"
        echo "Enter framework (or leave empty):"
        read -r NEW_FRAMEWORK
        DETECTED_FRAMEWORK="$NEW_FRAMEWORK"
        ;;
    "commands")
        echo "Enter build command (or leave empty):"
        read -r NEW_BUILD
        DETECTED_BUILD_CMD="$NEW_BUILD"
        echo "Enter test command (or leave empty):"
        read -r NEW_TEST
        DETECTED_TEST_CMD="$NEW_TEST"
        echo "Enter lint command (or leave empty):"
        read -r NEW_LINT
        DETECTED_LINT_CMD="$NEW_LINT"
        ;;
    "security")
        echo "Enter security level (low, moderate, high):"
        read -r NEW_SECURITY
        [ -n "$NEW_SECURITY" ] && DETECTED_SECURITY_LEVEL="$NEW_SECURITY"
        ;;
esac
```

### Step 5: Update Profile with Confirmed Values

```bash
# Update the profile with user-specified values
cat > agent-os/config/project-profile.yml << CONFIRMED_EOF
# Project Profile
# Auto-detected and confirmed by user
# Generated: $(date -Iseconds)

gathered:
  project_type: ${DETECTED_PROJECT_TYPE:-unknown}
  tech_stack:
    language: ${DETECTED_LANGUAGE:-unknown}
    framework: ${DETECTED_FRAMEWORK:-}
    backend: ${DETECTED_BACKEND:-}
    database: ${DETECTED_DATABASE:-}
  size:
    lines: ${DETECTED_LINE_COUNT:-0}
    files: ${DETECTED_FILE_COUNT:-0}
    modules: ${DETECTED_MODULE_COUNT:-0}
  commands:
    build: "${DETECTED_BUILD_CMD:-}"
    test: "${DETECTED_TEST_CMD:-}"
    lint: "${DETECTED_LINT_CMD:-}"

inferred:
  security_level: ${DETECTED_SECURITY_LEVEL:-moderate}
  complexity: ${INFERRED_COMPLEXITY:-moderate}

user_specified:
  compliance:
$(echo "$USER_COMPLIANCE" | tr ',' '\n' | sed 's/^/    - /' | grep -v "^    - none$" || echo "    []")
  human_review_level: ${USER_HUMAN_REVIEW:-moderate}

_meta:
  detected_at: $(date -Iseconds)
  confirmed_at: $(date -Iseconds)
  detection_confidence: ${DETECTION_CONFIDENCE:-0.80}
  user_confirmed: ${USER_CONFIRMED:-true}
  questions_asked: 2
  questions_auto_answered: ${DETECTIONS_SUCCESS:-0}

CONFIRMED_EOF

echo ""
echo "âœ… Profile saved to: agent-os/config/project-profile.yml"
echo ""
```

---

## Output Summary

After confirmation, display:

```bash
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "                        CONFIGURATION COMPLETE                               "
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Your project profile has been saved and will be used for:"
echo ""
echo "  â€¢ Basepoint generation (create-basepoints)"
echo "  â€¢ Agent specialization (deploy-agents)"
echo "  â€¢ Validation command configuration"
echo "  â€¢ Workflow complexity selection"
echo ""
echo "You can modify the profile at any time by editing:"
echo "  agent-os/config/project-profile.yml"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

---

## Important Constraints

- Default to accepting detected values (user presses Enter)
- Maximum 2-3 questions that can't be auto-detected
- Provide sensible defaults for all questions
- Allow overrides but don't require them
- Save confirmed profile for use by subsequent commands
` - To get user confirmation
- `# Workflow: Research Orchestrator

## Purpose

Main orchestrator for web research. Loads detected tech stack, determines research depth, calls appropriate research workflows, and aggregates results into the enriched-knowledge directory.

## Inputs

- `agent-os/config/project-profile.yml` - Detected project profile
- `RESEARCH_DEPTH` - minimal, standard, or comprehensive (default: standard)

## Outputs

- `agent-os/config/enriched-knowledge/` directory with research results

---

## Workflow

### Step 1: Load Project Profile

```bash
echo "ðŸ”¬ Starting knowledge enrichment research..."
echo ""

# Create enriched-knowledge directory
mkdir -p agent-os/config/enriched-knowledge

# Load project profile
if [ -f "agent-os/config/project-profile.yml" ]; then
    echo "ðŸ“‚ Loading project profile..."
    
    # Extract key values (simplified parsing)
    DETECTED_LANGUAGE=$(grep "language:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    DETECTED_FRAMEWORK=$(grep "framework:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    DETECTED_DATABASE=$(grep "database:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    PROJECT_TYPE=$(grep "project_type:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    SECURITY_LEVEL=$(grep "security_level:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    
    echo "   Language: $DETECTED_LANGUAGE"
    echo "   Framework: ${DETECTED_FRAMEWORK:-(none)}"
    echo "   Database: ${DETECTED_DATABASE:-(none)}"
    echo "   Project Type: $PROJECT_TYPE"
else
    echo "âš ï¸ No project profile found. Run detection first."
    echo "   Using defaults..."
    DETECTED_LANGUAGE="unknown"
fi
```

### Step 2: Determine Research Depth

```bash
# Set research depth (can be overridden)
RESEARCH_DEPTH="${RESEARCH_DEPTH:-standard}"

echo ""
echo "ðŸ“Š Research depth: $RESEARCH_DEPTH"
echo ""

# Define what each depth includes
case "$RESEARCH_DEPTH" in
    "minimal")
        echo "   â€¢ Latest versions"
        echo "   â€¢ Critical security issues"
        echo "   Estimated time: ~30 seconds"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=false
        DO_DOMAIN_RESEARCH=false
        ;;
    "standard")
        echo "   â€¢ Latest versions"
        echo "   â€¢ Security issues"
        echo "   â€¢ Best practices"
        echo "   â€¢ Common pitfalls"
        echo "   Estimated time: ~2 minutes"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=true
        DO_DOMAIN_RESEARCH=false
        ;;
    "comprehensive")
        echo "   â€¢ All standard research"
        echo "   â€¢ Architecture patterns"
        echo "   â€¢ Domain knowledge"
        echo "   â€¢ Migration guides"
        echo "   Estimated time: ~5 minutes"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=true
        DO_DOMAIN_RESEARCH=true
        ;;
esac

echo ""
```

### Step 3: Execute Research Workflows

```bash
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  EXECUTING RESEARCH WORKFLOWS"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Track what was researched
RESEARCH_COMPLETED=""

# 1. Library Research (always in minimal+)
if [ "$DO_LIBRARY_RESEARCH" = "true" ]; then
    echo "ðŸ“š Researching libraries and frameworks..."
    # Workflow: Research Library

## Purpose

Research best practices, known issues, security vulnerabilities, and latest versions for each detected library/framework. Compiles findings into structured markdown.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework (if any)
- `DETECTED_DATABASE` - Database (if any)

## Outputs

- `agent-os/config/enriched-knowledge/library-research.md`

---

## Web Search Queries

For each detected technology, perform the following searches:

### Query Templates

```
1. "[library] best practices [current_year]"
2. "[library] common mistakes to avoid"
3. "[library] known issues bugs"
4. "[library] latest stable version"
5. "[library] security vulnerabilities CVE"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/library-research.md"

cat > "$OUTPUT_FILE" << 'HEADER_EOF'
# Library Research

> Auto-generated knowledge enrichment from web research
> Generated: $(date -Iseconds)

This document contains best practices, known issues, and recommendations
for the libraries and frameworks detected in your project.

---

HEADER_EOF
```

### Step 2: Research Primary Language

```bash
if [ -n "$DETECTED_LANGUAGE" ] && [ "$DETECTED_LANGUAGE" != "unknown" ]; then
    echo "   Researching $DETECTED_LANGUAGE..."
    
    cat >> "$OUTPUT_FILE" << LANG_EOF

## $DETECTED_LANGUAGE

### Best Practices

<!-- Web search: "$DETECTED_LANGUAGE best practices $CURRENT_YEAR" -->

**Recommended practices for $DETECTED_LANGUAGE development:**

- Follow official style guides and conventions
- Use type annotations where available
- Implement proper error handling
- Write comprehensive tests
- Use linting and formatting tools

### Common Pitfalls

<!-- Web search: "$DETECTED_LANGUAGE common mistakes to avoid" -->

**Common mistakes to avoid:**

- Ignoring error handling
- Not using proper dependency management
- Skipping type safety features
- Insufficient testing
- Poor code organization

### Resources

- Official documentation
- Community style guides
- Popular learning resources

---

LANG_EOF
fi
```

### Step 3: Research Framework

```bash
if [ -n "$DETECTED_FRAMEWORK" ] && [ "$DETECTED_FRAMEWORK" != "" ]; then
    echo "   Researching $DETECTED_FRAMEWORK..."
    
    cat >> "$OUTPUT_FILE" << FRAMEWORK_EOF

## $DETECTED_FRAMEWORK

### Best Practices

<!-- Web search: "$DETECTED_FRAMEWORK best practices $CURRENT_YEAR" -->

**Recommended practices for $DETECTED_FRAMEWORK:**

- Follow the framework's recommended project structure
- Use built-in features before reaching for third-party solutions
- Implement proper state management patterns
- Optimize for performance from the start
- Follow security guidelines

### Architecture Recommendations

<!-- Web search: "$DETECTED_FRAMEWORK architecture patterns" -->

**Recommended architecture patterns:**

- Component-based architecture (for UI frameworks)
- Clean separation of concerns
- Proper routing and navigation patterns
- Effective data fetching strategies
- Error boundary implementation

### Known Issues

<!-- Web search: "$DETECTED_FRAMEWORK known issues bugs" -->

**Common issues to be aware of:**

- Check the framework's GitHub issues for current bugs
- Review migration guides for breaking changes
- Monitor security advisories

### Performance Tips

<!-- Web search: "$DETECTED_FRAMEWORK performance optimization" -->

**Performance optimization strategies:**

- Implement lazy loading
- Use memoization appropriately
- Optimize bundle size
- Monitor and profile regularly

---

FRAMEWORK_EOF
fi
```

### Step 4: Research Database

```bash
if [ -n "$DETECTED_DATABASE" ] && [ "$DETECTED_DATABASE" != "" ]; then
    echo "   Researching $DETECTED_DATABASE..."
    
    cat >> "$OUTPUT_FILE" << DB_EOF

## $DETECTED_DATABASE

### Best Practices

<!-- Web search: "$DETECTED_DATABASE best practices $CURRENT_YEAR" -->

**Database best practices:**

- Use proper indexing strategies
- Implement connection pooling
- Follow security best practices
- Regular backup and maintenance
- Monitor query performance

### Security Considerations

<!-- Web search: "$DETECTED_DATABASE security best practices" -->

**Security recommendations:**

- Use parameterized queries (prevent SQL injection)
- Implement proper authentication
- Encrypt sensitive data
- Regular security audits
- Keep database updated

### Performance Optimization

<!-- Web search: "$DETECTED_DATABASE performance tuning" -->

**Performance tips:**

- Optimize query patterns
- Use appropriate indexes
- Monitor slow queries
- Consider caching strategies
- Regular maintenance

---

DB_EOF
fi
```

### Step 5: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

---

## How to Use This Document

1. **Review recommendations** before implementing features
2. **Check known issues** when debugging problems
3. **Follow best practices** for code quality
4. **Monitor security advisories** for your dependencies

## Sources

Research compiled from:
- Official documentation
- GitHub issues and discussions
- Stack Overflow community
- Security advisory databases
- Community best practices

---

*Generated by Geist Adaptive Questionnaire System*
*For the most current information, verify with official sources*

FOOTER_EOF

echo "   âœ“ Library research saved to $OUTPUT_FILE"
```

---

## Web Search Integration

When executing this workflow, the AI agent should use the `web_search` tool to gather current information:

```markdown
### Example Web Search Calls

For React framework:
- web_search("React 18 best practices 2026")
- web_search("React common mistakes to avoid")
- web_search("React known issues bugs")
- web_search("React latest stable version")

For PostgreSQL:
- web_search("PostgreSQL best practices 2026")
- web_search("PostgreSQL security hardening")
- web_search("PostgreSQL performance tuning")
```

---

## Important Constraints

- Must attribute information sources
- Should include publication dates for time-sensitive info
- Must handle missing/unknown technologies gracefully
- Should prioritize official documentation over blog posts
- Results should be actionable, not just informational

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}library,"
fi

# 2. Security Research (always in minimal+)
if [ "$DO_SECURITY_RESEARCH" = "true" ]; then
    echo ""
    echo "ðŸ”’ Researching security vulnerabilities..."
    # Workflow: Research Security

## Purpose

Research security vulnerabilities (CVEs), security advisories, and security best practices for the detected dependencies.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework
- `DETECTED_DATABASE` - Database
- Dependencies from package.json, Cargo.toml, etc.

## Outputs

- `agent-os/config/enriched-knowledge/security-notes.md`

---

## Web Search Queries

### Query Templates

```
1. "[dependency] CVE vulnerabilities [year]"
2. "[dependency] security advisory"
3. "[framework] security best practices"
4. "[language] OWASP top 10 prevention"
5. "[dependency] security patch"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/security-notes.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Security Notes

> Security vulnerabilities and recommendations for your dependencies
> Generated: $(date -Iseconds)

âš ï¸ **Important**: Always verify security information with official sources.
Check npm audit, cargo audit, or pip-audit for real-time vulnerability scanning.

---

HEADER_EOF
```

### Step 2: Research Language Security

```bash
if [ -n "$DETECTED_LANGUAGE" ] && [ "$DETECTED_LANGUAGE" != "unknown" ]; then
    cat >> "$OUTPUT_FILE" << LANG_SEC_EOF

## $DETECTED_LANGUAGE Security

<!-- Web search: "$DETECTED_LANGUAGE security best practices $CURRENT_YEAR" -->

### Common Vulnerabilities

| Vulnerability | Risk | Prevention |
|---------------|------|------------|
| Injection attacks | High | Input validation, parameterized queries |
| XSS (if web) | High | Output encoding, CSP headers |
| Insecure dependencies | Medium | Regular audits, version pinning |
| Secrets exposure | Critical | Environment variables, secret managers |

### Security Tools

LANG_SEC_EOF

    case "$DETECTED_LANGUAGE" in
        "javascript"|"typescript")
            cat >> "$OUTPUT_FILE" << 'JSTOOLS_EOF'
- **npm audit** - Check for vulnerable dependencies
- **Snyk** - Continuous vulnerability monitoring
- **ESLint security plugins** - Static code analysis
- **Helmet** - Security headers for Express

```bash
# Run security audit
npm audit
npm audit fix

# Use Snyk
npx snyk test
```

JSTOOLS_EOF
            ;;
        "rust")
            cat >> "$OUTPUT_FILE" << 'RSTOOLS_EOF'
- **cargo audit** - Check for vulnerable dependencies
- **cargo deny** - Lint dependencies
- **clippy** - Catch common mistakes

```bash
# Run security audit
cargo audit

# Install if needed
cargo install cargo-audit
```

RSTOOLS_EOF
            ;;
        "python")
            cat >> "$OUTPUT_FILE" << 'PYTOOLS_EOF'
- **pip-audit** - Check for vulnerable dependencies
- **safety** - Check dependencies against safety db
- **bandit** - Security linter

```bash
# Run security audit
pip-audit

# Or use safety
safety check
```

PYTOOLS_EOF
            ;;
        "go")
            cat >> "$OUTPUT_FILE" << 'GOTOOLS_EOF'
- **govulncheck** - Official Go vulnerability scanner
- **gosec** - Security linter

```bash
# Run vulnerability check
govulncheck ./...

# Install if needed
go install golang.org/x/vuln/cmd/govulncheck@latest
```

GOTOOLS_EOF
            ;;
    esac
    
    echo "---" >> "$OUTPUT_FILE"
fi
```

### Step 3: Research Framework Security

```bash
if [ -n "$DETECTED_FRAMEWORK" ]; then
    cat >> "$OUTPUT_FILE" << FRAMEWORK_SEC_EOF

## $DETECTED_FRAMEWORK Security

<!-- Web search: "$DETECTED_FRAMEWORK security vulnerabilities CVE" -->

### Security Checklist

- [ ] Keep framework updated to latest stable version
- [ ] Review security advisories regularly
- [ ] Follow framework's security guidelines
- [ ] Implement recommended security middleware
- [ ] Use built-in security features

### Known Security Considerations

FRAMEWORK_SEC_EOF

    case "$DETECTED_FRAMEWORK" in
        "react"|"vue"|"angular")
            cat >> "$OUTPUT_FILE" << 'FRONTEND_SEC_EOF'
**Frontend Framework Security:**

1. **XSS Prevention**
   - Use framework's built-in escaping
   - Avoid `dangerouslySetInnerHTML` (React) or `v-html` (Vue)
   - Sanitize user input before display

2. **CSRF Protection**
   - Use anti-CSRF tokens
   - SameSite cookie attribute
   - Verify origin headers

3. **Secure Dependencies**
   - Regular `npm audit`
   - Lock file integrity
   - Review new dependencies

FRONTEND_SEC_EOF
            ;;
        "express"|"fastify"|"koa")
            cat >> "$OUTPUT_FILE" << 'BACKEND_SEC_EOF'
**Backend Framework Security:**

1. **Request Validation**
   - Validate all input
   - Sanitize data
   - Use schema validation (Zod, Joi)

2. **Authentication**
   - Secure session management
   - Rate limiting on auth endpoints
   - Brute force protection

3. **Headers & HTTPS**
   - Use Helmet.js for security headers
   - Force HTTPS in production
   - Set secure cookie flags

BACKEND_SEC_EOF
            ;;
    esac
    
    echo "---" >> "$OUTPUT_FILE"
fi
```

### Step 4: Research Database Security

```bash
if [ -n "$DETECTED_DATABASE" ]; then
    cat >> "$OUTPUT_FILE" << DB_SEC_EOF

## $DETECTED_DATABASE Security

<!-- Web search: "$DETECTED_DATABASE security best practices" -->

### Security Checklist

- [ ] Use strong, unique passwords
- [ ] Enable authentication
- [ ] Encrypt connections (TLS/SSL)
- [ ] Regular security patches
- [ ] Principle of least privilege for users

### Common Vulnerabilities

| Risk | Prevention |
|------|------------|
| SQL Injection | Parameterized queries, ORM |
| Unauthorized access | Authentication, firewall |
| Data exposure | Encryption, access control |
| Backup theft | Encrypted backups |

### Secure Configuration

- Disable default/public access
- Use connection pooling with limits
- Enable query logging (audit)
- Regular backup testing

---

DB_SEC_EOF
fi
```

### Step 5: Add OWASP Reference

```bash
cat >> "$OUTPUT_FILE" << 'OWASP_EOF'

## OWASP Top 10 Reference

The most critical web application security risks:

| # | Risk | Key Prevention |
|---|------|----------------|
| 1 | Broken Access Control | Deny by default, validate permissions |
| 2 | Cryptographic Failures | Encrypt sensitive data, strong algorithms |
| 3 | Injection | Input validation, parameterized queries |
| 4 | Insecure Design | Threat modeling, secure patterns |
| 5 | Security Misconfiguration | Hardened configs, remove defaults |
| 6 | Vulnerable Components | Regular updates, dependency scanning |
| 7 | Auth Failures | MFA, rate limiting, secure sessions |
| 8 | Data Integrity Failures | Digital signatures, CI/CD security |
| 9 | Logging Failures | Comprehensive logging, monitoring |
| 10 | SSRF | Validate URLs, network segmentation |

---

OWASP_EOF
```

### Step 6: Add Severity Legend and Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Severity Levels

| Level | Description | Action |
|-------|-------------|--------|
| ðŸ”´ **CRITICAL** | Actively exploited, patch immediately | Stop and fix now |
| ðŸŸ  **HIGH** | Significant risk, patch soon | Fix within 24-48 hours |
| ðŸŸ¡ **MEDIUM** | Moderate risk | Fix within 1 week |
| ðŸŸ¢ **LOW** | Minor risk | Fix in next release |

## Recommended Actions

1. **Immediate**: Run security audit tool for your language
2. **Weekly**: Review security advisories for your dependencies
3. **Monthly**: Update dependencies to latest stable versions
4. **Quarterly**: Perform security review/penetration testing

## Resources

- [OWASP](https://owasp.org)
- [CVE Database](https://cve.mitre.org)
- [GitHub Security Advisories](https://github.com/advisories)
- [Snyk Vulnerability Database](https://snyk.io/vuln/)

---

*Generated by Geist Adaptive Questionnaire System*
*Always verify security information with official sources*

FOOTER_EOF

echo "   âœ“ Security notes saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Must clearly indicate severity of issues
- Should provide actionable remediation steps
- Must recommend official security scanning tools
- Should link to authoritative sources
- Must emphasize verification with real-time tools

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}security,"
fi

# 3. Stack Patterns (standard+)
if [ "$DO_STACK_PATTERNS" = "true" ]; then
    echo ""
    echo "ðŸ—ï¸ Researching architecture patterns..."
    # Workflow: Research Stack Patterns

## Purpose

Research architecture patterns, project structure conventions, and testing strategies for the detected tech stack combination.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework
- `DETECTED_BACKEND` - Backend technology
- `DETECTED_DATABASE` - Database

## Outputs

- `agent-os/config/enriched-knowledge/stack-best-practices.md`

---

## Web Search Queries

### Query Templates

```
1. "[frontend] [backend] architecture patterns"
2. "[stack] project structure best practices"
3. "[stack] folder organization"
4. "[stack] testing strategy"
5. "[frontend] [backend] full stack patterns"
```

---

## Workflow

### Step 1: Build Stack String

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/stack-best-practices.md"

# Build a stack description string
STACK_PARTS=""
[ -n "$DETECTED_FRAMEWORK" ] && STACK_PARTS="$DETECTED_FRAMEWORK"
[ -n "$DETECTED_BACKEND" ] && STACK_PARTS="$STACK_PARTS $DETECTED_BACKEND"
[ -n "$DETECTED_DATABASE" ] && STACK_PARTS="$STACK_PARTS $DETECTED_DATABASE"
STACK_STRING=$(echo "$STACK_PARTS" | xargs)  # Trim whitespace

echo "   Researching patterns for: $STACK_STRING"
```

### Step 2: Initialize Output File

```bash
cat > "$OUTPUT_FILE" << HEADER_EOF
# Tech Stack Best Practices

> Architecture patterns and recommendations for your tech stack
> Generated: $(date -Iseconds)

**Detected Stack:** $STACK_STRING

---

HEADER_EOF
```

### Step 3: Research Architecture Patterns

```bash
cat >> "$OUTPUT_FILE" << 'ARCH_EOF'

## Recommended Architecture

<!-- Web search: "[stack] architecture patterns [year]" -->

### Overview

Based on your tech stack, consider these architectural approaches:

### Layer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Presentation Layer            â”‚
â”‚    (UI Components, Views, Templates)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Application Layer             â”‚
â”‚   (Controllers, Services, Use Cases)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚             Domain Layer                â”‚
â”‚    (Business Logic, Entities, Rules)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          Infrastructure Layer           â”‚
â”‚  (Database, External APIs, File System) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Principles

1. **Separation of Concerns** - Each layer has a distinct responsibility
2. **Dependency Inversion** - High-level modules don't depend on low-level modules
3. **Single Responsibility** - Each component does one thing well
4. **Open/Closed** - Open for extension, closed for modification

---

ARCH_EOF
```

### Step 4: Research Project Structure

```bash
cat >> "$OUTPUT_FILE" << 'STRUCTURE_EOF'

## Recommended Project Structure

<!-- Web search: "[stack] project structure best practices" -->

### General Structure

```
project-root/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/     # UI components (if applicable)
â”‚   â”œâ”€â”€ services/       # Business logic services
â”‚   â”œâ”€â”€ models/         # Data models/entities
â”‚   â”œâ”€â”€ utils/          # Utility functions
â”‚   â”œâ”€â”€ config/         # Configuration files
â”‚   â””â”€â”€ index.ts        # Entry point
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/           # Unit tests
â”‚   â”œâ”€â”€ integration/    # Integration tests
â”‚   â””â”€â”€ e2e/            # End-to-end tests
â”œâ”€â”€ docs/               # Documentation
â”œâ”€â”€ scripts/            # Build/deploy scripts
â””â”€â”€ config files        # package.json, tsconfig, etc.
```

### Naming Conventions

- **Files**: `kebab-case.ts` or `PascalCase.tsx` for components
- **Directories**: `kebab-case/`
- **Tests**: `*.test.ts` or `*.spec.ts`
- **Types/Interfaces**: `PascalCase`

### Module Organization

- Group by feature, not by type (feature-first)
- Keep related files close together
- Limit folder nesting (max 3-4 levels)
- Use barrel exports (index.ts) for clean imports

---

STRUCTURE_EOF
```

### Step 5: Research Testing Strategy

```bash
cat >> "$OUTPUT_FILE" << 'TESTING_EOF'

## Testing Strategy

<!-- Web search: "[stack] testing strategy" -->

### Test Pyramid

```
         /\
        /  \
       / E2E\        <- Few, slow, expensive
      /â”€â”€â”€â”€â”€â”€\
     /  Int.  \      <- Some, medium speed
    /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
   /    Unit    \    <- Many, fast, cheap
  /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
```

### Recommended Coverage

| Layer | Coverage Target | Focus |
|-------|----------------|-------|
| Unit | 80%+ | Business logic, utilities |
| Integration | 60%+ | API endpoints, services |
| E2E | Critical paths | User journeys |

### Testing Best Practices

1. **Write tests first** (TDD) for complex logic
2. **Test behavior, not implementation**
3. **Use meaningful test names** that describe the scenario
4. **Keep tests independent** - no shared state
5. **Mock external dependencies** in unit tests
6. **Use factories** for test data creation

### Recommended Tools

- **Unit Testing**: Jest, Vitest, pytest, cargo test
- **Integration Testing**: Supertest, pytest, integration frameworks
- **E2E Testing**: Playwright, Cypress, Selenium
- **Mocking**: MSW, unittest.mock, mockall

---

TESTING_EOF
```

### Step 6: Research Data Flow Patterns

```bash
cat >> "$OUTPUT_FILE" << 'DATAFLOW_EOF'

## Data Flow Patterns

<!-- Web search: "[stack] state management patterns" -->

### Frontend State Management

For complex applications, consider:

1. **Local State** - Component-level state for UI
2. **Server State** - Data from API (use React Query, SWR, etc.)
3. **Global State** - Shared across components (Context, Redux, Zustand)

### API Design Patterns

- **REST** - Resource-based, stateless, cacheable
- **GraphQL** - Query language, single endpoint, typed
- **tRPC** - End-to-end type safety for TypeScript

### Data Fetching Strategies

1. **Server-Side Rendering (SSR)** - SEO, initial load
2. **Static Site Generation (SSG)** - Performance, caching
3. **Client-Side Rendering (CSR)** - Interactivity
4. **Incremental Static Regeneration (ISR)** - Best of both

---

DATAFLOW_EOF
```

### Step 7: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Implementation Checklist

Before implementing, verify:

- [ ] Architecture aligns with team expertise
- [ ] Project structure supports scalability
- [ ] Testing strategy covers critical paths
- [ ] Data flow patterns match requirements
- [ ] Security considerations addressed

## Sources

Patterns compiled from:
- Official framework documentation
- Community best practices
- Industry standards (Clean Architecture, DDD)
- Real-world project examples

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Stack patterns saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Patterns should be practical, not theoretical
- Should adapt recommendations to detected stack
- Must provide concrete examples where possible
- Should highlight trade-offs for different approaches

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}patterns,"
fi

# 4. Domain Research (comprehensive only)
if [ "$DO_DOMAIN_RESEARCH" = "true" ]; then
    echo ""
    echo "ðŸŽ¯ Researching domain-specific knowledge..."
    # Workflow: Research Domain

## Purpose

Research domain-specific patterns, compliance requirements, and industry best practices based on the detected project type.

## Inputs

- `PROJECT_TYPE` - web_app, cli, api, library, monorepo
- `DETECTED_FRAMEWORK` - For context
- `SECURITY_LEVEL` - Informs compliance research depth

## Outputs

- `agent-os/config/enriched-knowledge/domain-knowledge.md`

---

## Web Search Queries

### Query Templates

```
1. "[project_type] software architecture patterns"
2. "[project_type] industry best practices"
3. "[project_type] compliance requirements"
4. "[project_type] common challenges"
5. "[project_type] scalability patterns"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/domain-knowledge.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Domain Knowledge

> Industry-specific patterns and considerations
> Generated: $(date -Iseconds)

**Project Type:** ${PROJECT_TYPE:-unknown}

---

HEADER_EOF
```

### Step 2: Research Based on Project Type

```bash
case "${PROJECT_TYPE:-unknown}" in
    "web_app")
        cat >> "$OUTPUT_FILE" << 'WEBAPP_EOF'

## Web Application Patterns

<!-- Web search: "web application architecture patterns [year]" -->

### Common Architectures

1. **Single Page Application (SPA)**
   - Rich client-side interactivity
   - API-driven data fetching
   - Client-side routing
   
2. **Server-Side Rendered (SSR)**
   - Better SEO
   - Faster initial load
   - Server load considerations

3. **Hybrid (SSR + SPA)**
   - Best of both worlds
   - Complexity trade-off
   - Popular in modern frameworks

### User Experience Considerations

- **Performance**: First Contentful Paint < 1.8s
- **Accessibility**: WCAG 2.1 compliance
- **Responsive Design**: Mobile-first approach
- **Progressive Enhancement**: Works without JS

### Security Requirements

- **Authentication**: Secure login flows
- **Authorization**: Role-based access control
- **Data Protection**: HTTPS, CSP headers
- **Input Validation**: Client and server-side

---

WEBAPP_EOF
        ;;
        
    "api")
        cat >> "$OUTPUT_FILE" << 'API_EOF'

## API Service Patterns

<!-- Web search: "API design best practices [year]" -->

### API Design Principles

1. **RESTful Design**
   - Resource-oriented URLs
   - HTTP methods for operations
   - Stateless communication
   
2. **Versioning Strategy**
   - URL versioning: `/api/v1/`
   - Header versioning
   - Query parameter versioning

3. **Error Handling**
   - Consistent error format
   - Meaningful status codes
   - Detailed error messages (dev only)

### Performance Patterns

- **Pagination**: Cursor-based for large datasets
- **Caching**: ETags, Cache-Control headers
- **Rate Limiting**: Protect against abuse
- **Compression**: gzip/brotli responses

### Documentation

- OpenAPI/Swagger specification
- Interactive documentation
- Code examples
- Versioned docs

---

API_EOF
        ;;
        
    "cli")
        cat >> "$OUTPUT_FILE" << 'CLI_EOF'

## CLI Tool Patterns

<!-- Web search: "CLI application best practices" -->

### CLI Design Principles

1. **User Experience**
   - Clear help text (`--help`)
   - Intuitive command structure
   - Meaningful error messages
   - Progress indicators for long operations

2. **Command Structure**
   ```
   tool <command> [subcommand] [options] [arguments]
   ```

3. **Configuration**
   - Config file support
   - Environment variables
   - Command-line flags (highest priority)

### Best Practices

- Follow POSIX conventions
- Support piping and redirection
- Provide quiet (`-q`) and verbose (`-v`) modes
- Exit with appropriate codes
- Support both short (`-h`) and long (`--help`) flags

### Distribution

- Single binary if possible
- Package manager support (npm, cargo, brew)
- Auto-update mechanism
- Cross-platform builds

---

CLI_EOF
        ;;
        
    "library")
        cat >> "$OUTPUT_FILE" << 'LIB_EOF'

## Library Design Patterns

<!-- Web search: "library design best practices" -->

### API Design

1. **Minimal Surface Area**
   - Expose only what's needed
   - Internal vs. public APIs
   - Deprecation strategy

2. **Consistency**
   - Naming conventions
   - Error handling patterns
   - Return value patterns

3. **Extensibility**
   - Plugin/middleware support
   - Configuration options
   - Hooks for customization

### Documentation

- Comprehensive README
- API documentation
- Usage examples
- Migration guides
- Changelog

### Versioning

- Semantic versioning (SemVer)
- Clear breaking change policy
- Deprecation warnings
- LTS versions for stability

---

LIB_EOF
        ;;
        
    *)
        cat >> "$OUTPUT_FILE" << 'DEFAULT_EOF'

## General Software Patterns

### Universal Best Practices

1. **Code Quality**
   - Consistent formatting
   - Meaningful naming
   - Documentation
   - Testing

2. **Architecture**
   - Separation of concerns
   - Dependency management
   - Configuration management
   - Error handling

3. **Operations**
   - Logging and monitoring
   - Health checks
   - Graceful shutdown
   - Configuration via environment

---

DEFAULT_EOF
        ;;
esac
```

### Step 3: Research Compliance Requirements

```bash
if [ "$SECURITY_LEVEL" = "high" ]; then
    cat >> "$OUTPUT_FILE" << 'COMPLIANCE_EOF'

## Compliance Considerations

<!-- Web search: "software compliance requirements [year]" -->

### Common Compliance Frameworks

| Framework | Focus | Key Requirements |
|-----------|-------|------------------|
| **GDPR** | Data Privacy (EU) | Consent, data rights, breach notification |
| **SOC 2** | Security Controls | Access control, encryption, monitoring |
| **HIPAA** | Healthcare Data | PHI protection, access logs, encryption |
| **PCI-DSS** | Payment Data | Cardholder data protection, network security |

### General Compliance Checklist

- [ ] Data encryption at rest and in transit
- [ ] Access control and authentication
- [ ] Audit logging
- [ ] Data retention policies
- [ ] Incident response plan
- [ ] Regular security assessments
- [ ] Privacy policy and terms of service

### Security Controls

1. **Authentication**
   - Multi-factor authentication
   - Session management
   - Password policies

2. **Authorization**
   - Principle of least privilege
   - Role-based access control
   - Resource-level permissions

3. **Data Protection**
   - Encryption standards (AES-256)
   - Key management
   - Secure deletion

---

COMPLIANCE_EOF
fi
```

### Step 4: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Implementation Priority

Based on your project type, prioritize:

1. **Core Functionality** - Get the basics right first
2. **Security** - Build security in from the start
3. **Performance** - Optimize critical paths
4. **Scalability** - Design for growth
5. **Maintainability** - Think long-term

## Sources

Domain knowledge compiled from:
- Industry standards and frameworks
- Community best practices
- Regulatory requirements
- Real-world case studies

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Domain knowledge saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Research should be specific to detected project type
- Compliance info should match security level
- Should provide actionable recommendations
- Must include implementation priorities

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}domain,"
fi

# 5. Version Analysis (always)
echo ""
echo "ðŸ“¦ Analyzing dependency versions..."
# Workflow: Version Analysis

## Purpose

Compare detected dependency versions against latest stable versions, flag outdated dependencies, and note breaking changes in newer versions.

## Inputs

- `package.json` (Node.js)
- `Cargo.toml` (Rust)
- `go.mod` (Go)
- `requirements.txt` / `pyproject.toml` (Python)

## Outputs

- `agent-os/config/enriched-knowledge/version-analysis.md`

---

## Web Search Queries

### Query Templates

```
1. "[package] latest version"
2. "[package] changelog breaking changes"
3. "[package] migration guide v[old] to v[new]"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/version-analysis.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Version Analysis

> Dependency version status and update recommendations
> Generated: $(date -Iseconds)

This analysis compares your current dependency versions against the latest
stable releases and flags potential updates.

---

## Summary

HEADER_EOF
```

### Step 2: Analyze Node.js Dependencies

```bash
if [ -f "package.json" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Node.js Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`npm outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Package | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|---------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Extract key dependencies (simplified - in practice, use npm outdated)
    # This is a template showing the expected output format
    
    # Check for common frameworks and their typical update status
    if grep -q '"react"' package.json 2>/dev/null; then
        REACT_VER=$(grep '"react"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        if [ "${REACT_VER%%.*}" -lt "18" ] 2>/dev/null; then
            echo "| react | ${REACT_VER:-unknown} | âš ï¸ OUTDATED | Consider upgrading to React 18 |" >> "$OUTPUT_FILE"
        else
            echo "| react | ${REACT_VER:-unknown} | âœ… Current | |" >> "$OUTPUT_FILE"
        fi
    fi
    
    if grep -q '"next"' package.json 2>/dev/null; then
        NEXT_VER=$(grep '"next"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| next | ${NEXT_VER:-unknown} | â„¹ï¸ Check | Major versions may have breaking changes |" >> "$OUTPUT_FILE"
    fi
    
    if grep -q '"typescript"' package.json 2>/dev/null; then
        TS_VER=$(grep '"typescript"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| typescript | ${TS_VER:-unknown} | â„¹ï¸ Check | TypeScript 5.x has new features |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check all outdated packages" >> "$OUTPUT_FILE"
    echo "npm outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all packages (minor/patch)" >> "$OUTPUT_FILE"
    echo "npm update" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Interactive update (recommended)" >> "$OUTPUT_FILE"
    echo "npx npm-check-updates -i" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 3: Analyze Rust Dependencies

```bash
if [ -f "Cargo.toml" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Rust Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`cargo outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Crate | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|-------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check for common crates
    if grep -q 'tokio' Cargo.toml 2>/dev/null; then
        TOKIO_VER=$(grep 'tokio' Cargo.toml | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| tokio | ${TOKIO_VER:-unknown} | â„¹ï¸ Check | Async runtime |" >> "$OUTPUT_FILE"
    fi
    
    if grep -q 'serde' Cargo.toml 2>/dev/null; then
        SERDE_VER=$(grep 'serde' Cargo.toml | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| serde | ${SERDE_VER:-unknown} | â„¹ï¸ Check | Serialization |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Install cargo-outdated" >> "$OUTPUT_FILE"
    echo "cargo install cargo-outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Check outdated dependencies" >> "$OUTPUT_FILE"
    echo "cargo outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update dependencies" >> "$OUTPUT_FILE"
    echo "cargo update" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 4: Analyze Python Dependencies

```bash
if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Python Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`pip list --outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Package | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|---------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check for common packages
    PYDEPS=""
    [ -f "requirements.txt" ] && PYDEPS=$(cat requirements.txt)
    
    if echo "$PYDEPS" | grep -qi 'django'; then
        DJANGO_VER=$(echo "$PYDEPS" | grep -i 'django' | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| django | ${DJANGO_VER:-unknown} | â„¹ï¸ Check | Web framework |" >> "$OUTPUT_FILE"
    fi
    
    if echo "$PYDEPS" | grep -qi 'fastapi'; then
        FASTAPI_VER=$(echo "$PYDEPS" | grep -i 'fastapi' | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| fastapi | ${FASTAPI_VER:-unknown} | â„¹ï¸ Check | API framework |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check outdated packages" >> "$OUTPUT_FILE"
    echo "pip list --outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all packages" >> "$OUTPUT_FILE"
    echo "pip install --upgrade -r requirements.txt" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Use pip-tools for better dependency management" >> "$OUTPUT_FILE"
    echo "pip install pip-tools" >> "$OUTPUT_FILE"
    echo "pip-compile --upgrade" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 5: Analyze Go Dependencies

```bash
if [ -f "go.mod" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Go Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`go list -m -u all\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Module | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|--------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check Go version
    GO_VER=$(grep "^go " go.mod | awk '{print $2}')
    echo "| go (runtime) | ${GO_VER:-unknown} | â„¹ï¸ Check | Go version |" >> "$OUTPUT_FILE"
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check for available updates" >> "$OUTPUT_FILE"
    echo "go list -m -u all" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all dependencies" >> "$OUTPUT_FILE"
    echo "go get -u ./..." >> "$OUTPUT_FILE"
    echo "go mod tidy" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 6: Add Update Strategy

```bash
cat >> "$OUTPUT_FILE" << 'STRATEGY_EOF'

---

## Update Strategy

### Version Status Legend

| Status | Meaning | Action |
|--------|---------|--------|
| âœ… Current | Latest stable version | No action needed |
| â„¹ï¸ Check | Unknown/needs verification | Run version check tool |
| âš ï¸ OUTDATED | Behind latest stable | Plan update |
| ðŸ”´ CRITICAL | Security patch available | Update immediately |

### Safe Update Process

1. **Check current versions**
   ```bash
   # Use appropriate tool for your language
   npm outdated / cargo outdated / pip list --outdated
   ```

2. **Review changelogs**
   - Check for breaking changes
   - Review migration guides
   - Note deprecated features

3. **Update in stages**
   - Patch versions first (x.x.PATCH)
   - Then minor versions (x.MINOR.x)
   - Major versions last (MAJOR.x.x)

4. **Test thoroughly**
   - Run test suite after each update
   - Check critical paths manually
   - Monitor for regressions

### When to Update

| Situation | Recommended Action |
|-----------|-------------------|
| Security vulnerability | Update immediately |
| Major version behind | Plan migration sprint |
| Minor version behind | Update during maintenance |
| Patch version behind | Update with next release |

---

STRATEGY_EOF
```

### Step 7: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Automated Version Checking

For real-time, accurate version information, use these tools:

| Language | Tool | Command |
|----------|------|---------|
| Node.js | npm | `npm outdated` |
| Node.js | npx | `npx npm-check-updates` |
| Rust | cargo | `cargo outdated` |
| Python | pip | `pip list --outdated` |
| Go | go | `go list -m -u all` |

### CI Integration

Consider adding version checking to your CI pipeline:

```yaml
# Example GitHub Action
- name: Check for outdated dependencies
  run: npm outdated || true  # Don't fail build
```

---

*Generated by Geist Adaptive Questionnaire System*
*Run the appropriate version check tool for accurate, real-time information*

FOOTER_EOF

echo "   âœ“ Version analysis saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Must recommend using real-time tools for accuracy
- Should not make definitive version claims without verification
- Must provide safe update strategy
- Should distinguish between critical and non-critical updates
- Must include commands for each language ecosystem

RESEARCH_COMPLETED="${RESEARCH_COMPLETED}versions,"
```

### Step 4: Synthesize Knowledge

```bash
echo ""
echo "ðŸ”„ Synthesizing research findings..."
# Workflow: Synthesize Knowledge

## Purpose

Combine all research outputs, remove duplicates, prioritize actionable insights, and create a unified summary for easy consumption.

## Inputs

Research files from:
- `agent-os/config/enriched-knowledge/library-research.md`
- `agent-os/config/enriched-knowledge/stack-best-practices.md`
- `agent-os/config/enriched-knowledge/domain-knowledge.md`
- `agent-os/config/enriched-knowledge/version-analysis.md`
- `agent-os/config/enriched-knowledge/security-notes.md`

## Outputs

- Updates to individual research files (deduplication)
- `agent-os/config/enriched-knowledge/README.md` - Summary index

---

## Workflow

### Step 1: Create Summary Index

```bash
KNOWLEDGE_DIR="agent-os/config/enriched-knowledge"
SUMMARY_FILE="$KNOWLEDGE_DIR/README.md"

cat > "$SUMMARY_FILE" << HEADER_EOF
# Enriched Knowledge Index

> Consolidated research findings for your project
> Generated: $(date -Iseconds)

This directory contains research gathered during the adaptive questionnaire
process. Use this knowledge to inform your development decisions.

---

## Available Research

HEADER_EOF
```

### Step 2: Index Available Files

```bash
echo "### Research Files" >> "$SUMMARY_FILE"
echo "" >> "$SUMMARY_FILE"

# Check each expected file and add to index
if [ -f "$KNOWLEDGE_DIR/library-research.md" ]; then
    echo "- ðŸ“š [Library Research](library-research.md) - Best practices for your dependencies" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/stack-best-practices.md" ]; then
    echo "- ðŸ—ï¸ [Stack Best Practices](stack-best-practices.md) - Architecture patterns for your tech stack" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/domain-knowledge.md" ]; then
    echo "- ðŸŽ¯ [Domain Knowledge](domain-knowledge.md) - Industry-specific patterns" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/version-analysis.md" ]; then
    echo "- ðŸ“¦ [Version Analysis](version-analysis.md) - Dependency version status" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/security-notes.md" ]; then
    echo "- ðŸ”’ [Security Notes](security-notes.md) - Security considerations and CVEs" >> "$SUMMARY_FILE"
fi

echo "" >> "$SUMMARY_FILE"
```

### Step 3: Extract Key Insights

```bash
cat >> "$SUMMARY_FILE" << 'INSIGHTS_HEADER'

---

## Key Insights Summary

### ðŸ”´ Critical Items

Items requiring immediate attention:

INSIGHTS_HEADER

# Check for critical security issues
if [ -f "$KNOWLEDGE_DIR/security-notes.md" ]; then
    if grep -q "CRITICAL\|ðŸ”´" "$KNOWLEDGE_DIR/security-notes.md" 2>/dev/null; then
        echo "- âš ï¸ Critical security issues found - see [Security Notes](security-notes.md)" >> "$SUMMARY_FILE"
    else
        echo "- âœ… No critical security issues detected" >> "$SUMMARY_FILE"
    fi
fi

# Check for outdated dependencies
if [ -f "$KNOWLEDGE_DIR/version-analysis.md" ]; then
    if grep -q "OUTDATED\|Major update" "$KNOWLEDGE_DIR/version-analysis.md" 2>/dev/null; then
        echo "- âš ï¸ Outdated dependencies found - see [Version Analysis](version-analysis.md)" >> "$SUMMARY_FILE"
    else
        echo "- âœ… Dependencies are up to date" >> "$SUMMARY_FILE"
    fi
fi

echo "" >> "$SUMMARY_FILE"
```

### Step 4: Add Quick Reference

```bash
cat >> "$SUMMARY_FILE" << 'QUICKREF_EOF'

### ðŸ“‹ Quick Reference

| Area | Document | When to Use |
|------|----------|-------------|
| Starting a new feature | Stack Best Practices | Architecture decisions |
| Adding dependencies | Library Research | Evaluate libraries |
| Security review | Security Notes | Before deployment |
| Updating packages | Version Analysis | Maintenance cycles |
| Domain questions | Domain Knowledge | Business logic |

---

QUICKREF_EOF
```

### Step 5: Add Usage Instructions

```bash
cat >> "$SUMMARY_FILE" << 'USAGE_EOF'

## How to Use This Knowledge

### During Development

1. **Before implementing a feature**: Check Stack Best Practices for patterns
2. **When choosing libraries**: Review Library Research for recommendations
3. **During code review**: Reference Security Notes for secure coding
4. **When debugging**: Check Library Research for known issues

### During Maintenance

1. **Regular updates**: Use Version Analysis to prioritize updates
2. **Security patches**: Follow Security Notes recommendations
3. **Refactoring**: Reference Stack Best Practices for patterns

### During Planning

1. **Architecture decisions**: Stack Best Practices + Domain Knowledge
2. **Compliance requirements**: Domain Knowledge + Security Notes
3. **Technology choices**: Library Research + Version Analysis

---

USAGE_EOF
```

### Step 6: Add Refresh Instructions

```bash
cat >> "$SUMMARY_FILE" << 'REFRESH_EOF'

## Keeping Knowledge Fresh

This research was generated at a point in time. To refresh:

1. **Re-run detection**: This will update research with latest information
2. **Manual update**: Edit files directly for project-specific notes
3. **Research depth**: Adjust depth for more/less detail

### Research Depth Levels

| Level | Time | Best For |
|-------|------|----------|
| `minimal` | ~30s | Quick updates |
| `standard` | ~2m | Regular use |
| `comprehensive` | ~5m | Major decisions |

To change depth:
```bash
RESEARCH_DEPTH=comprehensive
# Then re-run adapt-to-product or the research orchestrator
```

---

## Files in This Directory

REFRESH_EOF

# List all files with sizes
ls -lh "$KNOWLEDGE_DIR"/*.md 2>/dev/null | awk '{print "- `" $NF "` (" $5 ")"}' >> "$SUMMARY_FILE" || echo "- No files found" >> "$SUMMARY_FILE"

cat >> "$SUMMARY_FILE" << 'FOOTER_EOF'

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Knowledge synthesis complete"
echo "   âœ“ Summary index saved to $SUMMARY_FILE"
```

---

## Deduplication Logic

When synthesizing, the workflow identifies and consolidates:

1. **Repeated recommendations** across files
2. **Conflicting advice** (flags for human review)
3. **Version-specific information** (keeps most recent)
4. **Source attribution** (maintains for verification)

---

## Important Constraints

- Must maintain source attribution
- Should flag conflicting recommendations
- Must create navigable index
- Should highlight critical items prominently
- Must provide clear usage guidance

```

### Step 5: Generate Research Summary

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  RESEARCH COMPLETE"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ðŸ“ Enriched knowledge saved to: agent-os/config/enriched-knowledge/"
echo ""
echo "Files generated:"
ls -la agent-os/config/enriched-knowledge/ 2>/dev/null | grep ".md" | awk '{print "   â€¢ " $NF}'
echo ""
echo "Research areas completed: $(echo $RESEARCH_COMPLETED | sed 's/,$//' | tr ',' ', ')"
echo ""
```

---

## Integration

This workflow is called by:
- `adapt-to-product/1-setup-and-information-gathering.md` (after detection)
- `create-basepoints/1-validate-prerequisites.md` (for architecture research)

The enriched knowledge is used by:
- `deploy-agents` for specialization
- Validation workflows for security checks
- Human review for flagging issues

---

## Research Depth Guidelines

| Depth | Time | Use Case |
|-------|------|----------|
| `minimal` | ~30s | Quick setup, simple projects |
| `standard` | ~2m | Most projects (default) |
| `comprehensive` | ~5m | Enterprise, complex projects |

---

## Important Constraints

- Must handle web search failures gracefully
- Should cache results to avoid redundant searches
- Must attribute sources in output
- Should prioritize actionable insights over raw data
` - To enrich with web research

---

## Important Constraints

- Must handle missing files gracefully (no errors)
- Must provide sensible defaults when detection fails
- Must flag low-confidence detections for user questions
- Must be idempotent (can run multiple times safely)

```

This will:
- Detect language, framework, and database from config files
- Extract build/test/lint commands from package.json, Makefile, etc.
- Analyze directory structure for project type and architecture
- Check for security indicators (auth, secrets management)
- Calculate overall detection confidence

### 0.2: Enrich with Web Research

Research best practices and patterns for the detected tech stack:

```bash
# Set research depth (minimal for quick setup, standard for most projects)
RESEARCH_DEPTH="${RESEARCH_DEPTH:-standard}"

# Workflow: Research Orchestrator

## Purpose

Main orchestrator for web research. Loads detected tech stack, determines research depth, calls appropriate research workflows, and aggregates results into the enriched-knowledge directory.

## Inputs

- `agent-os/config/project-profile.yml` - Detected project profile
- `RESEARCH_DEPTH` - minimal, standard, or comprehensive (default: standard)

## Outputs

- `agent-os/config/enriched-knowledge/` directory with research results

---

## Workflow

### Step 1: Load Project Profile

```bash
echo "ðŸ”¬ Starting knowledge enrichment research..."
echo ""

# Create enriched-knowledge directory
mkdir -p agent-os/config/enriched-knowledge

# Load project profile
if [ -f "agent-os/config/project-profile.yml" ]; then
    echo "ðŸ“‚ Loading project profile..."
    
    # Extract key values (simplified parsing)
    DETECTED_LANGUAGE=$(grep "language:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    DETECTED_FRAMEWORK=$(grep "framework:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    DETECTED_DATABASE=$(grep "database:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    PROJECT_TYPE=$(grep "project_type:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    SECURITY_LEVEL=$(grep "security_level:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    
    echo "   Language: $DETECTED_LANGUAGE"
    echo "   Framework: ${DETECTED_FRAMEWORK:-(none)}"
    echo "   Database: ${DETECTED_DATABASE:-(none)}"
    echo "   Project Type: $PROJECT_TYPE"
else
    echo "âš ï¸ No project profile found. Run detection first."
    echo "   Using defaults..."
    DETECTED_LANGUAGE="unknown"
fi
```

### Step 2: Determine Research Depth

```bash
# Set research depth (can be overridden)
RESEARCH_DEPTH="${RESEARCH_DEPTH:-standard}"

echo ""
echo "ðŸ“Š Research depth: $RESEARCH_DEPTH"
echo ""

# Define what each depth includes
case "$RESEARCH_DEPTH" in
    "minimal")
        echo "   â€¢ Latest versions"
        echo "   â€¢ Critical security issues"
        echo "   Estimated time: ~30 seconds"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=false
        DO_DOMAIN_RESEARCH=false
        ;;
    "standard")
        echo "   â€¢ Latest versions"
        echo "   â€¢ Security issues"
        echo "   â€¢ Best practices"
        echo "   â€¢ Common pitfalls"
        echo "   Estimated time: ~2 minutes"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=true
        DO_DOMAIN_RESEARCH=false
        ;;
    "comprehensive")
        echo "   â€¢ All standard research"
        echo "   â€¢ Architecture patterns"
        echo "   â€¢ Domain knowledge"
        echo "   â€¢ Migration guides"
        echo "   Estimated time: ~5 minutes"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=true
        DO_DOMAIN_RESEARCH=true
        ;;
esac

echo ""
```

### Step 3: Execute Research Workflows

```bash
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  EXECUTING RESEARCH WORKFLOWS"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Track what was researched
RESEARCH_COMPLETED=""

# 1. Library Research (always in minimal+)
if [ "$DO_LIBRARY_RESEARCH" = "true" ]; then
    echo "ðŸ“š Researching libraries and frameworks..."
    # Workflow: Research Library

## Purpose

Research best practices, known issues, security vulnerabilities, and latest versions for each detected library/framework. Compiles findings into structured markdown.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework (if any)
- `DETECTED_DATABASE` - Database (if any)

## Outputs

- `agent-os/config/enriched-knowledge/library-research.md`

---

## Web Search Queries

For each detected technology, perform the following searches:

### Query Templates

```
1. "[library] best practices [current_year]"
2. "[library] common mistakes to avoid"
3. "[library] known issues bugs"
4. "[library] latest stable version"
5. "[library] security vulnerabilities CVE"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/library-research.md"

cat > "$OUTPUT_FILE" << 'HEADER_EOF'
# Library Research

> Auto-generated knowledge enrichment from web research
> Generated: $(date -Iseconds)

This document contains best practices, known issues, and recommendations
for the libraries and frameworks detected in your project.

---

HEADER_EOF
```

### Step 2: Research Primary Language

```bash
if [ -n "$DETECTED_LANGUAGE" ] && [ "$DETECTED_LANGUAGE" != "unknown" ]; then
    echo "   Researching $DETECTED_LANGUAGE..."
    
    cat >> "$OUTPUT_FILE" << LANG_EOF

## $DETECTED_LANGUAGE

### Best Practices

<!-- Web search: "$DETECTED_LANGUAGE best practices $CURRENT_YEAR" -->

**Recommended practices for $DETECTED_LANGUAGE development:**

- Follow official style guides and conventions
- Use type annotations where available
- Implement proper error handling
- Write comprehensive tests
- Use linting and formatting tools

### Common Pitfalls

<!-- Web search: "$DETECTED_LANGUAGE common mistakes to avoid" -->

**Common mistakes to avoid:**

- Ignoring error handling
- Not using proper dependency management
- Skipping type safety features
- Insufficient testing
- Poor code organization

### Resources

- Official documentation
- Community style guides
- Popular learning resources

---

LANG_EOF
fi
```

### Step 3: Research Framework

```bash
if [ -n "$DETECTED_FRAMEWORK" ] && [ "$DETECTED_FRAMEWORK" != "" ]; then
    echo "   Researching $DETECTED_FRAMEWORK..."
    
    cat >> "$OUTPUT_FILE" << FRAMEWORK_EOF

## $DETECTED_FRAMEWORK

### Best Practices

<!-- Web search: "$DETECTED_FRAMEWORK best practices $CURRENT_YEAR" -->

**Recommended practices for $DETECTED_FRAMEWORK:**

- Follow the framework's recommended project structure
- Use built-in features before reaching for third-party solutions
- Implement proper state management patterns
- Optimize for performance from the start
- Follow security guidelines

### Architecture Recommendations

<!-- Web search: "$DETECTED_FRAMEWORK architecture patterns" -->

**Recommended architecture patterns:**

- Component-based architecture (for UI frameworks)
- Clean separation of concerns
- Proper routing and navigation patterns
- Effective data fetching strategies
- Error boundary implementation

### Known Issues

<!-- Web search: "$DETECTED_FRAMEWORK known issues bugs" -->

**Common issues to be aware of:**

- Check the framework's GitHub issues for current bugs
- Review migration guides for breaking changes
- Monitor security advisories

### Performance Tips

<!-- Web search: "$DETECTED_FRAMEWORK performance optimization" -->

**Performance optimization strategies:**

- Implement lazy loading
- Use memoization appropriately
- Optimize bundle size
- Monitor and profile regularly

---

FRAMEWORK_EOF
fi
```

### Step 4: Research Database

```bash
if [ -n "$DETECTED_DATABASE" ] && [ "$DETECTED_DATABASE" != "" ]; then
    echo "   Researching $DETECTED_DATABASE..."
    
    cat >> "$OUTPUT_FILE" << DB_EOF

## $DETECTED_DATABASE

### Best Practices

<!-- Web search: "$DETECTED_DATABASE best practices $CURRENT_YEAR" -->

**Database best practices:**

- Use proper indexing strategies
- Implement connection pooling
- Follow security best practices
- Regular backup and maintenance
- Monitor query performance

### Security Considerations

<!-- Web search: "$DETECTED_DATABASE security best practices" -->

**Security recommendations:**

- Use parameterized queries (prevent SQL injection)
- Implement proper authentication
- Encrypt sensitive data
- Regular security audits
- Keep database updated

### Performance Optimization

<!-- Web search: "$DETECTED_DATABASE performance tuning" -->

**Performance tips:**

- Optimize query patterns
- Use appropriate indexes
- Monitor slow queries
- Consider caching strategies
- Regular maintenance

---

DB_EOF
fi
```

### Step 5: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

---

## How to Use This Document

1. **Review recommendations** before implementing features
2. **Check known issues** when debugging problems
3. **Follow best practices** for code quality
4. **Monitor security advisories** for your dependencies

## Sources

Research compiled from:
- Official documentation
- GitHub issues and discussions
- Stack Overflow community
- Security advisory databases
- Community best practices

---

*Generated by Geist Adaptive Questionnaire System*
*For the most current information, verify with official sources*

FOOTER_EOF

echo "   âœ“ Library research saved to $OUTPUT_FILE"
```

---

## Web Search Integration

When executing this workflow, the AI agent should use the `web_search` tool to gather current information:

```markdown
### Example Web Search Calls

For React framework:
- web_search("React 18 best practices 2026")
- web_search("React common mistakes to avoid")
- web_search("React known issues bugs")
- web_search("React latest stable version")

For PostgreSQL:
- web_search("PostgreSQL best practices 2026")
- web_search("PostgreSQL security hardening")
- web_search("PostgreSQL performance tuning")
```

---

## Important Constraints

- Must attribute information sources
- Should include publication dates for time-sensitive info
- Must handle missing/unknown technologies gracefully
- Should prioritize official documentation over blog posts
- Results should be actionable, not just informational

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}library,"
fi

# 2. Security Research (always in minimal+)
if [ "$DO_SECURITY_RESEARCH" = "true" ]; then
    echo ""
    echo "ðŸ”’ Researching security vulnerabilities..."
    # Workflow: Research Security

## Purpose

Research security vulnerabilities (CVEs), security advisories, and security best practices for the detected dependencies.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework
- `DETECTED_DATABASE` - Database
- Dependencies from package.json, Cargo.toml, etc.

## Outputs

- `agent-os/config/enriched-knowledge/security-notes.md`

---

## Web Search Queries

### Query Templates

```
1. "[dependency] CVE vulnerabilities [year]"
2. "[dependency] security advisory"
3. "[framework] security best practices"
4. "[language] OWASP top 10 prevention"
5. "[dependency] security patch"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/security-notes.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Security Notes

> Security vulnerabilities and recommendations for your dependencies
> Generated: $(date -Iseconds)

âš ï¸ **Important**: Always verify security information with official sources.
Check npm audit, cargo audit, or pip-audit for real-time vulnerability scanning.

---

HEADER_EOF
```

### Step 2: Research Language Security

```bash
if [ -n "$DETECTED_LANGUAGE" ] && [ "$DETECTED_LANGUAGE" != "unknown" ]; then
    cat >> "$OUTPUT_FILE" << LANG_SEC_EOF

## $DETECTED_LANGUAGE Security

<!-- Web search: "$DETECTED_LANGUAGE security best practices $CURRENT_YEAR" -->

### Common Vulnerabilities

| Vulnerability | Risk | Prevention |
|---------------|------|------------|
| Injection attacks | High | Input validation, parameterized queries |
| XSS (if web) | High | Output encoding, CSP headers |
| Insecure dependencies | Medium | Regular audits, version pinning |
| Secrets exposure | Critical | Environment variables, secret managers |

### Security Tools

LANG_SEC_EOF

    case "$DETECTED_LANGUAGE" in
        "javascript"|"typescript")
            cat >> "$OUTPUT_FILE" << 'JSTOOLS_EOF'
- **npm audit** - Check for vulnerable dependencies
- **Snyk** - Continuous vulnerability monitoring
- **ESLint security plugins** - Static code analysis
- **Helmet** - Security headers for Express

```bash
# Run security audit
npm audit
npm audit fix

# Use Snyk
npx snyk test
```

JSTOOLS_EOF
            ;;
        "rust")
            cat >> "$OUTPUT_FILE" << 'RSTOOLS_EOF'
- **cargo audit** - Check for vulnerable dependencies
- **cargo deny** - Lint dependencies
- **clippy** - Catch common mistakes

```bash
# Run security audit
cargo audit

# Install if needed
cargo install cargo-audit
```

RSTOOLS_EOF
            ;;
        "python")
            cat >> "$OUTPUT_FILE" << 'PYTOOLS_EOF'
- **pip-audit** - Check for vulnerable dependencies
- **safety** - Check dependencies against safety db
- **bandit** - Security linter

```bash
# Run security audit
pip-audit

# Or use safety
safety check
```

PYTOOLS_EOF
            ;;
        "go")
            cat >> "$OUTPUT_FILE" << 'GOTOOLS_EOF'
- **govulncheck** - Official Go vulnerability scanner
- **gosec** - Security linter

```bash
# Run vulnerability check
govulncheck ./...

# Install if needed
go install golang.org/x/vuln/cmd/govulncheck@latest
```

GOTOOLS_EOF
            ;;
    esac
    
    echo "---" >> "$OUTPUT_FILE"
fi
```

### Step 3: Research Framework Security

```bash
if [ -n "$DETECTED_FRAMEWORK" ]; then
    cat >> "$OUTPUT_FILE" << FRAMEWORK_SEC_EOF

## $DETECTED_FRAMEWORK Security

<!-- Web search: "$DETECTED_FRAMEWORK security vulnerabilities CVE" -->

### Security Checklist

- [ ] Keep framework updated to latest stable version
- [ ] Review security advisories regularly
- [ ] Follow framework's security guidelines
- [ ] Implement recommended security middleware
- [ ] Use built-in security features

### Known Security Considerations

FRAMEWORK_SEC_EOF

    case "$DETECTED_FRAMEWORK" in
        "react"|"vue"|"angular")
            cat >> "$OUTPUT_FILE" << 'FRONTEND_SEC_EOF'
**Frontend Framework Security:**

1. **XSS Prevention**
   - Use framework's built-in escaping
   - Avoid `dangerouslySetInnerHTML` (React) or `v-html` (Vue)
   - Sanitize user input before display

2. **CSRF Protection**
   - Use anti-CSRF tokens
   - SameSite cookie attribute
   - Verify origin headers

3. **Secure Dependencies**
   - Regular `npm audit`
   - Lock file integrity
   - Review new dependencies

FRONTEND_SEC_EOF
            ;;
        "express"|"fastify"|"koa")
            cat >> "$OUTPUT_FILE" << 'BACKEND_SEC_EOF'
**Backend Framework Security:**

1. **Request Validation**
   - Validate all input
   - Sanitize data
   - Use schema validation (Zod, Joi)

2. **Authentication**
   - Secure session management
   - Rate limiting on auth endpoints
   - Brute force protection

3. **Headers & HTTPS**
   - Use Helmet.js for security headers
   - Force HTTPS in production
   - Set secure cookie flags

BACKEND_SEC_EOF
            ;;
    esac
    
    echo "---" >> "$OUTPUT_FILE"
fi
```

### Step 4: Research Database Security

```bash
if [ -n "$DETECTED_DATABASE" ]; then
    cat >> "$OUTPUT_FILE" << DB_SEC_EOF

## $DETECTED_DATABASE Security

<!-- Web search: "$DETECTED_DATABASE security best practices" -->

### Security Checklist

- [ ] Use strong, unique passwords
- [ ] Enable authentication
- [ ] Encrypt connections (TLS/SSL)
- [ ] Regular security patches
- [ ] Principle of least privilege for users

### Common Vulnerabilities

| Risk | Prevention |
|------|------------|
| SQL Injection | Parameterized queries, ORM |
| Unauthorized access | Authentication, firewall |
| Data exposure | Encryption, access control |
| Backup theft | Encrypted backups |

### Secure Configuration

- Disable default/public access
- Use connection pooling with limits
- Enable query logging (audit)
- Regular backup testing

---

DB_SEC_EOF
fi
```

### Step 5: Add OWASP Reference

```bash
cat >> "$OUTPUT_FILE" << 'OWASP_EOF'

## OWASP Top 10 Reference

The most critical web application security risks:

| # | Risk | Key Prevention |
|---|------|----------------|
| 1 | Broken Access Control | Deny by default, validate permissions |
| 2 | Cryptographic Failures | Encrypt sensitive data, strong algorithms |
| 3 | Injection | Input validation, parameterized queries |
| 4 | Insecure Design | Threat modeling, secure patterns |
| 5 | Security Misconfiguration | Hardened configs, remove defaults |
| 6 | Vulnerable Components | Regular updates, dependency scanning |
| 7 | Auth Failures | MFA, rate limiting, secure sessions |
| 8 | Data Integrity Failures | Digital signatures, CI/CD security |
| 9 | Logging Failures | Comprehensive logging, monitoring |
| 10 | SSRF | Validate URLs, network segmentation |

---

OWASP_EOF
```

### Step 6: Add Severity Legend and Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Severity Levels

| Level | Description | Action |
|-------|-------------|--------|
| ðŸ”´ **CRITICAL** | Actively exploited, patch immediately | Stop and fix now |
| ðŸŸ  **HIGH** | Significant risk, patch soon | Fix within 24-48 hours |
| ðŸŸ¡ **MEDIUM** | Moderate risk | Fix within 1 week |
| ðŸŸ¢ **LOW** | Minor risk | Fix in next release |

## Recommended Actions

1. **Immediate**: Run security audit tool for your language
2. **Weekly**: Review security advisories for your dependencies
3. **Monthly**: Update dependencies to latest stable versions
4. **Quarterly**: Perform security review/penetration testing

## Resources

- [OWASP](https://owasp.org)
- [CVE Database](https://cve.mitre.org)
- [GitHub Security Advisories](https://github.com/advisories)
- [Snyk Vulnerability Database](https://snyk.io/vuln/)

---

*Generated by Geist Adaptive Questionnaire System*
*Always verify security information with official sources*

FOOTER_EOF

echo "   âœ“ Security notes saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Must clearly indicate severity of issues
- Should provide actionable remediation steps
- Must recommend official security scanning tools
- Should link to authoritative sources
- Must emphasize verification with real-time tools

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}security,"
fi

# 3. Stack Patterns (standard+)
if [ "$DO_STACK_PATTERNS" = "true" ]; then
    echo ""
    echo "ðŸ—ï¸ Researching architecture patterns..."
    # Workflow: Research Stack Patterns

## Purpose

Research architecture patterns, project structure conventions, and testing strategies for the detected tech stack combination.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework
- `DETECTED_BACKEND` - Backend technology
- `DETECTED_DATABASE` - Database

## Outputs

- `agent-os/config/enriched-knowledge/stack-best-practices.md`

---

## Web Search Queries

### Query Templates

```
1. "[frontend] [backend] architecture patterns"
2. "[stack] project structure best practices"
3. "[stack] folder organization"
4. "[stack] testing strategy"
5. "[frontend] [backend] full stack patterns"
```

---

## Workflow

### Step 1: Build Stack String

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/stack-best-practices.md"

# Build a stack description string
STACK_PARTS=""
[ -n "$DETECTED_FRAMEWORK" ] && STACK_PARTS="$DETECTED_FRAMEWORK"
[ -n "$DETECTED_BACKEND" ] && STACK_PARTS="$STACK_PARTS $DETECTED_BACKEND"
[ -n "$DETECTED_DATABASE" ] && STACK_PARTS="$STACK_PARTS $DETECTED_DATABASE"
STACK_STRING=$(echo "$STACK_PARTS" | xargs)  # Trim whitespace

echo "   Researching patterns for: $STACK_STRING"
```

### Step 2: Initialize Output File

```bash
cat > "$OUTPUT_FILE" << HEADER_EOF
# Tech Stack Best Practices

> Architecture patterns and recommendations for your tech stack
> Generated: $(date -Iseconds)

**Detected Stack:** $STACK_STRING

---

HEADER_EOF
```

### Step 3: Research Architecture Patterns

```bash
cat >> "$OUTPUT_FILE" << 'ARCH_EOF'

## Recommended Architecture

<!-- Web search: "[stack] architecture patterns [year]" -->

### Overview

Based on your tech stack, consider these architectural approaches:

### Layer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Presentation Layer            â”‚
â”‚    (UI Components, Views, Templates)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Application Layer             â”‚
â”‚   (Controllers, Services, Use Cases)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚             Domain Layer                â”‚
â”‚    (Business Logic, Entities, Rules)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          Infrastructure Layer           â”‚
â”‚  (Database, External APIs, File System) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Principles

1. **Separation of Concerns** - Each layer has a distinct responsibility
2. **Dependency Inversion** - High-level modules don't depend on low-level modules
3. **Single Responsibility** - Each component does one thing well
4. **Open/Closed** - Open for extension, closed for modification

---

ARCH_EOF
```

### Step 4: Research Project Structure

```bash
cat >> "$OUTPUT_FILE" << 'STRUCTURE_EOF'

## Recommended Project Structure

<!-- Web search: "[stack] project structure best practices" -->

### General Structure

```
project-root/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/     # UI components (if applicable)
â”‚   â”œâ”€â”€ services/       # Business logic services
â”‚   â”œâ”€â”€ models/         # Data models/entities
â”‚   â”œâ”€â”€ utils/          # Utility functions
â”‚   â”œâ”€â”€ config/         # Configuration files
â”‚   â””â”€â”€ index.ts        # Entry point
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/           # Unit tests
â”‚   â”œâ”€â”€ integration/    # Integration tests
â”‚   â””â”€â”€ e2e/            # End-to-end tests
â”œâ”€â”€ docs/               # Documentation
â”œâ”€â”€ scripts/            # Build/deploy scripts
â””â”€â”€ config files        # package.json, tsconfig, etc.
```

### Naming Conventions

- **Files**: `kebab-case.ts` or `PascalCase.tsx` for components
- **Directories**: `kebab-case/`
- **Tests**: `*.test.ts` or `*.spec.ts`
- **Types/Interfaces**: `PascalCase`

### Module Organization

- Group by feature, not by type (feature-first)
- Keep related files close together
- Limit folder nesting (max 3-4 levels)
- Use barrel exports (index.ts) for clean imports

---

STRUCTURE_EOF
```

### Step 5: Research Testing Strategy

```bash
cat >> "$OUTPUT_FILE" << 'TESTING_EOF'

## Testing Strategy

<!-- Web search: "[stack] testing strategy" -->

### Test Pyramid

```
         /\
        /  \
       / E2E\        <- Few, slow, expensive
      /â”€â”€â”€â”€â”€â”€\
     /  Int.  \      <- Some, medium speed
    /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
   /    Unit    \    <- Many, fast, cheap
  /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
```

### Recommended Coverage

| Layer | Coverage Target | Focus |
|-------|----------------|-------|
| Unit | 80%+ | Business logic, utilities |
| Integration | 60%+ | API endpoints, services |
| E2E | Critical paths | User journeys |

### Testing Best Practices

1. **Write tests first** (TDD) for complex logic
2. **Test behavior, not implementation**
3. **Use meaningful test names** that describe the scenario
4. **Keep tests independent** - no shared state
5. **Mock external dependencies** in unit tests
6. **Use factories** for test data creation

### Recommended Tools

- **Unit Testing**: Jest, Vitest, pytest, cargo test
- **Integration Testing**: Supertest, pytest, integration frameworks
- **E2E Testing**: Playwright, Cypress, Selenium
- **Mocking**: MSW, unittest.mock, mockall

---

TESTING_EOF
```

### Step 6: Research Data Flow Patterns

```bash
cat >> "$OUTPUT_FILE" << 'DATAFLOW_EOF'

## Data Flow Patterns

<!-- Web search: "[stack] state management patterns" -->

### Frontend State Management

For complex applications, consider:

1. **Local State** - Component-level state for UI
2. **Server State** - Data from API (use React Query, SWR, etc.)
3. **Global State** - Shared across components (Context, Redux, Zustand)

### API Design Patterns

- **REST** - Resource-based, stateless, cacheable
- **GraphQL** - Query language, single endpoint, typed
- **tRPC** - End-to-end type safety for TypeScript

### Data Fetching Strategies

1. **Server-Side Rendering (SSR)** - SEO, initial load
2. **Static Site Generation (SSG)** - Performance, caching
3. **Client-Side Rendering (CSR)** - Interactivity
4. **Incremental Static Regeneration (ISR)** - Best of both

---

DATAFLOW_EOF
```

### Step 7: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Implementation Checklist

Before implementing, verify:

- [ ] Architecture aligns with team expertise
- [ ] Project structure supports scalability
- [ ] Testing strategy covers critical paths
- [ ] Data flow patterns match requirements
- [ ] Security considerations addressed

## Sources

Patterns compiled from:
- Official framework documentation
- Community best practices
- Industry standards (Clean Architecture, DDD)
- Real-world project examples

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Stack patterns saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Patterns should be practical, not theoretical
- Should adapt recommendations to detected stack
- Must provide concrete examples where possible
- Should highlight trade-offs for different approaches

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}patterns,"
fi

# 4. Domain Research (comprehensive only)
if [ "$DO_DOMAIN_RESEARCH" = "true" ]; then
    echo ""
    echo "ðŸŽ¯ Researching domain-specific knowledge..."
    # Workflow: Research Domain

## Purpose

Research domain-specific patterns, compliance requirements, and industry best practices based on the detected project type.

## Inputs

- `PROJECT_TYPE` - web_app, cli, api, library, monorepo
- `DETECTED_FRAMEWORK` - For context
- `SECURITY_LEVEL` - Informs compliance research depth

## Outputs

- `agent-os/config/enriched-knowledge/domain-knowledge.md`

---

## Web Search Queries

### Query Templates

```
1. "[project_type] software architecture patterns"
2. "[project_type] industry best practices"
3. "[project_type] compliance requirements"
4. "[project_type] common challenges"
5. "[project_type] scalability patterns"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/domain-knowledge.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Domain Knowledge

> Industry-specific patterns and considerations
> Generated: $(date -Iseconds)

**Project Type:** ${PROJECT_TYPE:-unknown}

---

HEADER_EOF
```

### Step 2: Research Based on Project Type

```bash
case "${PROJECT_TYPE:-unknown}" in
    "web_app")
        cat >> "$OUTPUT_FILE" << 'WEBAPP_EOF'

## Web Application Patterns

<!-- Web search: "web application architecture patterns [year]" -->

### Common Architectures

1. **Single Page Application (SPA)**
   - Rich client-side interactivity
   - API-driven data fetching
   - Client-side routing
   
2. **Server-Side Rendered (SSR)**
   - Better SEO
   - Faster initial load
   - Server load considerations

3. **Hybrid (SSR + SPA)**
   - Best of both worlds
   - Complexity trade-off
   - Popular in modern frameworks

### User Experience Considerations

- **Performance**: First Contentful Paint < 1.8s
- **Accessibility**: WCAG 2.1 compliance
- **Responsive Design**: Mobile-first approach
- **Progressive Enhancement**: Works without JS

### Security Requirements

- **Authentication**: Secure login flows
- **Authorization**: Role-based access control
- **Data Protection**: HTTPS, CSP headers
- **Input Validation**: Client and server-side

---

WEBAPP_EOF
        ;;
        
    "api")
        cat >> "$OUTPUT_FILE" << 'API_EOF'

## API Service Patterns

<!-- Web search: "API design best practices [year]" -->

### API Design Principles

1. **RESTful Design**
   - Resource-oriented URLs
   - HTTP methods for operations
   - Stateless communication
   
2. **Versioning Strategy**
   - URL versioning: `/api/v1/`
   - Header versioning
   - Query parameter versioning

3. **Error Handling**
   - Consistent error format
   - Meaningful status codes
   - Detailed error messages (dev only)

### Performance Patterns

- **Pagination**: Cursor-based for large datasets
- **Caching**: ETags, Cache-Control headers
- **Rate Limiting**: Protect against abuse
- **Compression**: gzip/brotli responses

### Documentation

- OpenAPI/Swagger specification
- Interactive documentation
- Code examples
- Versioned docs

---

API_EOF
        ;;
        
    "cli")
        cat >> "$OUTPUT_FILE" << 'CLI_EOF'

## CLI Tool Patterns

<!-- Web search: "CLI application best practices" -->

### CLI Design Principles

1. **User Experience**
   - Clear help text (`--help`)
   - Intuitive command structure
   - Meaningful error messages
   - Progress indicators for long operations

2. **Command Structure**
   ```
   tool <command> [subcommand] [options] [arguments]
   ```

3. **Configuration**
   - Config file support
   - Environment variables
   - Command-line flags (highest priority)

### Best Practices

- Follow POSIX conventions
- Support piping and redirection
- Provide quiet (`-q`) and verbose (`-v`) modes
- Exit with appropriate codes
- Support both short (`-h`) and long (`--help`) flags

### Distribution

- Single binary if possible
- Package manager support (npm, cargo, brew)
- Auto-update mechanism
- Cross-platform builds

---

CLI_EOF
        ;;
        
    "library")
        cat >> "$OUTPUT_FILE" << 'LIB_EOF'

## Library Design Patterns

<!-- Web search: "library design best practices" -->

### API Design

1. **Minimal Surface Area**
   - Expose only what's needed
   - Internal vs. public APIs
   - Deprecation strategy

2. **Consistency**
   - Naming conventions
   - Error handling patterns
   - Return value patterns

3. **Extensibility**
   - Plugin/middleware support
   - Configuration options
   - Hooks for customization

### Documentation

- Comprehensive README
- API documentation
- Usage examples
- Migration guides
- Changelog

### Versioning

- Semantic versioning (SemVer)
- Clear breaking change policy
- Deprecation warnings
- LTS versions for stability

---

LIB_EOF
        ;;
        
    *)
        cat >> "$OUTPUT_FILE" << 'DEFAULT_EOF'

## General Software Patterns

### Universal Best Practices

1. **Code Quality**
   - Consistent formatting
   - Meaningful naming
   - Documentation
   - Testing

2. **Architecture**
   - Separation of concerns
   - Dependency management
   - Configuration management
   - Error handling

3. **Operations**
   - Logging and monitoring
   - Health checks
   - Graceful shutdown
   - Configuration via environment

---

DEFAULT_EOF
        ;;
esac
```

### Step 3: Research Compliance Requirements

```bash
if [ "$SECURITY_LEVEL" = "high" ]; then
    cat >> "$OUTPUT_FILE" << 'COMPLIANCE_EOF'

## Compliance Considerations

<!-- Web search: "software compliance requirements [year]" -->

### Common Compliance Frameworks

| Framework | Focus | Key Requirements |
|-----------|-------|------------------|
| **GDPR** | Data Privacy (EU) | Consent, data rights, breach notification |
| **SOC 2** | Security Controls | Access control, encryption, monitoring |
| **HIPAA** | Healthcare Data | PHI protection, access logs, encryption |
| **PCI-DSS** | Payment Data | Cardholder data protection, network security |

### General Compliance Checklist

- [ ] Data encryption at rest and in transit
- [ ] Access control and authentication
- [ ] Audit logging
- [ ] Data retention policies
- [ ] Incident response plan
- [ ] Regular security assessments
- [ ] Privacy policy and terms of service

### Security Controls

1. **Authentication**
   - Multi-factor authentication
   - Session management
   - Password policies

2. **Authorization**
   - Principle of least privilege
   - Role-based access control
   - Resource-level permissions

3. **Data Protection**
   - Encryption standards (AES-256)
   - Key management
   - Secure deletion

---

COMPLIANCE_EOF
fi
```

### Step 4: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Implementation Priority

Based on your project type, prioritize:

1. **Core Functionality** - Get the basics right first
2. **Security** - Build security in from the start
3. **Performance** - Optimize critical paths
4. **Scalability** - Design for growth
5. **Maintainability** - Think long-term

## Sources

Domain knowledge compiled from:
- Industry standards and frameworks
- Community best practices
- Regulatory requirements
- Real-world case studies

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Domain knowledge saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Research should be specific to detected project type
- Compliance info should match security level
- Should provide actionable recommendations
- Must include implementation priorities

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}domain,"
fi

# 5. Version Analysis (always)
echo ""
echo "ðŸ“¦ Analyzing dependency versions..."
# Workflow: Version Analysis

## Purpose

Compare detected dependency versions against latest stable versions, flag outdated dependencies, and note breaking changes in newer versions.

## Inputs

- `package.json` (Node.js)
- `Cargo.toml` (Rust)
- `go.mod` (Go)
- `requirements.txt` / `pyproject.toml` (Python)

## Outputs

- `agent-os/config/enriched-knowledge/version-analysis.md`

---

## Web Search Queries

### Query Templates

```
1. "[package] latest version"
2. "[package] changelog breaking changes"
3. "[package] migration guide v[old] to v[new]"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/version-analysis.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Version Analysis

> Dependency version status and update recommendations
> Generated: $(date -Iseconds)

This analysis compares your current dependency versions against the latest
stable releases and flags potential updates.

---

## Summary

HEADER_EOF
```

### Step 2: Analyze Node.js Dependencies

```bash
if [ -f "package.json" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Node.js Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`npm outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Package | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|---------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Extract key dependencies (simplified - in practice, use npm outdated)
    # This is a template showing the expected output format
    
    # Check for common frameworks and their typical update status
    if grep -q '"react"' package.json 2>/dev/null; then
        REACT_VER=$(grep '"react"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        if [ "${REACT_VER%%.*}" -lt "18" ] 2>/dev/null; then
            echo "| react | ${REACT_VER:-unknown} | âš ï¸ OUTDATED | Consider upgrading to React 18 |" >> "$OUTPUT_FILE"
        else
            echo "| react | ${REACT_VER:-unknown} | âœ… Current | |" >> "$OUTPUT_FILE"
        fi
    fi
    
    if grep -q '"next"' package.json 2>/dev/null; then
        NEXT_VER=$(grep '"next"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| next | ${NEXT_VER:-unknown} | â„¹ï¸ Check | Major versions may have breaking changes |" >> "$OUTPUT_FILE"
    fi
    
    if grep -q '"typescript"' package.json 2>/dev/null; then
        TS_VER=$(grep '"typescript"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| typescript | ${TS_VER:-unknown} | â„¹ï¸ Check | TypeScript 5.x has new features |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check all outdated packages" >> "$OUTPUT_FILE"
    echo "npm outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all packages (minor/patch)" >> "$OUTPUT_FILE"
    echo "npm update" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Interactive update (recommended)" >> "$OUTPUT_FILE"
    echo "npx npm-check-updates -i" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 3: Analyze Rust Dependencies

```bash
if [ -f "Cargo.toml" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Rust Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`cargo outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Crate | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|-------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check for common crates
    if grep -q 'tokio' Cargo.toml 2>/dev/null; then
        TOKIO_VER=$(grep 'tokio' Cargo.toml | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| tokio | ${TOKIO_VER:-unknown} | â„¹ï¸ Check | Async runtime |" >> "$OUTPUT_FILE"
    fi
    
    if grep -q 'serde' Cargo.toml 2>/dev/null; then
        SERDE_VER=$(grep 'serde' Cargo.toml | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| serde | ${SERDE_VER:-unknown} | â„¹ï¸ Check | Serialization |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Install cargo-outdated" >> "$OUTPUT_FILE"
    echo "cargo install cargo-outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Check outdated dependencies" >> "$OUTPUT_FILE"
    echo "cargo outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update dependencies" >> "$OUTPUT_FILE"
    echo "cargo update" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 4: Analyze Python Dependencies

```bash
if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Python Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`pip list --outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Package | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|---------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check for common packages
    PYDEPS=""
    [ -f "requirements.txt" ] && PYDEPS=$(cat requirements.txt)
    
    if echo "$PYDEPS" | grep -qi 'django'; then
        DJANGO_VER=$(echo "$PYDEPS" | grep -i 'django' | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| django | ${DJANGO_VER:-unknown} | â„¹ï¸ Check | Web framework |" >> "$OUTPUT_FILE"
    fi
    
    if echo "$PYDEPS" | grep -qi 'fastapi'; then
        FASTAPI_VER=$(echo "$PYDEPS" | grep -i 'fastapi' | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| fastapi | ${FASTAPI_VER:-unknown} | â„¹ï¸ Check | API framework |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check outdated packages" >> "$OUTPUT_FILE"
    echo "pip list --outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all packages" >> "$OUTPUT_FILE"
    echo "pip install --upgrade -r requirements.txt" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Use pip-tools for better dependency management" >> "$OUTPUT_FILE"
    echo "pip install pip-tools" >> "$OUTPUT_FILE"
    echo "pip-compile --upgrade" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 5: Analyze Go Dependencies

```bash
if [ -f "go.mod" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Go Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`go list -m -u all\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Module | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|--------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check Go version
    GO_VER=$(grep "^go " go.mod | awk '{print $2}')
    echo "| go (runtime) | ${GO_VER:-unknown} | â„¹ï¸ Check | Go version |" >> "$OUTPUT_FILE"
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check for available updates" >> "$OUTPUT_FILE"
    echo "go list -m -u all" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all dependencies" >> "$OUTPUT_FILE"
    echo "go get -u ./..." >> "$OUTPUT_FILE"
    echo "go mod tidy" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 6: Add Update Strategy

```bash
cat >> "$OUTPUT_FILE" << 'STRATEGY_EOF'

---

## Update Strategy

### Version Status Legend

| Status | Meaning | Action |
|--------|---------|--------|
| âœ… Current | Latest stable version | No action needed |
| â„¹ï¸ Check | Unknown/needs verification | Run version check tool |
| âš ï¸ OUTDATED | Behind latest stable | Plan update |
| ðŸ”´ CRITICAL | Security patch available | Update immediately |

### Safe Update Process

1. **Check current versions**
   ```bash
   # Use appropriate tool for your language
   npm outdated / cargo outdated / pip list --outdated
   ```

2. **Review changelogs**
   - Check for breaking changes
   - Review migration guides
   - Note deprecated features

3. **Update in stages**
   - Patch versions first (x.x.PATCH)
   - Then minor versions (x.MINOR.x)
   - Major versions last (MAJOR.x.x)

4. **Test thoroughly**
   - Run test suite after each update
   - Check critical paths manually
   - Monitor for regressions

### When to Update

| Situation | Recommended Action |
|-----------|-------------------|
| Security vulnerability | Update immediately |
| Major version behind | Plan migration sprint |
| Minor version behind | Update during maintenance |
| Patch version behind | Update with next release |

---

STRATEGY_EOF
```

### Step 7: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Automated Version Checking

For real-time, accurate version information, use these tools:

| Language | Tool | Command |
|----------|------|---------|
| Node.js | npm | `npm outdated` |
| Node.js | npx | `npx npm-check-updates` |
| Rust | cargo | `cargo outdated` |
| Python | pip | `pip list --outdated` |
| Go | go | `go list -m -u all` |

### CI Integration

Consider adding version checking to your CI pipeline:

```yaml
# Example GitHub Action
- name: Check for outdated dependencies
  run: npm outdated || true  # Don't fail build
```

---

*Generated by Geist Adaptive Questionnaire System*
*Run the appropriate version check tool for accurate, real-time information*

FOOTER_EOF

echo "   âœ“ Version analysis saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Must recommend using real-time tools for accuracy
- Should not make definitive version claims without verification
- Must provide safe update strategy
- Should distinguish between critical and non-critical updates
- Must include commands for each language ecosystem

RESEARCH_COMPLETED="${RESEARCH_COMPLETED}versions,"
```

### Step 4: Synthesize Knowledge

```bash
echo ""
echo "ðŸ”„ Synthesizing research findings..."
# Workflow: Synthesize Knowledge

## Purpose

Combine all research outputs, remove duplicates, prioritize actionable insights, and create a unified summary for easy consumption.

## Inputs

Research files from:
- `agent-os/config/enriched-knowledge/library-research.md`
- `agent-os/config/enriched-knowledge/stack-best-practices.md`
- `agent-os/config/enriched-knowledge/domain-knowledge.md`
- `agent-os/config/enriched-knowledge/version-analysis.md`
- `agent-os/config/enriched-knowledge/security-notes.md`

## Outputs

- Updates to individual research files (deduplication)
- `agent-os/config/enriched-knowledge/README.md` - Summary index

---

## Workflow

### Step 1: Create Summary Index

```bash
KNOWLEDGE_DIR="agent-os/config/enriched-knowledge"
SUMMARY_FILE="$KNOWLEDGE_DIR/README.md"

cat > "$SUMMARY_FILE" << HEADER_EOF
# Enriched Knowledge Index

> Consolidated research findings for your project
> Generated: $(date -Iseconds)

This directory contains research gathered during the adaptive questionnaire
process. Use this knowledge to inform your development decisions.

---

## Available Research

HEADER_EOF
```

### Step 2: Index Available Files

```bash
echo "### Research Files" >> "$SUMMARY_FILE"
echo "" >> "$SUMMARY_FILE"

# Check each expected file and add to index
if [ -f "$KNOWLEDGE_DIR/library-research.md" ]; then
    echo "- ðŸ“š [Library Research](library-research.md) - Best practices for your dependencies" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/stack-best-practices.md" ]; then
    echo "- ðŸ—ï¸ [Stack Best Practices](stack-best-practices.md) - Architecture patterns for your tech stack" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/domain-knowledge.md" ]; then
    echo "- ðŸŽ¯ [Domain Knowledge](domain-knowledge.md) - Industry-specific patterns" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/version-analysis.md" ]; then
    echo "- ðŸ“¦ [Version Analysis](version-analysis.md) - Dependency version status" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/security-notes.md" ]; then
    echo "- ðŸ”’ [Security Notes](security-notes.md) - Security considerations and CVEs" >> "$SUMMARY_FILE"
fi

echo "" >> "$SUMMARY_FILE"
```

### Step 3: Extract Key Insights

```bash
cat >> "$SUMMARY_FILE" << 'INSIGHTS_HEADER'

---

## Key Insights Summary

### ðŸ”´ Critical Items

Items requiring immediate attention:

INSIGHTS_HEADER

# Check for critical security issues
if [ -f "$KNOWLEDGE_DIR/security-notes.md" ]; then
    if grep -q "CRITICAL\|ðŸ”´" "$KNOWLEDGE_DIR/security-notes.md" 2>/dev/null; then
        echo "- âš ï¸ Critical security issues found - see [Security Notes](security-notes.md)" >> "$SUMMARY_FILE"
    else
        echo "- âœ… No critical security issues detected" >> "$SUMMARY_FILE"
    fi
fi

# Check for outdated dependencies
if [ -f "$KNOWLEDGE_DIR/version-analysis.md" ]; then
    if grep -q "OUTDATED\|Major update" "$KNOWLEDGE_DIR/version-analysis.md" 2>/dev/null; then
        echo "- âš ï¸ Outdated dependencies found - see [Version Analysis](version-analysis.md)" >> "$SUMMARY_FILE"
    else
        echo "- âœ… Dependencies are up to date" >> "$SUMMARY_FILE"
    fi
fi

echo "" >> "$SUMMARY_FILE"
```

### Step 4: Add Quick Reference

```bash
cat >> "$SUMMARY_FILE" << 'QUICKREF_EOF'

### ðŸ“‹ Quick Reference

| Area | Document | When to Use |
|------|----------|-------------|
| Starting a new feature | Stack Best Practices | Architecture decisions |
| Adding dependencies | Library Research | Evaluate libraries |
| Security review | Security Notes | Before deployment |
| Updating packages | Version Analysis | Maintenance cycles |
| Domain questions | Domain Knowledge | Business logic |

---

QUICKREF_EOF
```

### Step 5: Add Usage Instructions

```bash
cat >> "$SUMMARY_FILE" << 'USAGE_EOF'

## How to Use This Knowledge

### During Development

1. **Before implementing a feature**: Check Stack Best Practices for patterns
2. **When choosing libraries**: Review Library Research for recommendations
3. **During code review**: Reference Security Notes for secure coding
4. **When debugging**: Check Library Research for known issues

### During Maintenance

1. **Regular updates**: Use Version Analysis to prioritize updates
2. **Security patches**: Follow Security Notes recommendations
3. **Refactoring**: Reference Stack Best Practices for patterns

### During Planning

1. **Architecture decisions**: Stack Best Practices + Domain Knowledge
2. **Compliance requirements**: Domain Knowledge + Security Notes
3. **Technology choices**: Library Research + Version Analysis

---

USAGE_EOF
```

### Step 6: Add Refresh Instructions

```bash
cat >> "$SUMMARY_FILE" << 'REFRESH_EOF'

## Keeping Knowledge Fresh

This research was generated at a point in time. To refresh:

1. **Re-run detection**: This will update research with latest information
2. **Manual update**: Edit files directly for project-specific notes
3. **Research depth**: Adjust depth for more/less detail

### Research Depth Levels

| Level | Time | Best For |
|-------|------|----------|
| `minimal` | ~30s | Quick updates |
| `standard` | ~2m | Regular use |
| `comprehensive` | ~5m | Major decisions |

To change depth:
```bash
RESEARCH_DEPTH=comprehensive
# Then re-run adapt-to-product or the research orchestrator
```

---

## Files in This Directory

REFRESH_EOF

# List all files with sizes
ls -lh "$KNOWLEDGE_DIR"/*.md 2>/dev/null | awk '{print "- `" $NF "` (" $5 ")"}' >> "$SUMMARY_FILE" || echo "- No files found" >> "$SUMMARY_FILE"

cat >> "$SUMMARY_FILE" << 'FOOTER_EOF'

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Knowledge synthesis complete"
echo "   âœ“ Summary index saved to $SUMMARY_FILE"
```

---

## Deduplication Logic

When synthesizing, the workflow identifies and consolidates:

1. **Repeated recommendations** across files
2. **Conflicting advice** (flags for human review)
3. **Version-specific information** (keeps most recent)
4. **Source attribution** (maintains for verification)

---

## Important Constraints

- Must maintain source attribution
- Should flag conflicting recommendations
- Must create navigable index
- Should highlight critical items prominently
- Must provide clear usage guidance

```

### Step 5: Generate Research Summary

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  RESEARCH COMPLETE"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ðŸ“ Enriched knowledge saved to: agent-os/config/enriched-knowledge/"
echo ""
echo "Files generated:"
ls -la agent-os/config/enriched-knowledge/ 2>/dev/null | grep ".md" | awk '{print "   â€¢ " $NF}'
echo ""
echo "Research areas completed: $(echo $RESEARCH_COMPLETED | sed 's/,$//' | tr ',' ', ')"
echo ""
```

---

## Integration

This workflow is called by:
- `adapt-to-product/1-setup-and-information-gathering.md` (after detection)
- `create-basepoints/1-validate-prerequisites.md` (for architecture research)

The enriched knowledge is used by:
- `deploy-agents` for specialization
- Validation workflows for security checks
- Human review for flagging issues

---

## Research Depth Guidelines

| Depth | Time | Use Case |
|-------|------|----------|
| `minimal` | ~30s | Quick setup, simple projects |
| `standard` | ~2m | Most projects (default) |
| `comprehensive` | ~5m | Enterprise, complex projects |

---

## Important Constraints

- Must handle web search failures gracefully
- Should cache results to avoid redundant searches
- Must attribute sources in output
- Should prioritize actionable insights over raw data

```

This will:
- Research best practices for detected libraries
- Gather architecture patterns for your tech stack
- Check for security vulnerabilities and updates
- Store findings in `agent-os/config/enriched-knowledge/`

### 0.3: Present Findings and Confirm

Display detected configuration and ask for confirmation:

```bash
# Workflow: Present and Confirm

## Purpose

Format detected values for display, present a confirmation prompt to the user, handle user overrides, and output the final confirmed profile.

## Inputs

Expects these variables to be set (from prior detection workflows):
- `DETECTED_PROJECT_TYPE`
- `DETECTED_LANGUAGE`
- `DETECTED_FRAMEWORK`
- `DETECTED_DATABASE`
- `DETECTED_BUILD_CMD`
- `DETECTED_TEST_CMD`
- `DETECTED_LINT_CMD`
- `DETECTED_SECURITY_LEVEL`
- `DETECTED_FILE_COUNT`
- `DETECTED_LINE_COUNT`
- `DETECTION_CONFIDENCE`

## Outputs

- Updates `agent-os/config/project-profile.yml` with confirmed values
- Sets `USER_CONFIRMED=true` when user accepts

---

## Workflow

### Step 1: Display Detected Configuration

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "                      DETECTED PROJECT CONFIGURATION                         "
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Project Profile Section
echo "ðŸ“¦ PROJECT PROFILE"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Type:" "${DETECTED_PROJECT_TYPE:-unknown} âœ“"
printf "   %-20s %s\n" "Size:" "${DETECTED_FILE_COUNT:-?} files, ~${DETECTED_LINE_COUNT:-?} lines âœ“"
printf "   %-20s %s\n" "Maturity:" "$([ -d '.git' ] && echo 'Version controlled' || echo 'Unknown') âœ“"
echo ""

# Tech Stack Section
echo "ðŸ”§ TECH STACK"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Language:" "${DETECTED_LANGUAGE:-unknown} âœ“"
[ -n "$DETECTED_FRAMEWORK" ] && printf "   %-20s %s\n" "Framework:" "$DETECTED_FRAMEWORK âœ“"
[ -n "$DETECTED_BACKEND" ] && printf "   %-20s %s\n" "Backend:" "$DETECTED_BACKEND âœ“"
[ -n "$DETECTED_DATABASE" ] && printf "   %-20s %s\n" "Database:" "$DETECTED_DATABASE âœ“"
echo ""

# Commands Section
echo "âš™ï¸  DETECTED COMMANDS"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Build:" "${DETECTED_BUILD_CMD:-(not detected)} âœ“"
printf "   %-20s %s\n" "Test:" "${DETECTED_TEST_CMD:-(not detected)} âœ“"
printf "   %-20s %s\n" "Lint:" "${DETECTED_LINT_CMD:-(not detected)} âœ“"
echo ""

# Inferred Settings Section
echo "ðŸ”’ INFERRED SETTINGS"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Security Level:" "${DETECTED_SECURITY_LEVEL:-moderate} âœ“"
printf "   %-20s %s\n" "Complexity:" "${INFERRED_COMPLEXITY:-moderate} âœ“"
echo ""

# Confidence
echo "ðŸ“Š DETECTION CONFIDENCE: ${DETECTION_CONFIDENCE:-0.80}"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

### Step 2: Present Minimal Questions

Only ask questions for things that cannot be detected:

```bash
echo ""
echo "âš ï¸  QUESTIONS REQUIRING YOUR INPUT"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "These items cannot be determined automatically from your codebase:"
echo ""

# Question 1: Compliance Requirements
echo "1. What compliance requirements apply to this project?"
echo ""
echo "   [ ] None (default)"
echo "   [ ] SOC 2"
echo "   [ ] HIPAA"
echo "   [ ] GDPR"
echo "   [ ] PCI-DSS"
echo "   [ ] Other"
echo ""
echo "   Enter your choice (e.g., 'none', 'gdpr', 'soc2,hipaa'): "

# In non-interactive mode or if user presses Enter, use default
USER_COMPLIANCE="${USER_COMPLIANCE:-none}"

echo ""

# Question 2: Human Review Level
echo "2. How much human oversight do you want for AI-generated changes?"
echo ""
echo "   [ ] minimal  - Trust AI, review only critical changes"
echo "   [ ] moderate - Review architectural decisions (default)"
echo "   [ ] high     - Review all significant changes"
echo ""
echo "   Enter your choice (minimal/moderate/high): "

# Default to moderate
USER_HUMAN_REVIEW="${USER_HUMAN_REVIEW:-moderate}"

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

### Step 3: Confirmation Prompt

```bash
echo ""
echo "ðŸ“‹ CONFIRMATION"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "Press Enter to accept these detected values, or type a section name to modify:"
echo ""
echo "   â€¢ 'type'      - Change project type"
echo "   â€¢ 'language'  - Change language/framework"
echo "   â€¢ 'commands'  - Change build/test/lint commands"
echo "   â€¢ 'security'  - Change security level"
echo "   â€¢ 'all'       - Review all settings interactively"
echo ""
echo "Your choice (Enter to accept): "

# For non-interactive execution, assume acceptance
USER_CHOICE="${USER_CHOICE:-accept}"

if [ "$USER_CHOICE" = "" ] || [ "$USER_CHOICE" = "accept" ]; then
    echo ""
    echo "âœ… Configuration confirmed!"
    USER_CONFIRMED="true"
fi
```

### Step 4: Handle Overrides (if requested)

```bash
# Handle override requests
case "$USER_CHOICE" in
    "type")
        echo "Enter new project type (web_app, cli, api, library, monorepo):"
        read -r NEW_TYPE
        [ -n "$NEW_TYPE" ] && DETECTED_PROJECT_TYPE="$NEW_TYPE"
        ;;
    "language")
        echo "Enter primary language (typescript, javascript, rust, python, go):"
        read -r NEW_LANG
        [ -n "$NEW_LANG" ] && DETECTED_LANGUAGE="$NEW_LANG"
        echo "Enter framework (or leave empty):"
        read -r NEW_FRAMEWORK
        DETECTED_FRAMEWORK="$NEW_FRAMEWORK"
        ;;
    "commands")
        echo "Enter build command (or leave empty):"
        read -r NEW_BUILD
        DETECTED_BUILD_CMD="$NEW_BUILD"
        echo "Enter test command (or leave empty):"
        read -r NEW_TEST
        DETECTED_TEST_CMD="$NEW_TEST"
        echo "Enter lint command (or leave empty):"
        read -r NEW_LINT
        DETECTED_LINT_CMD="$NEW_LINT"
        ;;
    "security")
        echo "Enter security level (low, moderate, high):"
        read -r NEW_SECURITY
        [ -n "$NEW_SECURITY" ] && DETECTED_SECURITY_LEVEL="$NEW_SECURITY"
        ;;
esac
```

### Step 5: Update Profile with Confirmed Values

```bash
# Update the profile with user-specified values
cat > agent-os/config/project-profile.yml << CONFIRMED_EOF
# Project Profile
# Auto-detected and confirmed by user
# Generated: $(date -Iseconds)

gathered:
  project_type: ${DETECTED_PROJECT_TYPE:-unknown}
  tech_stack:
    language: ${DETECTED_LANGUAGE:-unknown}
    framework: ${DETECTED_FRAMEWORK:-}
    backend: ${DETECTED_BACKEND:-}
    database: ${DETECTED_DATABASE:-}
  size:
    lines: ${DETECTED_LINE_COUNT:-0}
    files: ${DETECTED_FILE_COUNT:-0}
    modules: ${DETECTED_MODULE_COUNT:-0}
  commands:
    build: "${DETECTED_BUILD_CMD:-}"
    test: "${DETECTED_TEST_CMD:-}"
    lint: "${DETECTED_LINT_CMD:-}"

inferred:
  security_level: ${DETECTED_SECURITY_LEVEL:-moderate}
  complexity: ${INFERRED_COMPLEXITY:-moderate}

user_specified:
  compliance:
$(echo "$USER_COMPLIANCE" | tr ',' '\n' | sed 's/^/    - /' | grep -v "^    - none$" || echo "    []")
  human_review_level: ${USER_HUMAN_REVIEW:-moderate}

_meta:
  detected_at: $(date -Iseconds)
  confirmed_at: $(date -Iseconds)
  detection_confidence: ${DETECTION_CONFIDENCE:-0.80}
  user_confirmed: ${USER_CONFIRMED:-true}
  questions_asked: 2
  questions_auto_answered: ${DETECTIONS_SUCCESS:-0}

CONFIRMED_EOF

echo ""
echo "âœ… Profile saved to: agent-os/config/project-profile.yml"
echo ""
```

---

## Output Summary

After confirmation, display:

```bash
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "                        CONFIGURATION COMPLETE                               "
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Your project profile has been saved and will be used for:"
echo ""
echo "  â€¢ Basepoint generation (create-basepoints)"
echo "  â€¢ Agent specialization (deploy-agents)"
echo "  â€¢ Validation command configuration"
echo "  â€¢ Workflow complexity selection"
echo ""
echo "You can modify the profile at any time by editing:"
echo "  agent-os/config/project-profile.yml"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

---

## Important Constraints

- Default to accepting detected values (user presses Enter)
- Maximum 2-3 questions that can't be auto-detected
- Provide sensible defaults for all questions
- Allow overrides but don't require them
- Save confirmed profile for use by subsequent commands

```

### 0.4: Ask Minimal Questions

Only ask questions that cannot be detected from the codebase:

```bash
# Workflow: Question Templates

## Purpose

Provide minimal question templates for information that cannot be automatically detected from the codebase. Maximum 2-3 questions per command.

---

## Questions That MUST Be Asked

These questions gather information that cannot be detected from code:

### Question 1: Compliance Requirements

```markdown
**1. Compliance Requirements** (can't detect from code)

What compliance standards apply to this project?

   [ ] None (default)
   [ ] SOC 2 - Security/availability controls
   [ ] HIPAA - Healthcare data protection
   [ ] GDPR - EU data privacy
   [ ] PCI-DSS - Payment card data
   [ ] Other (specify)

Enter your choice (e.g., 'none', 'gdpr', 'soc2,hipaa'):
```

**Default:** `none`

### Question 2: Human Review Preference

```markdown
**2. Human Review Preference**

How much human oversight do you want for AI-generated changes?

   [ ] minimal  - Trust AI, review only critical changes
   [ ] moderate - Review architectural decisions (default)
   [ ] high     - Review all significant changes

Enter your choice (minimal/moderate/high):
```

**Default:** `moderate`

---

## Questions Asked Only If Detection Fails

These are only asked when automatic detection confidence is below 70%:

### Question 3: Project Type (if ambiguous)

```markdown
**3. Project Type** (detection was uncertain)

I couldn't confidently determine the project type. Is this a:

   [ ] web_app  - Web application
   [ ] api      - API/Backend service
   [ ] cli      - Command-line tool
   [ ] library  - Reusable library
   [ ] monorepo - Multiple packages
   [ ] other    - Something else

Enter your choice:
```

### Question 4: Module Boundaries (if unclear)

```markdown
**4. Module Boundaries** (structure was unclear)

How should I identify module boundaries for basepoint generation?

   [ ] directory  - Each top-level directory is a module
   [ ] package    - Each package.json/Cargo.toml is a module
   [ ] manual     - Let me specify the modules

Enter your choice:
```

---

## Implementation

### Bash Script for Questions

```bash
# Function to ask compliance question
ask_compliance() {
    echo ""
    echo "1. What compliance requirements apply to this project?"
    echo ""
    echo "   [ ] None (default)"
    echo "   [ ] SOC 2"
    echo "   [ ] HIPAA"
    echo "   [ ] GDPR"
    echo "   [ ] PCI-DSS"
    echo "   [ ] Other"
    echo ""
    echo "   Enter your choice (e.g., 'none', 'gdpr', 'soc2,hipaa'):"
    
    # In non-interactive mode, use default
    USER_COMPLIANCE="${USER_COMPLIANCE:-none}"
    echo "   â†’ Using: $USER_COMPLIANCE"
}

# Function to ask human review preference
ask_human_review() {
    echo ""
    echo "2. How much human oversight do you want for AI changes?"
    echo ""
    echo "   [ ] minimal  - Trust AI, review only critical"
    echo "   [ ] moderate - Review architectural decisions (default)"
    echo "   [ ] high     - Review all significant changes"
    echo ""
    echo "   Enter your choice (minimal/moderate/high):"
    
    # In non-interactive mode, use default
    USER_HUMAN_REVIEW="${USER_HUMAN_REVIEW:-moderate}"
    echo "   â†’ Using: $USER_HUMAN_REVIEW"
}

# Function to ask project type (only if detection failed)
ask_project_type() {
    if [ "$NEEDS_USER_INPUT_TYPE" = "true" ]; then
        echo ""
        echo "3. I couldn't determine the project type. Is this a:"
        echo ""
        echo "   [ ] web_app  - Web application"
        echo "   [ ] api      - API/Backend service"
        echo "   [ ] cli      - Command-line tool"
        echo "   [ ] library  - Reusable library"
        echo "   [ ] monorepo - Multiple packages"
        echo ""
        echo "   Enter your choice:"
        
        # In non-interactive mode, use unknown
        USER_PROJECT_TYPE="${USER_PROJECT_TYPE:-unknown}"
        DETECTED_PROJECT_TYPE="$USER_PROJECT_TYPE"
    fi
}

# Main question flow
ask_minimal_questions() {
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "  QUESTIONS REQUIRING YOUR INPUT"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "These items cannot be determined from your codebase:"
    
    ask_compliance
    ask_human_review
    
    # Only ask additional questions if detection was uncertain
    ask_project_type
    
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Press Enter to accept these values, or re-run with"
    echo "environment variables to override:"
    echo ""
    echo "   USER_COMPLIANCE=gdpr USER_HUMAN_REVIEW=high /adapt-to-product"
    echo ""
}

# Execute questions
ask_minimal_questions
```

---

## Storing Answers

After questions are answered, update the project profile:

```bash
# Update project profile with user answers
update_profile_with_answers() {
    PROFILE_FILE="agent-os/config/project-profile.yml"
    
    if [ -f "$PROFILE_FILE" ]; then
        # Update user_specified section
        # Note: In practice, use yq or proper YAML parsing
        
        echo ""
        echo "Updating profile with your preferences..."
        echo "   Compliance: $USER_COMPLIANCE"
        echo "   Human Review: $USER_HUMAN_REVIEW"
        
        # The profile should already exist from detection
        # This just updates the user_specified section
    fi
}
```

---

## Question Count by Command

| Command | Questions Asked | Conditional Questions |
|---------|-----------------|----------------------|
| `/adapt-to-product` | 2 | +1 if type detection failed |
| `/create-basepoints` | 0-1 | Only if modules unclear |
| `/deploy-agents` | 0-2 | Only if not set in prior command |

**Total across all commands:** Maximum 3-5 questions (vs 20+ traditional approach)

---

## Important Constraints

- Maximum 2-3 questions per command
- All questions have sensible defaults
- User can press Enter to accept defaults
- Questions are skipped if already answered in prior command
- Additional questions only asked when detection confidence is low

```

Questions asked (maximum 2-3):
1. **Compliance requirements** - GDPR, HIPAA, SOC 2, etc. (can't detect from code)
2. **Human review preference** - How much AI oversight do you want?

The profile is saved to `agent-os/config/project-profile.yml` for use by subsequent commands.

---

## Step 1: Gather Product Information

The FIRST STEP is to set up folders and gather comprehensive product information from multiple sources by following these instructions:

# Gather Product Information from Existing Codebase

Collect comprehensive product information from multiple sources for adapting an existing codebase into product documentation.

## Step 1: Create Inheritance Folder

Create the inheritance folder for user-provided documents:

```bash
# Create inheritance folder
mkdir -p agent-os/product/inheritance

# Check if inheritance folder already exists
if [ -d "agent-os/product/inheritance" ]; then
    if [ "$(ls -A agent-os/product/inheritance 2>/dev/null)" ]; then
        echo "Inheritance folder already contains files. Using existing files or add new ones?"
        # List existing files in inheritance folder
        ls -la agent-os/product/inheritance/
    else
        echo "Inheritance folder created. Please place any product-related documents here."
    fi
fi
```

## Step 2: Check Product Folder

Check if product folder already exists and handle accordingly:

```bash
# Check if product folder already exists
if [ -d "agent-os/product" ]; then
    echo "Product documentation already exists. Review existing files or start fresh?"
    # List existing product files
    ls -la agent-os/product/
fi
```

## Step 3: Gather Information from Multiple Sources

Collect comprehensive product information from the following sources:

### A. User Input - Interactive User Prompts

Prompt user interactively for product information following the same patterns as plan-product (reference: `agent-os/commands/plan-product/1-product-concept.md`):

1. **Initial Information Gathering:**
   - Start with product name and core concept (if not already known from codebase)
   - Ask about product purpose and main value proposition
   - Gather key features (minimum 3 if not inferrable from codebase)
   - Ask about target users and use cases (at least 1 user segment)

2. **Extended Information Gathering:**
   - Prompt for web pages: "Do you have any product web pages or documentation URLs to include?"
   - Prompt for public resources: "Any public resources, documentation sites, or references?"
   - Prompt for private documents: "Do you have private documents or files describing the product?"
   - Prompt for links: "Please provide any relevant links (documentation, marketing pages, technical specs, etc.)"
   - Prompt for @command syntax: "You can use @command syntax to describe specific aspects. For example: @command + 'describe the product mission' or @command + 'outline the product roadmap'"

3. **Follow-up Questions:**
   - Ask whatever questions are needed to create the most accurate product files
   - If information is missing or unclear, prompt for clarification
   - Follow similar user interaction patterns from plan-product but adapted for existing codebase context

If any critical information is missing, prompt user:
```
Please provide the following to create your product plan:
1. Product name and core concept (if not clear from codebase)
2. Any web pages, documents, or links related to the product
3. Additional information using @command syntax if needed (e.g., @command + "describe the product mission")
4. Any specific details about target users, features, or differentiators
```

### B. Inheritance Folder Document Processing

Read and process files from `agent-os/product/inheritance/`:

```bash
# Check for files in inheritance folder
if [ -d "agent-os/product/inheritance" ] && [ "$(ls -A agent-os/product/inheritance 2>/dev/null)" ]; then
    echo "Processing files from inheritance folder..."
    # List all files
    find agent-os/product/inheritance -type f | while read file; do
        echo "Processing: $file"
        # Extract content based on file type
    done
fi
```

Process files from inheritance folder:

1. **File Type Detection and Reading:**
   - **Markdown files**: .md, .markdown - Read and extract text content, preserve structure
   - **Text files**: .txt, .text - Read plain text content
   - **JSON files**: .json - Parse and extract structured data, look for product-related fields
   - **YAML files**: .yml, .yaml - Parse and extract structured data
   - **HTML files**: .html, .htm - Extract text content, strip HTML tags
   - **PDF files**: .pdf - Extract text content if possible (may require external tools)
   - **Other formats**: Attempt to read as text if supported

2. **Information Extraction:**
   - Extract product description and purpose
   - Identify features, capabilities, and use cases
   - Extract user information and personas
   - Identify goals, objectives, and differentiators
   - Extract tech stack mentions (if present)
   - Extract roadmap or feature plans (if present)

3. **Merge into Product Knowledge:**
   - Combine extracted information with other sources
   - Tag information with source (inheritance folder)
   - Preserve file references for traceability
   - Handle duplicate information gracefully

### C. Automatic Link/Web Page Scraping

For any links provided by the user, automatically scrape and analyze web page content:

```bash
# For each link provided by user:
# Use web scraping capabilities to fetch and parse content
# Note: Actual implementation may use tools like curl, wget, or web scraping libraries
```

Process links provided by user:

1. **Link Validation:**
   - Verify URL format is valid
   - Check if link is accessible (HTTP/HTTPS)
   - Handle different URL formats (with/without protocol, fragments, etc.)

2. **Automatic Content Scraping:**
   - Fetch web page content automatically (do not ask user, just scrape)
   - Extract main content from HTML pages
   - Strip HTML tags and formatting
   - Preserve important structure (headings, lists, etc.) where possible
   - Extract text content for analysis

3. **Information Extraction:**
   - Extract product description and purpose from scraped content
   - Identify features and capabilities mentioned
   - Extract user information and target audience
   - Identify product goals and value propositions
   - Extract technical information if present

4. **Error Handling:**
   - Handle unreachable links gracefully (log error, continue with other sources)
   - Handle parsing failures (invalid HTML, encoding issues, etc.)
   - Handle timeouts and network errors
   - Continue processing other links even if one fails
   - Inform user if critical links cannot be accessed (but do not block the process)

### D. @command Syntax Support

If user provides `@command + "prompt"` syntax, parse and execute:

```bash
# Example: @command + "describe the product mission"
# Parse the command reference and prompt text
# Execute the referenced command with the prompt
# Capture output for product knowledge
```

Implement @command syntax support:

1. **Parsing @command References:**
   - Detect `@command` or `@<command-name>` patterns in user input
   - Extract command name (default to generic command if not specified)
   - Extract prompt text after `+` or following quotation marks
   - Handle variations: `@command + "prompt"`, `@mycommand "prompt"`, `@command prompt`

2. **Command Execution:**
   - Execute the referenced command with the provided prompt
   - Pass the prompt as input to the command
   - Capture all command output (stdout and stderr)
   - Handle command execution errors gracefully

3. **Output Capture:**
   - Capture command output text
   - Parse structured output if applicable (JSON, YAML, etc.)
   - Extract product-relevant information from command output

4. **Supported Use Cases:**
   - `@command + "describe the product mission"` - Generate mission description
   - `@command + "outline the product roadmap"` - Generate roadmap outline
   - `@command + "list the tech stack"` - Generate tech stack information
   - `@command + "describe key features"` - Generate feature descriptions
   - Support any command that can provide product-related information

5. **Integration:**
   - Merge command output into product knowledge
   - Tag information with source (command output)
   - Treat command output as high-priority information source

### E. Optional Web Search Integration

If user explicitly requests web search:

1. **Web Search Request Handling:**
   - Check if user explicitly requested web search (do NOT execute automatically)
   - Prompt user for search terms or topics if not provided
   - Confirm search query before executing
   - Example: "Would you like me to search the web for additional product information? If yes, what should I search for?"

2. **Perform Web Search from Public Sources:**
   - Use available web search capabilities to search for product-related information
   - Search public sources (documentation sites, forums, articles, etc.)
   - Focus search on product information, features, competitors, market research
   - Execute search only when explicitly requested by user

3. **Process Search Results:**
   - Extract relevant information from search results
   - Identify product-relevant content from search results
   - Extract features, descriptions, use cases, or other relevant information
   - Filter out irrelevant or low-quality results

4. **Integrate Search Results:**
   - Add web search results to unified product knowledge base
   - Tag information with source (web search)
   - Integrate findings into product knowledge synthesis
   - Treat web search results as lower priority than other sources

5. **Error Handling:**
   - Handle web search errors gracefully (network failures, API errors, etc.)
   - Handle cases where no relevant results are found
   - Continue with other information sources even if web search fails
   - Inform user if web search fails but do not block the process

## Step 4: Merge All Information Sources

After gathering information from all sources, merge everything into a unified product knowledge base:

### Information Merging Logic

1. **Collect Information from All Sources:**
   - User input (interactive prompts)
   - Inheritance folder documents (from `agent-os/product/inheritance/`)
   - Scraped web content (from links provided)
   - Command outputs (from @command syntax)
   - Web search results (if user requested web search)

2. **Resolve Conflicts Between Information Sources:**
   - When same information appears from multiple sources with different values:
     * Prioritize user-provided information (highest priority)
     * Use inheritance folder documents if user input is unclear
     * Use scraped content if no user/inheritance information
     * Use codebase findings (from Phase 2) as supporting information
     * Use web search results as additional context (lowest priority)
   - Merge complementary information (combine details from different sources)
   - Identify gaps where information is missing from all sources

3. **Prioritization Order (Highest to Lowest):**
   1. User-provided information (explicit user input)
   2. @command outputs (user-requested command execution)
   3. Inheritance folder documents (user-provided files)
   4. Scraped web content (user-provided links)
   5. Codebase analysis findings (Phase 2)
   6. Web search results (if requested)

4. **Create Unified Product Knowledge Base:**
   - Combine all information into structured knowledge base
   - Organize by categories: mission, users, features, tech stack, etc.
   - Tag each piece of information with its source for traceability
   - Preserve important details from all sources
   - Remove duplicates while keeping the highest priority version
   - Prepare unified knowledge for use in subsequent phases (mission, roadmap, tech-stack generation)



## User Standards & Preferences Compliance

When gathering product information, use the user's standards and preferences for context and baseline assumptions, as documented in these files:

@agent-os/standards/global/codebase-analysis.md
@agent-os/standards/global/coding-style.md
@agent-os/standards/global/commenting.md
@agent-os/standards/global/conventions.md
@agent-os/standards/global/enriched-knowledge-templates.md
@agent-os/standards/global/error-handling.md
@agent-os/standards/global/project-profile-schema.md
@agent-os/standards/global/tech-stack.md
@agent-os/standards/global/validation-commands.md
@agent-os/standards/global/validation.md

# PHASE 2: Analyze Codebase

Now that you've gathered information from external sources, proceed to analyze the existing codebase to infer product information automatically.

# Analyze Codebase for Product Information

Analyze existing codebase to infer product information automatically for adapting an existing codebase into product documentation.

## Step 1: Software-Agnostic Codebase Traversal

Traverse the existing codebase to find relevant files using software-agnostic approach:

```bash
# Traverse codebase to identify project structure
# This should work for any project type (Node.js, Python, Ruby, Go, Java, etc.)
# Look for common files and directory patterns

# Find top-level directories
find . -maxdepth 1 -type d ! -name "." ! -name ".git" ! -name "node_modules" ! -name "vendor" | sort

# Find common root-level files that indicate project type
find . -maxdepth 1 -type f -name "package.json" -o -name "requirements.txt" -o -name "Gemfile" -o -name "go.mod" -o -name "pom.xml" -o -name "build.gradle" -o -name "README*" | sort

# Identify main source directories (common patterns across languages)
find . -maxdepth 2 -type d \( -name "src" -o -name "lib" -o -name "app" -o -name "src/main" \) | sort
```

Analyze code structure, directories, and file organization:

1. **Detect project type without hardcoding specific technologies:**
   - Look for configuration files (package.json, requirements.txt, Gemfile, go.mod, etc.)
   - Identify file extensions patterns (.js, .py, .rb, .go, .java, etc.)
   - Analyze directory naming conventions (src/, lib/, app/, etc.)
   - Use tech-stack agnostic descriptions in analysis (avoid using specific technology names where possible)

2. **Identify main directories, modules, and components:**
   - Traverse directory structure recursively
   - Identify source code directories vs configuration vs documentation
   - Map directory organization to logical modules/components
   - Identify patterns in file naming and organization

3. **Use tech-stack agnostic descriptions in analysis:**
   - Describe patterns in generic terms (e.g., "dependency management file" instead of "package.json")
   - Focus on structural patterns rather than specific technologies

## Step 2: Configuration File Analysis

Detect and parse dependency/configuration files across different project types:

```bash
# Check for common dependency/configuration files
# Node.js
if [ -f "package.json" ]; then
    echo "Found Node.js project"
    cat package.json
fi

# Python
if [ -f "requirements.txt" ]; then
    echo "Found Python requirements"
    cat requirements.txt
fi
if [ -f "setup.py" ] || [ -f "pyproject.toml" ] || [ -f "Pipfile" ]; then
    echo "Found Python project configuration"
fi

# Ruby
if [ -f "Gemfile" ]; then
    echo "Found Ruby project"
    cat Gemfile
fi

# Go
if [ -f "go.mod" ]; then
    echo "Found Go project"
    cat go.mod
fi

# Java
if [ -f "pom.xml" ] || [ -f "build.gradle" ]; then
    echo "Found Java project"
fi

# Other common files
find . -maxdepth 1 -type f \( -name "*.toml" -o -name "*.yaml" -o -name "*.yml" -o -name "*.json" -o -name "Cargo.toml" -o -name "composer.json" \) | head -10
```

Detect and parse dependency/configuration files for these project types:
- **Node.js**: package.json, package-lock.json, yarn.lock, pnpm-lock.yaml
- **Python**: requirements.txt, setup.py, pyproject.toml, Pipfile, Pipfile.lock
- **Ruby**: Gemfile, Gemfile.lock
- **Go**: go.mod, go.sum
- **Java**: pom.xml, build.gradle, build.gradle.kts
- **Rust**: Cargo.toml, Cargo.lock
- **PHP**: composer.json, composer.lock
- **Other**: Look for common dependency and build configuration files

Extract from configuration files:
- **Tech stack information**: Frameworks, libraries, tools, runtime versions
- **Build and deployment configuration**: Build scripts, deployment settings, environment configurations
- **Project metadata**: Name, version, description, author, license
- **Dependencies**: Production and development dependencies, their versions and purposes

Handle multiple configuration file formats gracefully:
- Parse JSON, YAML, TOML, and text-based formats
- Handle missing or malformed configuration files
- Extract relevant information even if file structure varies

## Step 3: Documentation Extraction

Read and analyze documentation files:

```bash
# Find README files
find . -maxdepth 2 -type f -iname "README*" | head -5

# Find documentation directories
find . -maxdepth 2 -type d \( -name "docs" -o -name "documentation" -o -name "wiki" -o -name "doc" \) | head -5

# Find common documentation files
find . -maxdepth 2 -type f \( -iname "CHANGELOG*" -o -iname "CONTRIBUTING*" -o -iname "LICENSE*" -o -iname "HISTORY*" \) | head -10
```

Read and analyze documentation files:
- **README files**: README.md, README.txt, README.rst, README.markdown, etc.
- **Documentation directories**: docs/, documentation/, wiki/, doc/, guides/
- **Configuration docs**: CHANGELOG.md, CONTRIBUTING.md, LICENSE, HISTORY.md, etc.
- **Project-specific docs**: Any files in documentation directories

Extract from documentation:
- **Product description and purpose**: What the product does, why it exists
- **Feature lists and capabilities**: Key features, functionality, capabilities
- **User guides and use cases**: How users interact with the product, common use cases
- **Goals and objectives**: Product goals, mission, objectives
- **Architecture information**: System architecture, design decisions
- **Installation and setup**: How to set up and run the product

Process documentation in various formats:
- **Markdown**: .md, .markdown files
- **Text**: .txt files
- **ReStructuredText**: .rst files
- **HTML**: .html documentation files
- Extract text content from all formats, preserving structure where possible

## Step 4: Code Structure Analysis

Analyze directory structure for project architecture:

```bash
# Analyze directory structure
# Get top-level structure
tree -L 2 -d -I 'node_modules|vendor|.git' . 2>/dev/null || find . -maxdepth 2 -type d ! -path "*/node_modules/*" ! -path "*/.git/*" ! -path "*/vendor/*" | sort

# Identify common architectural patterns
# Look for MVC, module-based, feature-based, or layered architectures
# Common patterns: src/, lib/, app/, components/, modules/, features/, services/, controllers/, models/, views/
```

Analyze directory structure for project architecture:

1. **Identify main modules, components, and features from code organization:**
   - Map directory structure to logical modules
   - Identify feature-based vs module-based organization
   - Recognize patterns: MVC, layered architecture, microservices, monolith
   - Extract component boundaries from directory structure

2. **Infer product purpose from codebase structure:**
   - Analyze naming conventions in directories and files
   - Identify domain concepts from structure (e.g., user/, product/, order/ directories)
   - Infer functionality from organizational patterns
   - Map code organization to business features

3. **Map code organization to feature sets:**
   - Extract feature names from directory/folder names
   - Identify related code groupings
   - Understand feature boundaries and relationships
   - Build feature inventory from structure

4. **Extract architectural patterns:**
   - Identify architectural style (MVC, REST API, CLI, library, etc.)
   - Recognize framework patterns (if framework usage is evident)
   - Understand separation of concerns from structure
   - Note design patterns evident in organization

Use software-agnostic analysis that works with any project type:
- Focus on structural patterns rather than language-specific conventions
- Describe patterns in generic terms
- Avoid assumptions about specific technologies

## Step 5: Merge Codebase Findings with Other Sources

Integrate codebase analysis results into product knowledge:
- Combine codebase findings with user input, documents, and scraped content
- Resolve conflicts and prioritize information sources appropriately
- Create comprehensive product understanding from all sources

Prioritization guidance:
1. User-provided information (highest priority)
2. Inheritance folder documents
3. Scraped web content and link analysis
4. Codebase analysis findings
5. Web search results (if requested)



## User Standards & Preferences Compliance

## User Standards & Preferences Compliance

When analyzing the codebase, use tech-stack agnostic descriptions and follow the user's standards and preferences, as documented in these files:

@agent-os/standards/global/codebase-analysis.md
@agent-os/standards/global/coding-style.md
@agent-os/standards/global/commenting.md
@agent-os/standards/global/conventions.md
@agent-os/standards/global/enriched-knowledge-templates.md
@agent-os/standards/global/error-handling.md
@agent-os/standards/global/project-profile-schema.md
@agent-os/standards/global/tech-stack.md
@agent-os/standards/global/validation-commands.md
@agent-os/standards/global/validation.md

# PHASE 3: Create Mission

Now that you've gathered information from all sources and analyzed the codebase, use that unified product knowledge to create the mission document in `agent-os/product/mission.md` by following these instructions:

Create `agent-os/product/mission.md` with comprehensive product definition following this structure for its' content:

#### Mission Structure:
```markdown
# Product Mission

## Pitch
[PRODUCT_NAME] is a [PRODUCT_TYPE] that helps [TARGET_USERS] [SOLVE_PROBLEM]
by providing [KEY_VALUE_PROPOSITION].

## Users

### Primary Customers
- [CUSTOMER_SEGMENT_1]: [DESCRIPTION]
- [CUSTOMER_SEGMENT_2]: [DESCRIPTION]

### User Personas
**[USER_TYPE]** ([AGE_RANGE])
- **Role:** [JOB_TITLE/CONTEXT]
- **Context:** [BUSINESS/PERSONAL_CONTEXT]
- **Pain Points:** [SPECIFIC_PROBLEMS]
- **Goals:** [DESIRED_OUTCOMES]

## The Problem

### [PROBLEM_TITLE]
[PROBLEM_DESCRIPTION]. [QUANTIFIABLE_IMPACT].

**Our Solution:** [SOLUTION_APPROACH]

## Differentiators

### [DIFFERENTIATOR_TITLE]
Unlike [COMPETITOR/ALTERNATIVE], we provide [SPECIFIC_ADVANTAGE].
This results in [MEASURABLE_BENEFIT].

## Key Features

### Core Features
- **[FEATURE_NAME]:** [USER_BENEFIT_DESCRIPTION]

### Collaboration Features
- **[FEATURE_NAME]:** [USER_BENEFIT_DESCRIPTION]

### Advanced Features
- **[FEATURE_NAME]:** [USER_BENEFIT_DESCRIPTION]
```

#### Important Constraints

- **Focus on user benefits** in feature descriptions, not technical details
- **Keep it concise** and easy for users to scan and get the more important concepts quickly



## User Standards & Preferences Compliance

IMPORTANT: Ensure the product mission is ALIGNED and DOES NOT CONFLICT with the user's preferences and standards as detailed in the following files:

@agent-os/standards/global/codebase-analysis.md
@agent-os/standards/global/coding-style.md
@agent-os/standards/global/commenting.md
@agent-os/standards/global/conventions.md
@agent-os/standards/global/enriched-knowledge-templates.md
@agent-os/standards/global/error-handling.md
@agent-os/standards/global/project-profile-schema.md
@agent-os/standards/global/tech-stack.md
@agent-os/standards/global/validation-commands.md
@agent-os/standards/global/validation.md

# PHASE 4: Create Roadmap

Now that you've created this product's mission.md, use that to guide your creation of the roadmap in `agent-os/product/roadmap.md` by following these instructions:

Generate `agent-os/product/roadmap.md` with an ordered feature checklist:

Do not include any tasks for initializing a new codebase or bootstrapping a new application. Assume the user is already inside the project's codebase and has a bare-bones application initialized.

#### Creating the Roadmap:

1. **Review the Mission** - Read `agent-os/product/mission.md` to understand the product's goals, target users, and success criteria.

2. **Identify Features** - Based on the mission, determine the list of concrete features needed to achieve the product vision.

3. **Strategic Ordering** - Order features based on:
   - Technical dependencies (foundational features first)
   - Most direct path to achieving the mission
   - Building incrementally from MVP to full product

4. **Create the Roadmap** - Use the structure below as your template. Replace all bracketed placeholders (e.g., `[FEATURE_NAME]`, `[DESCRIPTION]`, `[EFFORT]`) with real content that you create based on the mission.

#### Roadmap Structure:
```markdown
# Product Roadmap

1. [ ] [FEATURE_NAME] â€” [1-2 SENTENCE DESCRIPTION OF COMPLETE, TESTABLE FEATURE] `[EFFORT]`
2. [ ] [FEATURE_NAME] â€” [1-2 SENTENCE DESCRIPTION OF COMPLETE, TESTABLE FEATURE] `[EFFORT]`
3. [ ] [FEATURE_NAME] â€” [1-2 SENTENCE DESCRIPTION OF COMPLETE, TESTABLE FEATURE] `[EFFORT]`
4. [ ] [FEATURE_NAME] â€” [1-2 SENTENCE DESCRIPTION OF COMPLETE, TESTABLE FEATURE] `[EFFORT]`
5. [ ] [FEATURE_NAME] â€” [1-2 SENTENCE DESCRIPTION OF COMPLETE, TESTABLE FEATURE] `[EFFORT]`
6. [ ] [FEATURE_NAME] â€” [1-2 SENTENCE DESCRIPTION OF COMPLETE, TESTABLE FEATURE] `[EFFORT]`
7. [ ] [FEATURE_NAME] â€” [1-2 SENTENCE DESCRIPTION OF COMPLETE, TESTABLE FEATURE] `[EFFORT]`
8. [ ] [FEATURE_NAME] â€” [1-2 SENTENCE DESCRIPTION OF COMPLETE, TESTABLE FEATURE] `[EFFORT]`

> Notes
> - Order items by technical dependencies and product architecture
> - Each item should represent an end-to-end functional and testable feature
```

Effort scale:
- `XS`: 1 day
- `S`: 2-3 days
- `M`: 1 week
- `L`: 2 weeks
- `XL`: 3+ weeks

#### Important Constraints

- **Make roadmap actionable** - include effort estimates and dependencies
- **Priorities guided by mission** - When deciding on order, aim for the most direct path to achieving the mission as documented in mission.md
- **Ensure phases are achievable** - start with MVP, build incrementally



## User Standards & Preferences Compliance

IMPORTANT: Ensure the product roadmap is ALIGNED and DOES NOT CONFLICT with the user's preferences and standards as detailed in the following files:

@agent-os/standards/global/codebase-analysis.md
@agent-os/standards/global/coding-style.md
@agent-os/standards/global/commenting.md
@agent-os/standards/global/conventions.md
@agent-os/standards/global/enriched-knowledge-templates.md
@agent-os/standards/global/error-handling.md
@agent-os/standards/global/project-profile-schema.md
@agent-os/standards/global/tech-stack.md
@agent-os/standards/global/validation-commands.md
@agent-os/standards/global/validation.md

# PHASE 5: Create Tech Stack

The final part of our product planning process is to document this product's tech stack in `agent-os/product/tech-stack.md`.  Follow these instructions to do so:

Create `agent-os/product/tech-stack.md` with a list of all tech stack choices that cover all aspects of this product's codebase.

### Creating the Tech Stack document

#### Step 1: Note User's Input Regarding Tech Stack

IF the user has provided specific information in the current conversation in regards to tech stack choices, these notes ALWAYS take precidence.  These must be reflected in your final `tech-stack.md` document that you will create.

#### Step 2: Gather User's Default Tech Stack Information

Reconcile and fill in the remaining gaps in the tech stack list by finding, reading and analyzing information regarding the tech stack.  Find this information in the following sources, in this order:

1. If user has provided their default tech stack under "User Standards & Preferences Compliance", READ and analyze this document.
2. If the current project has any of these files, read them to find information regarding tech stack choices for this codebase:
  - `claude.md`
  - `agents.md`

#### Step 3: Create the Tech Stack Document

Create `agent-os/product/tech-stack.md` and populate it with the final list of all technical stack choices, reconciled between the information the user has provided to you and the information found in provided sources.



## User Standards & Preferences Compliance

The user may provide information regarding their tech stack, which should take precedence when documenting the product's tech stack.  To fill in any gaps, find the user's usual tech stack information as documented in any of these files:

@agent-os/standards/global/codebase-analysis.md
@agent-os/standards/global/coding-style.md
@agent-os/standards/global/commenting.md
@agent-os/standards/global/conventions.md
@agent-os/standards/global/enriched-knowledge-templates.md
@agent-os/standards/global/error-handling.md
@agent-os/standards/global/project-profile-schema.md
@agent-os/standards/global/tech-stack.md
@agent-os/standards/global/validation-commands.md
@agent-os/standards/global/validation.md

# PHASE 6: Review And Combine Knowledge

Now that all product documents have been created, review and combine the knowledge to ensure consistency and completeness.

## Step 1: Load All Product Documents

Read and analyze all created product documents:

```bash
# Load all product files
MISSION=$(cat agent-os/product/mission.md 2>/dev/null)
ROADMAP=$(cat agent-os/product/roadmap.md 2>/dev/null)
TECH_STACK=$(cat agent-os/product/tech-stack.md 2>/dev/null)

# Verify all files exist
if [ -z "$MISSION" ] || [ -z "$ROADMAP" ] || [ -z "$TECH_STACK" ]; then
    echo "âš ï¸  Warning: Some product files are missing"
fi
```

## Step 2: Verify Consistency Between Documents

Analyze the documents together to verify consistency:

1. **Mission-Roadmap Alignment**: Verify roadmap features support the mission goals
2. **Roadmap-Tech Stack Alignment**: Verify tech stack supports roadmap features
3. **Mission-Tech Stack Alignment**: Verify tech choices align with product vision

Check for:
- Contradictions between documents
- Missing connections between mission goals and roadmap items
- Tech stack gaps that would block roadmap features
- Terminology consistency across all documents

## Step 3: Identify Gaps or Issues

Document any issues found:
- Missing information that should be added
- Inconsistencies that need resolution
- Questions that need user clarification

## Step 4: Create Product Summary Report

Create a summary document that combines insights from all product files:

```bash
mkdir -p agent-os/output/adapt-to-product/reports

cat > agent-os/output/adapt-to-product/reports/product-summary.md << 'EOF'
# Product Documentation Summary

## Documents Created
- `agent-os/product/mission.md` - Product mission and vision
- `agent-os/product/roadmap.md` - Development roadmap and phases
- `agent-os/product/tech-stack.md` - Technical stack documentation

## Consistency Analysis

### Mission-Roadmap Alignment
[Summary of how roadmap features support mission goals]

### Tech Stack Coverage
[Summary of how tech stack supports all planned features]

### Cross-Document Consistency
[Summary of terminology and concept consistency]

## Identified Gaps (if any)
[List any gaps or missing information]

## Recommendations
[Suggestions for improvements or additions]

## Next Steps
Ready to proceed with codebase analysis and basepoints generation.
EOF
```


## User Standards & Preferences Compliance

Ensure the review process follows the user's standards and preferences as documented in these files:

@agent-os/standards/global/codebase-analysis.md
@agent-os/standards/global/coding-style.md
@agent-os/standards/global/commenting.md
@agent-os/standards/global/conventions.md
@agent-os/standards/global/enriched-knowledge-templates.md
@agent-os/standards/global/error-handling.md
@agent-os/standards/global/project-profile-schema.md
@agent-os/standards/global/tech-stack.md
@agent-os/standards/global/validation-commands.md
@agent-os/standards/global/validation.md

# PHASE 7: Product Focused Cleanup

Now that product files (mission, tech-stack, roadmap) have been created, run the product-focused cleanup workflow to simplify and enhance agent-os files based on the detected product scope.

## Core Responsibilities

1. **Run Cleanup Workflow**: Execute the product-focused cleanup workflow to process agent-os files
2. **Review Results**: Present the cleanup report to the user for review
3. **Proceed to Next Phase**: Guide user to the final phase

## Workflow

Execute the product-focused cleanup workflow:

# Product-Focused Cleanup Workflow

After product files (mission, tech-stack, roadmap) are created, this workflow cleans irrelevant content from agent-os files and enhances remaining content with product-specific knowledge.

## Core Responsibilities

1. **Detect Product Scope**: Load and parse product files to understand language, framework, project type, and architecture
2. **Simplify Files**: Remove irrelevant technology examples, workflows, and patterns not matching the product scope
3. **Expand Knowledge**: Enhance remaining content with product-specific examples and web-validated best practices
4. **Generate Report**: Create a transparent report of all changes made

## Workflow

### Step 1: Load Product Scope Detection Sources

Load the product definition files to understand the project scope:

```bash
echo "ðŸ“‹ Loading product scope detection sources..."

# Initialize scope variables
DETECTED_LANGUAGE=""
DETECTED_FRAMEWORKS=""
DETECTED_PROJECT_TYPE=""
DETECTED_ARCHITECTURE=""

# Load tech-stack.md for language and framework detection
if [ -f "agent-os/product/tech-stack.md" ]; then
    TECH_STACK_CONTENT=$(cat agent-os/product/tech-stack.md)
    echo "âœ… Loaded tech-stack.md"
else
    echo "âš ï¸ tech-stack.md not found - scope detection limited"
fi

# Load mission.md for project type indicators
if [ -f "agent-os/product/mission.md" ]; then
    MISSION_CONTENT=$(cat agent-os/product/mission.md)
    echo "âœ… Loaded mission.md"
else
    echo "âš ï¸ mission.md not found - project type detection limited"
fi

# Load roadmap.md for feature scope
if [ -f "agent-os/product/roadmap.md" ]; then
    ROADMAP_CONTENT=$(cat agent-os/product/roadmap.md)
    echo "âœ… Loaded roadmap.md"
else
    echo "âš ï¸ roadmap.md not found - feature scope detection limited"
fi
```

### Step 2: Parse and Categorize Detected Scope

Analyze product files to categorize the project scope:

```bash
echo "ðŸ” Parsing and categorizing product scope..."

# Language Detection
detect_language() {
    local tech_stack="$1"
    local tech_lower=$(echo "$tech_stack" | tr '[:upper:]' '[:lower:]')
    
    # Check for common languages
    if echo "$tech_lower" | grep -qE "typescript|\.ts\b"; then
        echo "typescript"
    elif echo "$tech_lower" | grep -qE "javascript|\.js\b|node\.js|nodejs"; then
        echo "javascript"
    elif echo "$tech_lower" | grep -qE "python|\.py\b|django|flask|fastapi"; then
        echo "python"
    elif echo "$tech_lower" | grep -qE "rust|cargo|\.rs\b"; then
        echo "rust"
    elif echo "$tech_lower" | grep -qE "golang|go\b|\.go\b"; then
        echo "go"
    elif echo "$tech_lower" | grep -qE "ruby|rails|\.rb\b"; then
        echo "ruby"
    elif echo "$tech_lower" | grep -qE "java\b|spring|\.java\b"; then
        echo "java"
    elif echo "$tech_lower" | grep -qE "c#|csharp|\.net|dotnet"; then
        echo "csharp"
    elif echo "$tech_lower" | grep -qE "php|laravel|symfony"; then
        echo "php"
    elif echo "$tech_lower" | grep -qE "swift|ios|swiftui"; then
        echo "swift"
    elif echo "$tech_lower" | grep -qE "kotlin|android"; then
        echo "kotlin"
    else
        echo "unknown"
    fi
}

# Framework Detection
detect_frameworks() {
    local tech_stack="$1"
    local tech_lower=$(echo "$tech_stack" | tr '[:upper:]' '[:lower:]')
    local frameworks=""
    
    # Frontend frameworks
    echo "$tech_lower" | grep -qE "react|reactjs" && frameworks="$frameworks react"
    echo "$tech_lower" | grep -qE "vue|vuejs" && frameworks="$frameworks vue"
    echo "$tech_lower" | grep -qE "angular" && frameworks="$frameworks angular"
    echo "$tech_lower" | grep -qE "svelte" && frameworks="$frameworks svelte"
    echo "$tech_lower" | grep -qE "next\.js|nextjs" && frameworks="$frameworks nextjs"
    echo "$tech_lower" | grep -qE "nuxt" && frameworks="$frameworks nuxt"
    
    # Backend frameworks
    echo "$tech_lower" | grep -qE "express|expressjs" && frameworks="$frameworks express"
    echo "$tech_lower" | grep -qE "fastify" && frameworks="$frameworks fastify"
    echo "$tech_lower" | grep -qE "nest\.js|nestjs" && frameworks="$frameworks nestjs"
    echo "$tech_lower" | grep -qE "django" && frameworks="$frameworks django"
    echo "$tech_lower" | grep -qE "flask" && frameworks="$frameworks flask"
    echo "$tech_lower" | grep -qE "fastapi" && frameworks="$frameworks fastapi"
    echo "$tech_lower" | grep -qE "rails" && frameworks="$frameworks rails"
    echo "$tech_lower" | grep -qE "spring" && frameworks="$frameworks spring"
    echo "$tech_lower" | grep -qE "laravel" && frameworks="$frameworks laravel"
    
    # Mobile frameworks
    echo "$tech_lower" | grep -qE "react.native" && frameworks="$frameworks react-native"
    echo "$tech_lower" | grep -qE "flutter" && frameworks="$frameworks flutter"
    echo "$tech_lower" | grep -qE "expo" && frameworks="$frameworks expo"
    
    echo "$frameworks" | xargs
}

# Project Type Detection
detect_project_type() {
    local mission="$1"
    local roadmap="$2"
    local combined=$(echo "$mission $roadmap" | tr '[:upper:]' '[:lower:]')
    
    # Check for project type indicators
    if echo "$combined" | grep -qE "cli|command.line|terminal|console.app"; then
        echo "cli"
    elif echo "$combined" | grep -qE "api|rest|graphql|backend|server|microservice"; then
        echo "api"
    elif echo "$combined" | grep -qE "web.app|webapp|frontend|dashboard|portal|spa"; then
        echo "webapp"
    elif echo "$combined" | grep -qE "mobile|ios|android|app.store|play.store"; then
        echo "mobile"
    elif echo "$combined" | grep -qE "library|package|sdk|npm|pip|crate"; then
        echo "library"
    elif echo "$combined" | grep -qE "ai|ml|machine.learning|llm|agent|model"; then
        echo "ai-service"
    elif echo "$combined" | grep -qE "full.stack|fullstack"; then
        echo "fullstack"
    else
        echo "unknown"
    fi
}

# Architecture Detection
detect_architecture() {
    local tech_stack="$1"
    local roadmap="$2"
    local combined=$(echo "$tech_stack $roadmap" | tr '[:upper:]' '[:lower:]')
    
    if echo "$combined" | grep -qE "microservice|micro.service|distributed|kubernetes|k8s"; then
        echo "microservices"
    elif echo "$combined" | grep -qE "serverless|lambda|cloud.function|faas"; then
        echo "serverless"
    elif echo "$combined" | grep -qE "monolith|monolithic|single.app"; then
        echo "monolith"
    elif echo "$combined" | grep -qE "event.driven|event.sourcing|cqrs"; then
        echo "event-driven"
    else
        echo "monolith"  # Default to monolith
    fi
}

# Run detection
DETECTED_LANGUAGE=$(detect_language "$TECH_STACK_CONTENT")
DETECTED_FRAMEWORKS=$(detect_frameworks "$TECH_STACK_CONTENT")
DETECTED_PROJECT_TYPE=$(detect_project_type "$MISSION_CONTENT" "$ROADMAP_CONTENT")
DETECTED_ARCHITECTURE=$(detect_architecture "$TECH_STACK_CONTENT" "$ROADMAP_CONTENT")

echo ""
echo "ðŸ“Š Detected Product Scope:"
echo "   Language: $DETECTED_LANGUAGE"
echo "   Frameworks: $DETECTED_FRAMEWORKS"
echo "   Project Type: $DETECTED_PROJECT_TYPE"
echo "   Architecture: $DETECTED_ARCHITECTURE"

# Save scope detection results
mkdir -p agent-os/output/product-cleanup
cat > agent-os/output/product-cleanup/detected-scope.yml << EOF
# Product Scope Detection Results
# Generated by product-focused-cleanup workflow

language: $DETECTED_LANGUAGE
frameworks: [$DETECTED_FRAMEWORKS]
project_type: $DETECTED_PROJECT_TYPE
architecture: $DETECTED_ARCHITECTURE

detection_sources:
  tech_stack: agent-os/product/tech-stack.md
  mission: agent-os/product/mission.md
  roadmap: agent-os/product/roadmap.md
EOF

echo "âœ… Scope detection complete"
```

### Step 3: Phase A - Simplify (Remove Irrelevant Content)

Remove content that doesn't match the detected product scope:

```bash
echo ""
echo "ðŸ§¹ Phase A: Simplifying agent-os files..."

# Initialize tracking
REMOVED_FILES=()
MODIFIED_FILES=()
REMOVED_CONTENT=()

# Define what to remove based on detected scope
define_removal_rules() {
    local lang="$1"
    local proj_type="$2"
    local arch="$3"
    
    # Language-based removal patterns
    case "$lang" in
        "typescript"|"javascript")
            REMOVE_LANG_PATTERNS="python ruby rust go java csharp php swift kotlin"
            ;;
        "python")
            REMOVE_LANG_PATTERNS="typescript javascript ruby rust go java csharp php swift kotlin"
            ;;
        "rust")
            REMOVE_LANG_PATTERNS="typescript javascript python ruby go java csharp php swift kotlin"
            ;;
        "go")
            REMOVE_LANG_PATTERNS="typescript javascript python ruby rust java csharp php swift kotlin"
            ;;
        *)
            REMOVE_LANG_PATTERNS=""
            ;;
    esac
    
    # Project type-based removal patterns
    case "$proj_type" in
        "cli")
            REMOVE_TYPE_PATTERNS="ui frontend screen design mockup component widget button form modal"
            ;;
        "api")
            REMOVE_TYPE_PATTERNS="ui frontend screen design mockup component widget button form modal mobile ios android"
            ;;
        "webapp")
            REMOVE_TYPE_PATTERNS="mobile ios android cli terminal console"
            ;;
        "mobile")
            REMOVE_TYPE_PATTERNS="cli terminal console"
            ;;
        "library")
            REMOVE_TYPE_PATTERNS="ui frontend screen design mockup mobile ios android"
            ;;
        "ai-service")
            REMOVE_TYPE_PATTERNS="crud traditional-web mobile ios android"
            ;;
        *)
            REMOVE_TYPE_PATTERNS=""
            ;;
    esac
    
    # Architecture-based removal patterns
    case "$arch" in
        "monolith")
            REMOVE_ARCH_PATTERNS="microservice kubernetes k8s service-mesh distributed"
            ;;
        "microservices")
            REMOVE_ARCH_PATTERNS="monolith single-app"
            ;;
        "serverless")
            REMOVE_ARCH_PATTERNS="monolith traditional-server"
            ;;
        *)
            REMOVE_ARCH_PATTERNS=""
            ;;
    esac
}

# Run removal rules definition
define_removal_rules "$DETECTED_LANGUAGE" "$DETECTED_PROJECT_TYPE" "$DETECTED_ARCHITECTURE"

# Process each agent-os directory
process_directory_for_removal() {
    local dir="$1"
    local dir_name=$(basename "$dir")
    
    echo "   Processing $dir_name/..."
    
    # Find all markdown files
    for file in $(find "$dir" -name "*.md" -type f 2>/dev/null); do
        local file_name=$(basename "$file")
        local file_content=$(cat "$file")
        local file_lower=$(echo "$file_content" | tr '[:upper:]' '[:lower:]')
        local should_remove=false
        local removal_reason=""
        
        # Check language patterns
        for pattern in $REMOVE_LANG_PATTERNS; do
            if echo "$file_lower" | grep -qiE "\b${pattern}\b.*example|\b${pattern}\b.*pattern"; then
                # Check if this is a significant match (not just a mention)
                match_count=$(echo "$file_lower" | grep -oiE "\b${pattern}\b" | wc -l)
                if [ "$match_count" -gt 3 ]; then
                    should_remove=true
                    removal_reason="Language not in scope: $pattern"
                    break
                fi
            fi
        done
        
        # Check project type patterns (for workflow files)
        if [ "$should_remove" = false ] && [[ "$dir" == *"workflows"* ]]; then
            for pattern in $REMOVE_TYPE_PATTERNS; do
                if echo "$file_name $file_lower" | grep -qiE "\b${pattern}\b"; then
                    should_remove=true
                    removal_reason="Project type not in scope: $pattern"
                    break
                fi
            done
        fi
        
        # Instead of removing files, mark sections for removal or simplify
        if [ "$should_remove" = true ]; then
            REMOVED_CONTENT+=("$file: $removal_reason")
            # Add marker comment at top of file
            echo "<!-- PRODUCT-CLEANUP: This file contains content outside product scope ($removal_reason). Consider reviewing or removing. -->" | cat - "$file" > temp && mv temp "$file"
            MODIFIED_FILES+=("$file")
        fi
    done
}

# Process directories
for dir in agent-os/commands agent-os/workflows agent-os/standards agent-os/agents; do
    if [ -d "$dir" ]; then
        process_directory_for_removal "$dir"
    fi
done

echo "âœ… Phase A complete: ${#MODIFIED_FILES[@]} files marked for review"
```

### Step 4: Phase B - Expand (Enhance with Product-Specific Knowledge)

Enhance remaining content with product-specific examples and patterns:

```bash
echo ""
echo "ðŸ“š Phase B: Expanding with product-specific knowledge..."

# Initialize tracking
ENHANCED_FILES=()
ADDED_CONTENT=()

# Define enhancement rules based on detected scope
define_enhancement_rules() {
    local lang="$1"
    local frameworks="$2"
    local proj_type="$3"
    
    # Language-specific enhancements
    case "$lang" in
        "typescript")
            LANG_PATTERNS="TypeScript type safety, interfaces, generics, strict mode"
            LANG_EXAMPLES="Use TypeScript interfaces for data structures, leverage type inference"
            ;;
        "python")
            LANG_PATTERNS="Python type hints, dataclasses, async/await patterns"
            LANG_EXAMPLES="Use type hints for function signatures, leverage dataclasses for DTOs"
            ;;
        "rust")
            LANG_PATTERNS="Rust ownership, borrowing, lifetimes, Result/Option patterns"
            LANG_EXAMPLES="Use Result for error handling, leverage pattern matching"
            ;;
        "go")
            LANG_PATTERNS="Go interfaces, goroutines, channels, error handling"
            LANG_EXAMPLES="Use interfaces for abstraction, leverage goroutines for concurrency"
            ;;
        *)
            LANG_PATTERNS=""
            LANG_EXAMPLES=""
            ;;
    esac
    
    # Framework-specific enhancements
    FRAMEWORK_PATTERNS=""
    for fw in $frameworks; do
        case "$fw" in
            "react")
                FRAMEWORK_PATTERNS="$FRAMEWORK_PATTERNS React hooks, functional components, state management"
                ;;
            "express")
                FRAMEWORK_PATTERNS="$FRAMEWORK_PATTERNS Express middleware, routing, error handling"
                ;;
            "fastapi")
                FRAMEWORK_PATTERNS="$FRAMEWORK_PATTERNS FastAPI dependency injection, Pydantic models, async endpoints"
                ;;
            "nextjs")
                FRAMEWORK_PATTERNS="$FRAMEWORK_PATTERNS Next.js App Router, Server Components, API routes"
                ;;
        esac
    done
    
    # Project type enhancements
    case "$proj_type" in
        "cli")
            TYPE_PATTERNS="CLI argument parsing, interactive prompts, progress indicators, exit codes"
            ;;
        "api")
            TYPE_PATTERNS="REST conventions, API versioning, authentication, rate limiting, documentation"
            ;;
        "webapp")
            TYPE_PATTERNS="Component architecture, state management, routing, responsive design"
            ;;
        "ai-service")
            TYPE_PATTERNS="Prompt engineering, model integration, context management, token optimization"
            ;;
        *)
            TYPE_PATTERNS=""
            ;;
    esac
}

# Run enhancement rules definition
define_enhancement_rules "$DETECTED_LANGUAGE" "$DETECTED_FRAMEWORKS" "$DETECTED_PROJECT_TYPE"

# Inject product terminology from mission/roadmap
inject_product_terminology() {
    local file="$1"
    local mission="$2"
    local roadmap="$3"
    
    # Extract key terms from mission and roadmap
    # This is a simplified version - in practice, use NLP or keyword extraction
    local key_terms=$(echo "$mission $roadmap" | grep -oE '\b[A-Z][a-z]+[A-Z][a-zA-Z]*\b' | sort -u | head -10)
    
    if [ -n "$key_terms" ]; then
        echo "   Injecting product terminology into $file"
        ENHANCED_FILES+=("$file")
    fi
}

# Create product-specific enhancement section
create_enhancement_section() {
    cat << EOF

## Product-Specific Patterns

Based on detected product scope ($DETECTED_LANGUAGE / $DETECTED_PROJECT_TYPE):

### Language Patterns
$LANG_PATTERNS

### Framework Patterns
$FRAMEWORK_PATTERNS

### Project Type Patterns
$TYPE_PATTERNS

EOF
}

echo "âœ… Phase B complete: Enhancement rules defined"
echo "   Language patterns: $LANG_PATTERNS"
echo "   Framework patterns: $FRAMEWORK_PATTERNS"
echo "   Project type patterns: $TYPE_PATTERNS"
```

### Step 5: Web Search for Trusted Information

Search for current best practices and validate with multiple sources:

```bash
echo ""
echo "ðŸŒ Searching for trusted information (2+ source validation)..."

# Define search categories
SEARCH_CATEGORIES=(
    "best practices"
    "common patterns"
    "anti-patterns"
    "performance optimization"
    "security considerations"
    "testing strategies"
)

# Generate search queries based on detected scope
generate_search_queries() {
    local lang="$1"
    local frameworks="$2"
    local proj_type="$3"
    
    local queries=()
    
    # Language-specific queries
    [ -n "$lang" ] && [ "$lang" != "unknown" ] && queries+=("$lang best practices 2024")
    
    # Framework-specific queries
    for fw in $frameworks; do
        queries+=("$fw best practices patterns")
        queries+=("$fw common mistakes anti-patterns")
    done
    
    # Project type queries
    case "$proj_type" in
        "cli")
            queries+=("CLI application best practices")
            queries+=("command line tool UX patterns")
            ;;
        "api")
            queries+=("REST API design best practices")
            queries+=("API security best practices")
            ;;
        "webapp")
            queries+=("web application architecture patterns")
            queries+=("frontend performance optimization")
            ;;
        "ai-service")
            queries+=("LLM application best practices")
            queries+=("AI agent design patterns")
            ;;
    esac
    
    printf '%s\n' "${queries[@]}"
}

# Generate queries
SEARCH_QUERIES=$(generate_search_queries "$DETECTED_LANGUAGE" "$DETECTED_FRAMEWORKS" "$DETECTED_PROJECT_TYPE")

echo "   Generated search queries:"
echo "$SEARCH_QUERIES" | while read query; do
    [ -n "$query" ] && echo "   - $query"
done

# Note: Actual web search would be performed here
# The workflow instructs the AI agent to:
# 1. Perform web searches for each query
# 2. Validate information appears in 2+ independent sources
# 3. Prioritize: Official docs > Reputable blogs > Stack Overflow
# 4. Add validated information to enhancement section

echo ""
echo "ðŸ“ Web Search Instructions for AI Agent:"
echo "   1. Search for each query listed above"
echo "   2. Validate information from 2+ independent sources"
echo "   3. Prioritize sources: Official docs > Reputable tech blogs > Stack Overflow (high-voted)"
echo "   4. Include version-specific information when available"
echo "   5. Add validated patterns to agent-os files"

# Save search queries for reference
cat > agent-os/output/product-cleanup/search-queries.md << EOF
# Web Search Queries for Product Enhancement

Generated based on detected scope:
- Language: $DETECTED_LANGUAGE
- Frameworks: $DETECTED_FRAMEWORKS
- Project Type: $DETECTED_PROJECT_TYPE

## Queries to Execute

$(echo "$SEARCH_QUERIES" | while read query; do
    [ -n "$query" ] && echo "- [ ] $query"
done)

## Validation Requirements

- Information must appear in 2+ independent sources
- Prioritize: Official documentation > Reputable tech blogs > Stack Overflow
- Include version numbers when applicable
- Note the sources for each piece of information added

## Categories to Cover

$(for cat in "${SEARCH_CATEGORIES[@]}"; do
    echo "- [ ] $cat"
done)
EOF

echo "âœ… Search queries saved to agent-os/output/product-cleanup/search-queries.md"
```

### Step 6: Generate Cleanup/Enhancement Report

Create a comprehensive report of all changes:

```bash
echo ""
echo "ðŸ“Š Generating cleanup/enhancement report..."

# Create report directory
mkdir -p agent-os/output/product-cleanup

# Generate the report
cat > agent-os/output/product-cleanup/cleanup-report.md << EOF
# Product-Focused Cleanup Report

Generated: $(date -u +"%Y-%m-%d %H:%M:%S UTC")

## Detected Product Scope

| Attribute | Detected Value |
|-----------|----------------|
| Language | $DETECTED_LANGUAGE |
| Frameworks | $DETECTED_FRAMEWORKS |
| Project Type | $DETECTED_PROJECT_TYPE |
| Architecture | $DETECTED_ARCHITECTURE |

## Phase A: Simplification Summary

### Files Marked for Review
$(if [ ${#MODIFIED_FILES[@]} -gt 0 ]; then
    for file in "${MODIFIED_FILES[@]}"; do
        echo "- $file"
    done
else
    echo "No files marked for review"
fi)

### Content Flagged as Out-of-Scope
$(if [ ${#REMOVED_CONTENT[@]} -gt 0 ]; then
    for content in "${REMOVED_CONTENT[@]}"; do
        echo "- $content"
    done
else
    echo "No content flagged"
fi)

### Removal Rules Applied

**Language-based removals:** $REMOVE_LANG_PATTERNS
**Project type-based removals:** $REMOVE_TYPE_PATTERNS
**Architecture-based removals:** $REMOVE_ARCH_PATTERNS

## Phase B: Enhancement Summary

### Enhancement Rules Applied

**Language Patterns:** $LANG_PATTERNS
**Framework Patterns:** $FRAMEWORK_PATTERNS
**Project Type Patterns:** $TYPE_PATTERNS

### Files Enhanced
$(if [ ${#ENHANCED_FILES[@]} -gt 0 ]; then
    for file in "${ENHANCED_FILES[@]}"; do
        echo "- $file"
    done
else
    echo "Enhancement pending web search validation"
fi)

## Web Search Status

See \`search-queries.md\` for queries to execute and validate.

## Next Steps

1. Review files marked with PRODUCT-CLEANUP comments
2. Execute web searches and validate information
3. Add validated patterns to relevant agent-os files
4. Re-run this workflow to verify completeness

---

*This report was generated by the product-focused-cleanup workflow.*
*Location: agent-os/output/product-cleanup/cleanup-report.md*
EOF

echo "âœ… Report generated: agent-os/output/product-cleanup/cleanup-report.md"
```

## Display Confirmation

Once cleanup is complete, output the following:

```
âœ… Product-Focused Cleanup Complete!

ðŸ“Š Detected Scope:
   Language: [detected language]
   Frameworks: [detected frameworks]
   Project Type: [detected project type]
   Architecture: [detected architecture]

ðŸ§¹ Phase A (Simplify):
   Files reviewed: [count]
   Content flagged: [count]

ðŸ“š Phase B (Expand):
   Enhancement rules defined
   Web search queries generated

ðŸ“ Reports Generated:
   - agent-os/output/product-cleanup/detected-scope.yml
   - agent-os/output/product-cleanup/search-queries.md
   - agent-os/output/product-cleanup/cleanup-report.md

NEXT STEP ðŸ‘‰ Review the cleanup report and execute web searches for validated enhancements.
```

{{UNLESS compiled_single_command}}
## Display confirmation and next step

Output the confirmation message above after completing the cleanup workflow.
{{ENDUNLESS compiled_single_command}}

{{UNLESS standards_as_claude_code_skills}}
## User Standards & Preferences Compliance

Ensure cleanup follows the user's preferences:

@agent-os/standards/global/codebase-analysis.md
@agent-os/standards/global/coding-style.md
@agent-os/standards/global/commenting.md
@agent-os/standards/global/conventions.md
@agent-os/standards/global/enriched-knowledge-templates.md
@agent-os/standards/global/error-handling.md
@agent-os/standards/global/project-profile-schema.md
@agent-os/standards/global/tech-stack.md
@agent-os/standards/global/validation-commands.md
@agent-os/standards/global/validation.md
{{ENDUNLESS standards_as_claude_code_skills}}


## Display Confirmation and Next Step

Once the cleanup workflow is complete, output the following message:

```
âœ… Product-Focused Cleanup Complete!

ðŸ“Š Detected Scope:
   Language: [detected language]
   Frameworks: [detected frameworks]
   Project Type: [detected project type]
   Architecture: [detected architecture]

ðŸ§¹ Phase A (Simplify):
   Files reviewed: [count]
   Content flagged: [count]

ðŸ“š Phase B (Expand):
   Enhancement rules defined
   Web search queries generated

ðŸ“ Reports Generated:
   - agent-os/output/product-cleanup/detected-scope.yml
   - agent-os/output/product-cleanup/search-queries.md
   - agent-os/output/product-cleanup/cleanup-report.md

NEXT STEP ðŸ‘‰ Proceeding to Phase 8: Navigate to Next Command
```

## User Standards & Preferences Compliance

Ensure cleanup follows the user's preferences:

@agent-os/standards/global/codebase-analysis.md
@agent-os/standards/global/coding-style.md
@agent-os/standards/global/commenting.md
@agent-os/standards/global/conventions.md
@agent-os/standards/global/enriched-knowledge-templates.md
@agent-os/standards/global/error-handling.md
@agent-os/standards/global/project-profile-schema.md
@agent-os/standards/global/tech-stack.md
@agent-os/standards/global/validation-commands.md
@agent-os/standards/global/validation.md

# PHASE 8: Navigate To Next Command

The adapt-to-product process is now complete. This final phase provides a summary of what was accomplished and guidance on next steps.

## Summary of Accomplishments

Display a comprehensive summary of the adapt-to-product command results:

### Files Created

The following product documentation files have been created:

| File | Location | Purpose |
|------|----------|---------|
| Mission | `agent-os/product/mission.md` | Product vision, goals, and strategic direction |
| Roadmap | `agent-os/product/roadmap.md` | Phased development plan with prioritized features |
| Tech Stack | `agent-os/product/tech-stack.md` | Technical stack and architecture decisions |
| Summary | `agent-os/output/adapt-to-product/reports/product-summary.md` | Combined knowledge and consistency analysis |

### Process Completed

1. âœ… Setup and Information Gathering
2. âœ… Codebase Analysis
3. âœ… Mission Document Creation
4. âœ… Roadmap Creation
5. âœ… Tech Stack Documentation
6. âœ… Knowledge Review and Combination

## Next Steps

Now that product documentation is complete, the recommended next step is to analyze your codebase structure and create basepoints that document your code patterns.

### Recommended Command

Run the **create-basepoints** command to:
- Mirror your project structure
- Detect abstraction layers
- Analyze code patterns
- Generate module and parent basepoints
- Create a headquarter file that bridges product and code knowledge

## Output

Display the following completion message:

```
ðŸŽ‰ adapt-to-product Complete!

**Product Documentation Created:**
â”œâ”€â”€ agent-os/product/
â”‚   â”œâ”€â”€ mission.md       - Product vision and goals
â”‚   â”œâ”€â”€ roadmap.md       - Development roadmap
â”‚   â””â”€â”€ tech-stack.md    - Technical stack
â””â”€â”€ agent-os/output/adapt-to-product/reports/
    â””â”€â”€ product-summary.md - Knowledge summary

**What's Next?**

Your product documentation is ready. To continue setting up your Agent OS:

ðŸ‘‰ Run `/create-basepoints` to analyze your codebase and generate pattern documentation.

This will create basepoints that document your code patterns, making it easier for AI agents to understand and work with your codebase.
```

## User Standards & Preferences Compliance

Ensure the navigation guidance follows the user's standards and preferences as documented in these files:

@agent-os/standards/global/codebase-analysis.md
@agent-os/standards/global/coding-style.md
@agent-os/standards/global/commenting.md
@agent-os/standards/global/conventions.md
@agent-os/standards/global/enriched-knowledge-templates.md
@agent-os/standards/global/error-handling.md
@agent-os/standards/global/project-profile-schema.md
@agent-os/standards/global/tech-stack.md
@agent-os/standards/global/validation-commands.md
@agent-os/standards/global/validation.md
