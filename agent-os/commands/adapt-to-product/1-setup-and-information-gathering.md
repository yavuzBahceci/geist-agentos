This begins a multi-step process for adapting an existing codebase into product documentation.

## Step 0: Automatic Project Detection & Research (NEW)

Before gathering product information, automatically detect project characteristics and enrich with web research:

```bash
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  ADAPTIVE PROJECT DETECTION"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
```

### 0.1: Run Project Detection

Automatically analyze the codebase to detect tech stack, commands, architecture, and security level:

```bash
# Workflow: Detect Project Profile

## Purpose

Main orchestrator that calls all detection functions, aggregates results into a unified project profile, calculates overall confidence score, and handles fallbacks for failed detections.

## Inputs

- Project root directory (current working directory)
- Optional: Existing `agent-os/config/project-profile.yml` (for incremental updates)

## Outputs

- `agent-os/config/project-profile.yml` - Unified project profile
- `agent-os/config/detected-profile.yml` - Raw detection results (cache)

---

## Workflow

### Step 1: Initialize Detection Environment

```bash
echo "ðŸ” Starting project detection..."

# Initialize variables
DETECTION_CONFIDENCE=0
DETECTIONS_RUN=0
DETECTIONS_SUCCESS=0

# Create config directory if needed
mkdir -p agent-os/config

# Initialize raw detection results
cat > agent-os/config/detected-profile.yml << 'INIT_EOF'
# Auto-detected project profile
# Generated by detect-project-profile workflow

detected_at: $(date -Iseconds)
INIT_EOF
```

### Step 2: Run Detection Workflows

Execute each detection workflow in sequence:

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  PROJECT DETECTION"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Track detection results
declare -A DETECTION_RESULTS
declare -A DETECTION_CONFIDENCE

# 1. Detect Tech Stack
echo "ðŸ”§ Detecting tech stack..."
# Workflow: Detect Tech Stack

## Purpose

Parse project configuration files to detect languages, frameworks, and databases. Supports Node.js, Rust, Go, and Python projects.

## Outputs

Sets the following variables:
- `DETECTED_LANGUAGE` - Primary language (javascript, typescript, rust, python, go)
- `DETECTED_FRAMEWORK` - Web framework if detected (react, vue, express, fastapi, etc.)
- `DETECTED_BACKEND` - Backend runtime (nodejs, rust, python, go)
- `DETECTED_DATABASE` - Database if detected (postgresql, mongodb, mysql, redis)
- `TECH_STACK_CONFIDENCE` - Confidence score (0.0 - 1.0)

---

## Detection Logic

### Step 1: Detect from package.json (Node.js/JavaScript/TypeScript)

```bash
if [ -f "package.json" ]; then
    echo "   Found package.json - analyzing..."
    
    # Base language
    DETECTED_LANGUAGE="javascript"
    DETECTED_BACKEND="nodejs"
    
    # Check for TypeScript
    if [ -f "tsconfig.json" ] || grep -q '"typescript"' package.json 2>/dev/null; then
        DETECTED_LANGUAGE="typescript"
    fi
    
    # Detect frameworks from dependencies
    DEPS=$(cat package.json)
    
    # Frontend frameworks
    if echo "$DEPS" | grep -q '"react"'; then
        DETECTED_FRAMEWORK="react"
    elif echo "$DEPS" | grep -q '"vue"'; then
        DETECTED_FRAMEWORK="vue"
    elif echo "$DEPS" | grep -q '"angular"'; then
        DETECTED_FRAMEWORK="angular"
    elif echo "$DEPS" | grep -q '"svelte"'; then
        DETECTED_FRAMEWORK="svelte"
    fi
    
    # Meta-frameworks (override if found)
    if echo "$DEPS" | grep -q '"next"'; then
        DETECTED_FRAMEWORK="nextjs"
    elif echo "$DEPS" | grep -q '"nuxt"'; then
        DETECTED_FRAMEWORK="nuxt"
    elif echo "$DEPS" | grep -q '"remix"'; then
        DETECTED_FRAMEWORK="remix"
    fi
    
    # Backend frameworks
    if echo "$DEPS" | grep -q '"express"'; then
        DETECTED_BACKEND="nodejs-express"
    elif echo "$DEPS" | grep -q '"fastify"'; then
        DETECTED_BACKEND="nodejs-fastify"
    elif echo "$DEPS" | grep -q '"koa"'; then
        DETECTED_BACKEND="nodejs-koa"
    elif echo "$DEPS" | grep -q '"hono"'; then
        DETECTED_BACKEND="nodejs-hono"
    fi
    
    # Database detection
    if echo "$DEPS" | grep -qE '"(pg|postgres|postgresql)"'; then
        DETECTED_DATABASE="postgresql"
    elif echo "$DEPS" | grep -qE '"(mysql|mysql2)"'; then
        DETECTED_DATABASE="mysql"
    elif echo "$DEPS" | grep -qE '"(mongodb|mongoose)"'; then
        DETECTED_DATABASE="mongodb"
    elif echo "$DEPS" | grep -q '"redis"'; then
        DETECTED_DATABASE="redis"
    elif echo "$DEPS" | grep -qE '"(prisma|@prisma)"'; then
        # Prisma - check schema for db type
        if [ -f "prisma/schema.prisma" ]; then
            if grep -q 'provider = "postgresql"' prisma/schema.prisma 2>/dev/null; then
                DETECTED_DATABASE="postgresql"
            elif grep -q 'provider = "mysql"' prisma/schema.prisma 2>/dev/null; then
                DETECTED_DATABASE="mysql"
            elif grep -q 'provider = "mongodb"' prisma/schema.prisma 2>/dev/null; then
                DETECTED_DATABASE="mongodb"
            fi
        fi
    fi
    
    TECH_STACK_CONFIDENCE="0.95"
fi
```

### Step 2: Detect from Cargo.toml (Rust)

```bash
if [ -f "Cargo.toml" ]; then
    echo "   Found Cargo.toml - analyzing..."
    
    DETECTED_LANGUAGE="rust"
    DETECTED_BACKEND="rust"
    
    CARGO=$(cat Cargo.toml)
    
    # Web frameworks
    if echo "$CARGO" | grep -q 'actix-web'; then
        DETECTED_FRAMEWORK="actix-web"
    elif echo "$CARGO" | grep -q 'axum'; then
        DETECTED_FRAMEWORK="axum"
    elif echo "$CARGO" | grep -q 'rocket'; then
        DETECTED_FRAMEWORK="rocket"
    elif echo "$CARGO" | grep -q 'warp'; then
        DETECTED_FRAMEWORK="warp"
    fi
    
    # Database
    if echo "$CARGO" | grep -qE '(sqlx|postgres)'; then
        DETECTED_DATABASE="postgresql"
    elif echo "$CARGO" | grep -q 'mysql'; then
        DETECTED_DATABASE="mysql"
    elif echo "$CARGO" | grep -q 'mongodb'; then
        DETECTED_DATABASE="mongodb"
    fi
    
    TECH_STACK_CONFIDENCE="0.95"
fi
```

### Step 3: Detect from go.mod (Go)

```bash
if [ -f "go.mod" ]; then
    echo "   Found go.mod - analyzing..."
    
    DETECTED_LANGUAGE="go"
    DETECTED_BACKEND="go"
    
    GOMOD=$(cat go.mod)
    
    # Web frameworks
    if echo "$GOMOD" | grep -q 'github.com/gin-gonic/gin'; then
        DETECTED_FRAMEWORK="gin"
    elif echo "$GOMOD" | grep -q 'github.com/labstack/echo'; then
        DETECTED_FRAMEWORK="echo"
    elif echo "$GOMOD" | grep -q 'github.com/gofiber/fiber'; then
        DETECTED_FRAMEWORK="fiber"
    elif echo "$GOMOD" | grep -q 'github.com/gorilla/mux'; then
        DETECTED_FRAMEWORK="gorilla-mux"
    fi
    
    # Database
    if echo "$GOMOD" | grep -qE '(lib/pq|jackc/pgx)'; then
        DETECTED_DATABASE="postgresql"
    elif echo "$GOMOD" | grep -q 'go-sql-driver/mysql'; then
        DETECTED_DATABASE="mysql"
    elif echo "$GOMOD" | grep -q 'mongo-driver'; then
        DETECTED_DATABASE="mongodb"
    fi
    
    TECH_STACK_CONFIDENCE="0.95"
fi
```

### Step 4: Detect from Python files

```bash
if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ] || [ -f "setup.py" ]; then
    echo "   Found Python project files - analyzing..."
    
    DETECTED_LANGUAGE="python"
    DETECTED_BACKEND="python"
    
    # Combine all dependency sources
    PYDEPS=""
    [ -f "requirements.txt" ] && PYDEPS="$PYDEPS $(cat requirements.txt)"
    [ -f "pyproject.toml" ] && PYDEPS="$PYDEPS $(cat pyproject.toml)"
    [ -f "setup.py" ] && PYDEPS="$PYDEPS $(cat setup.py)"
    
    # Web frameworks
    if echo "$PYDEPS" | grep -qi 'fastapi'; then
        DETECTED_FRAMEWORK="fastapi"
    elif echo "$PYDEPS" | grep -qi 'django'; then
        DETECTED_FRAMEWORK="django"
    elif echo "$PYDEPS" | grep -qi 'flask'; then
        DETECTED_FRAMEWORK="flask"
    elif echo "$PYDEPS" | grep -qi 'starlette'; then
        DETECTED_FRAMEWORK="starlette"
    fi
    
    # Database
    if echo "$PYDEPS" | grep -qiE '(psycopg|asyncpg|postgres)'; then
        DETECTED_DATABASE="postgresql"
    elif echo "$PYDEPS" | grep -qi 'mysql'; then
        DETECTED_DATABASE="mysql"
    elif echo "$PYDEPS" | grep -qiE '(pymongo|motor)'; then
        DETECTED_DATABASE="mongodb"
    elif echo "$PYDEPS" | grep -qi 'redis'; then
        DETECTED_DATABASE="redis"
    fi
    
    TECH_STACK_CONFIDENCE="0.90"
fi
```

### Step 5: Fallback - Detect from file extensions

```bash
if [ -z "$DETECTED_LANGUAGE" ]; then
    echo "   No config files found - detecting from file extensions..."
    
    # Count files by extension
    TS_COUNT=$(find . -name "*.ts" -o -name "*.tsx" 2>/dev/null | wc -l)
    JS_COUNT=$(find . -name "*.js" -o -name "*.jsx" 2>/dev/null | wc -l)
    RS_COUNT=$(find . -name "*.rs" 2>/dev/null | wc -l)
    GO_COUNT=$(find . -name "*.go" 2>/dev/null | wc -l)
    PY_COUNT=$(find . -name "*.py" 2>/dev/null | wc -l)
    
    # Pick the most common
    MAX_COUNT=0
    if [ "$TS_COUNT" -gt "$MAX_COUNT" ] 2>/dev/null; then
        MAX_COUNT=$TS_COUNT
        DETECTED_LANGUAGE="typescript"
        DETECTED_BACKEND="nodejs"
    fi
    if [ "$JS_COUNT" -gt "$MAX_COUNT" ] 2>/dev/null; then
        MAX_COUNT=$JS_COUNT
        DETECTED_LANGUAGE="javascript"
        DETECTED_BACKEND="nodejs"
    fi
    if [ "$RS_COUNT" -gt "$MAX_COUNT" ] 2>/dev/null; then
        MAX_COUNT=$RS_COUNT
        DETECTED_LANGUAGE="rust"
        DETECTED_BACKEND="rust"
    fi
    if [ "$GO_COUNT" -gt "$MAX_COUNT" ] 2>/dev/null; then
        MAX_COUNT=$GO_COUNT
        DETECTED_LANGUAGE="go"
        DETECTED_BACKEND="go"
    fi
    if [ "$PY_COUNT" -gt "$MAX_COUNT" ] 2>/dev/null; then
        MAX_COUNT=$PY_COUNT
        DETECTED_LANGUAGE="python"
        DETECTED_BACKEND="python"
    fi
    
    TECH_STACK_CONFIDENCE="0.60"
fi
```

---

## Important Constraints

- Must not fail if config files are missing
- Must handle malformed JSON/TOML gracefully
- Should detect the MOST SPECIFIC framework possible
- Confidence score reflects detection certainty

DETECTIONS_RUN=$((DETECTIONS_RUN + 1))
if [ -n "$DETECTED_LANGUAGE" ]; then
    DETECTIONS_SUCCESS=$((DETECTIONS_SUCCESS + 1))
    echo "   âœ“ Language: $DETECTED_LANGUAGE"
    [ -n "$DETECTED_FRAMEWORK" ] && echo "   âœ“ Framework: $DETECTED_FRAMEWORK"
    [ -n "$DETECTED_DATABASE" ] && echo "   âœ“ Database: $DETECTED_DATABASE"
fi

# 2. Detect Commands
echo "âš™ï¸  Detecting build/test/lint commands..."
# Workflow: Detect Commands

## Purpose

Extract build, test, and lint commands from project configuration files (package.json scripts, Makefile targets, CI configs). Falls back to language-specific defaults when not found.

## Outputs

Sets the following variables:
- `DETECTED_BUILD_CMD` - Build/compile command
- `DETECTED_TEST_CMD` - Test command
- `DETECTED_LINT_CMD` - Lint/static analysis command
- `DETECTED_TYPECHECK_CMD` - Type checking command (if applicable)
- `COMMANDS_CONFIDENCE` - Confidence score (0.0 - 1.0)

---

## Detection Logic

### Step 1: Detect from package.json scripts

```bash
if [ -f "package.json" ]; then
    echo "   Checking package.json scripts..."
    
    # Extract scripts section
    SCRIPTS=$(cat package.json | grep -A 100 '"scripts"' | grep -B 100 -m 1 '^  }' 2>/dev/null || cat package.json)
    
    # Build command
    if echo "$SCRIPTS" | grep -q '"build"'; then
        DETECTED_BUILD_CMD="npm run build"
    elif echo "$SCRIPTS" | grep -q '"compile"'; then
        DETECTED_BUILD_CMD="npm run compile"
    fi
    
    # Test command
    if echo "$SCRIPTS" | grep -q '"test"'; then
        DETECTED_TEST_CMD="npm test"
    elif echo "$SCRIPTS" | grep -q '"test:unit"'; then
        DETECTED_TEST_CMD="npm run test:unit"
    fi
    
    # Lint command
    if echo "$SCRIPTS" | grep -q '"lint"'; then
        DETECTED_LINT_CMD="npm run lint"
    elif echo "$SCRIPTS" | grep -q '"eslint"'; then
        DETECTED_LINT_CMD="npm run eslint"
    fi
    
    # Type check command
    if echo "$SCRIPTS" | grep -q '"typecheck"'; then
        DETECTED_TYPECHECK_CMD="npm run typecheck"
    elif echo "$SCRIPTS" | grep -q '"type-check"'; then
        DETECTED_TYPECHECK_CMD="npm run type-check"
    elif [ -f "tsconfig.json" ]; then
        DETECTED_TYPECHECK_CMD="tsc --noEmit"
    fi
    
    COMMANDS_CONFIDENCE="0.95"
fi
```

### Step 2: Detect from Makefile

```bash
if [ -f "Makefile" ]; then
    echo "   Checking Makefile targets..."
    
    # Build target
    if grep -q "^build:" Makefile; then
        [ -z "$DETECTED_BUILD_CMD" ] && DETECTED_BUILD_CMD="make build"
    fi
    
    # Test target
    if grep -q "^test:" Makefile; then
        [ -z "$DETECTED_TEST_CMD" ] && DETECTED_TEST_CMD="make test"
    fi
    
    # Lint target
    if grep -q "^lint:" Makefile; then
        [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="make lint"
    fi
    
    # Check target (often type-check or full validation)
    if grep -q "^check:" Makefile; then
        [ -z "$DETECTED_TYPECHECK_CMD" ] && DETECTED_TYPECHECK_CMD="make check"
    fi
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.90"
fi
```

### Step 3: Detect from CI config files

```bash
# GitHub Actions
if [ -d ".github/workflows" ]; then
    echo "   Checking GitHub Actions workflows..."
    
    for workflow in .github/workflows/*.yml .github/workflows/*.yaml; do
        [ -f "$workflow" ] || continue
        
        WORKFLOW_CONTENT=$(cat "$workflow" 2>/dev/null)
        
        # Look for common CI steps
        if echo "$WORKFLOW_CONTENT" | grep -q "npm run build" && [ -z "$DETECTED_BUILD_CMD" ]; then
            DETECTED_BUILD_CMD="npm run build"
        fi
        if echo "$WORKFLOW_CONTENT" | grep -q "npm test" && [ -z "$DETECTED_TEST_CMD" ]; then
            DETECTED_TEST_CMD="npm test"
        fi
        if echo "$WORKFLOW_CONTENT" | grep -q "cargo test" && [ -z "$DETECTED_TEST_CMD" ]; then
            DETECTED_TEST_CMD="cargo test"
        fi
        if echo "$WORKFLOW_CONTENT" | grep -q "pytest" && [ -z "$DETECTED_TEST_CMD" ]; then
            DETECTED_TEST_CMD="pytest"
        fi
        if echo "$WORKFLOW_CONTENT" | grep -q "go test" && [ -z "$DETECTED_TEST_CMD" ]; then
            DETECTED_TEST_CMD="go test ./..."
        fi
    done
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.80"
fi

# GitLab CI
if [ -f ".gitlab-ci.yml" ]; then
    echo "   Checking GitLab CI config..."
    
    GITLAB_CI=$(cat .gitlab-ci.yml 2>/dev/null)
    
    if echo "$GITLAB_CI" | grep -q "npm run build" && [ -z "$DETECTED_BUILD_CMD" ]; then
        DETECTED_BUILD_CMD="npm run build"
    fi
    if echo "$GITLAB_CI" | grep -q "npm test" && [ -z "$DETECTED_TEST_CMD" ]; then
        DETECTED_TEST_CMD="npm test"
    fi
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.80"
fi
```

### Step 4: Detect from Cargo.toml (Rust)

```bash
if [ -f "Cargo.toml" ]; then
    echo "   Detecting Rust commands..."
    
    # Rust has standard commands
    [ -z "$DETECTED_BUILD_CMD" ] && DETECTED_BUILD_CMD="cargo build"
    [ -z "$DETECTED_TEST_CMD" ] && DETECTED_TEST_CMD="cargo test"
    [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="cargo clippy"
    [ -z "$DETECTED_TYPECHECK_CMD" ] && DETECTED_TYPECHECK_CMD="cargo check"
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.95"
fi
```

### Step 5: Detect from go.mod (Go)

```bash
if [ -f "go.mod" ]; then
    echo "   Detecting Go commands..."
    
    [ -z "$DETECTED_BUILD_CMD" ] && DETECTED_BUILD_CMD="go build ./..."
    [ -z "$DETECTED_TEST_CMD" ] && DETECTED_TEST_CMD="go test ./..."
    [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="golangci-lint run ./..."
    [ -z "$DETECTED_TYPECHECK_CMD" ] && DETECTED_TYPECHECK_CMD="go vet ./..."
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.95"
fi
```

### Step 6: Detect from Python files

```bash
if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ]; then
    echo "   Detecting Python commands..."
    
    # Python typically doesn't have a build step
    
    # Test framework detection
    if [ -f "pytest.ini" ] || [ -f "pyproject.toml" ] && grep -q "pytest" pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_TEST_CMD" ] && DETECTED_TEST_CMD="pytest"
    elif [ -f "setup.py" ]; then
        [ -z "$DETECTED_TEST_CMD" ] && DETECTED_TEST_CMD="python -m pytest"
    fi
    
    # Lint detection
    if grep -q "flake8" requirements.txt pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="flake8"
    elif grep -q "ruff" requirements.txt pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="ruff check"
    elif grep -q "pylint" requirements.txt pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_LINT_CMD" ] && DETECTED_LINT_CMD="pylint"
    fi
    
    # Type checking
    if grep -q "mypy" requirements.txt pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_TYPECHECK_CMD" ] && DETECTED_TYPECHECK_CMD="mypy ."
    elif grep -q "pyright" requirements.txt pyproject.toml 2>/dev/null; then
        [ -z "$DETECTED_TYPECHECK_CMD" ] && DETECTED_TYPECHECK_CMD="pyright"
    fi
    
    [ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.85"
fi
```

### Step 7: Output Detection Summary

```bash
# Set default confidence if nothing was detected
[ -z "$COMMANDS_CONFIDENCE" ] && COMMANDS_CONFIDENCE="0.50"

# Log what was found
if [ -n "$DETECTED_BUILD_CMD" ] || [ -n "$DETECTED_TEST_CMD" ] || [ -n "$DETECTED_LINT_CMD" ]; then
    echo "   Commands detected with confidence: $COMMANDS_CONFIDENCE"
else
    echo "   âš ï¸ No commands detected - will use language defaults"
fi
```

---

## Important Constraints

- Must not fail if config files are missing
- Should prefer explicit configs (package.json scripts) over inferred
- CI configs are lower confidence than direct project configs
- Must handle both YAML and JSON gracefully

DETECTIONS_RUN=$((DETECTIONS_RUN + 1))
if [ -n "$DETECTED_BUILD_CMD" ] || [ -n "$DETECTED_TEST_CMD" ]; then
    DETECTIONS_SUCCESS=$((DETECTIONS_SUCCESS + 1))
    [ -n "$DETECTED_BUILD_CMD" ] && echo "   âœ“ Build: $DETECTED_BUILD_CMD"
    [ -n "$DETECTED_TEST_CMD" ] && echo "   âœ“ Test: $DETECTED_TEST_CMD"
    [ -n "$DETECTED_LINT_CMD" ] && echo "   âœ“ Lint: $DETECTED_LINT_CMD"
fi

# 3. Detect Architecture
echo "ðŸ—ï¸  Analyzing project architecture..."
# Workflow: Detect Architecture

## Purpose

Analyze directory structure to detect project type, architecture patterns (monolith vs microservices), module boundaries, and project size metrics.

## Outputs

Sets the following variables:
- `DETECTED_PROJECT_TYPE` - web_app, cli, api, library, monorepo
- `DETECTED_ARCHITECTURE` - monolith, microservices, modular
- `DETECTED_FILE_COUNT` - Total number of source files
- `DETECTED_LINE_COUNT` - Approximate total lines of code
- `DETECTED_MODULE_COUNT` - Number of detected modules
- `DETECTED_MODULES` - Comma-separated list of module names
- `ARCHITECTURE_CONFIDENCE` - Confidence score (0.0 - 1.0)

---

## Detection Logic

### Step 1: Count Files and Lines

```bash
echo "   Counting files and lines..."

# Count source files (excluding node_modules, vendor, etc.)
DETECTED_FILE_COUNT=$(find . \
    -type f \
    \( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" \
       -o -name "*.rs" -o -name "*.go" -o -name "*.py" \
       -o -name "*.java" -o -name "*.kt" -o -name "*.swift" \
       -o -name "*.c" -o -name "*.cpp" -o -name "*.h" \) \
    ! -path "*/node_modules/*" \
    ! -path "*/vendor/*" \
    ! -path "*/.git/*" \
    ! -path "*/dist/*" \
    ! -path "*/build/*" \
    ! -path "*/target/*" \
    ! -path "*/__pycache__/*" \
    2>/dev/null | wc -l | tr -d ' ')

# Estimate line count (faster than wc -l on all files)
if [ "$DETECTED_FILE_COUNT" -gt 0 ] 2>/dev/null; then
    # Sample up to 50 files and extrapolate
    SAMPLE_LINES=$(find . \
        -type f \
        \( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" \
           -o -name "*.rs" -o -name "*.go" -o -name "*.py" \) \
        ! -path "*/node_modules/*" \
        ! -path "*/vendor/*" \
        ! -path "*/.git/*" \
        2>/dev/null | head -50 | xargs wc -l 2>/dev/null | tail -1 | awk '{print $1}')
    
    SAMPLE_COUNT=$(find . \
        -type f \
        \( -name "*.ts" -o -name "*.tsx" -o -name "*.js" -o -name "*.jsx" \
           -o -name "*.rs" -o -name "*.go" -o -name "*.py" \) \
        ! -path "*/node_modules/*" \
        ! -path "*/vendor/*" \
        ! -path "*/.git/*" \
        2>/dev/null | head -50 | wc -l | tr -d ' ')
    
    if [ "$SAMPLE_COUNT" -gt 0 ] 2>/dev/null; then
        AVG_LINES_PER_FILE=$((SAMPLE_LINES / SAMPLE_COUNT))
        DETECTED_LINE_COUNT=$((DETECTED_FILE_COUNT * AVG_LINES_PER_FILE))
    else
        DETECTED_LINE_COUNT=0
    fi
else
    DETECTED_LINE_COUNT=0
fi

echo "   Found $DETECTED_FILE_COUNT files, ~$DETECTED_LINE_COUNT lines"
```

### Step 2: Detect Project Type

```bash
echo "   Detecting project type..."

DETECTED_PROJECT_TYPE="unknown"
ARCHITECTURE_CONFIDENCE="0.50"

# Web Application indicators
if [ -d "src/components" ] || [ -d "app/components" ] || [ -d "components" ]; then
    DETECTED_PROJECT_TYPE="web_app"
    ARCHITECTURE_CONFIDENCE="0.90"
elif [ -d "pages" ] || [ -d "src/pages" ] || [ -d "app" ]; then
    # Could be Next.js, Nuxt, etc.
    if [ -f "next.config.js" ] || [ -f "next.config.mjs" ] || [ -f "nuxt.config.ts" ]; then
        DETECTED_PROJECT_TYPE="web_app"
        ARCHITECTURE_CONFIDENCE="0.95"
    fi
fi

# CLI Tool indicators
if [ -d "bin" ] || [ -f "cli.js" ] || [ -f "cli.ts" ]; then
    DETECTED_PROJECT_TYPE="cli"
    ARCHITECTURE_CONFIDENCE="0.85"
elif grep -q '"bin":' package.json 2>/dev/null; then
    DETECTED_PROJECT_TYPE="cli"
    ARCHITECTURE_CONFIDENCE="0.90"
fi

# API/Backend Service indicators
if [ -d "src/routes" ] || [ -d "src/api" ] || [ -d "routes" ] || [ -d "api" ]; then
    DETECTED_PROJECT_TYPE="api"
    ARCHITECTURE_CONFIDENCE="0.85"
elif [ -d "src/controllers" ] || [ -d "controllers" ]; then
    DETECTED_PROJECT_TYPE="api"
    ARCHITECTURE_CONFIDENCE="0.85"
fi

# Library indicators
if [ -f "src/index.ts" ] || [ -f "src/index.js" ] || [ -f "src/lib.rs" ]; then
    if [ ! -d "src/components" ] && [ ! -d "src/pages" ] && [ ! -d "src/routes" ]; then
        DETECTED_PROJECT_TYPE="library"
        ARCHITECTURE_CONFIDENCE="0.80"
    fi
fi

# Monorepo indicators
if [ -d "packages" ] || [ -f "lerna.json" ] || [ -f "pnpm-workspace.yaml" ]; then
    DETECTED_PROJECT_TYPE="monorepo"
    ARCHITECTURE_CONFIDENCE="0.95"
elif [ -f "turbo.json" ] || [ -f "nx.json" ]; then
    DETECTED_PROJECT_TYPE="monorepo"
    ARCHITECTURE_CONFIDENCE="0.95"
fi

echo "   Project type: $DETECTED_PROJECT_TYPE (confidence: $ARCHITECTURE_CONFIDENCE)"
```

### Step 3: Detect Architecture Pattern

```bash
echo "   Detecting architecture pattern..."

DETECTED_ARCHITECTURE="monolith"

# Check for microservices indicators
if [ -d "services" ] && [ $(ls -d services/*/ 2>/dev/null | wc -l) -gt 1 ]; then
    DETECTED_ARCHITECTURE="microservices"
elif [ -f "docker-compose.yml" ] || [ -f "docker-compose.yaml" ]; then
    # Check if docker-compose defines multiple services with different build contexts
    SERVICE_COUNT=$(grep -c "build:" docker-compose.yml docker-compose.yaml 2>/dev/null || echo 0)
    if [ "$SERVICE_COUNT" -gt 2 ]; then
        DETECTED_ARCHITECTURE="microservices"
    fi
fi

# Check for modular architecture
if [ -d "src/modules" ] || [ -d "modules" ] || [ -d "src/features" ]; then
    DETECTED_ARCHITECTURE="modular"
fi

# Monorepo is a form of modular
if [ "$DETECTED_PROJECT_TYPE" = "monorepo" ]; then
    DETECTED_ARCHITECTURE="modular"
fi

echo "   Architecture: $DETECTED_ARCHITECTURE"
```

### Step 4: Detect Module Boundaries

```bash
echo "   Detecting module boundaries..."

DETECTED_MODULE_COUNT=0
DETECTED_MODULES=""

# For monorepos, packages are modules
if [ -d "packages" ]; then
    DETECTED_MODULES=$(ls -d packages/*/ 2>/dev/null | xargs -n1 basename | tr '\n' ',' | sed 's/,$//')
    DETECTED_MODULE_COUNT=$(echo "$DETECTED_MODULES" | tr ',' '\n' | wc -l | tr -d ' ')
fi

# For services-based
if [ -d "services" ]; then
    SERVICES=$(ls -d services/*/ 2>/dev/null | xargs -n1 basename | tr '\n' ',' | sed 's/,$//')
    if [ -n "$SERVICES" ]; then
        DETECTED_MODULES="$SERVICES"
        DETECTED_MODULE_COUNT=$(echo "$DETECTED_MODULES" | tr ',' '\n' | wc -l | tr -d ' ')
    fi
fi

# For src/modules or src/features structure
if [ -d "src/modules" ]; then
    MODULES=$(ls -d src/modules/*/ 2>/dev/null | xargs -n1 basename | tr '\n' ',' | sed 's/,$//')
    if [ -n "$MODULES" ]; then
        DETECTED_MODULES="$MODULES"
        DETECTED_MODULE_COUNT=$(echo "$DETECTED_MODULES" | tr ',' '\n' | wc -l | tr -d ' ')
    fi
elif [ -d "src/features" ]; then
    FEATURES=$(ls -d src/features/*/ 2>/dev/null | xargs -n1 basename | tr '\n' ',' | sed 's/,$//')
    if [ -n "$FEATURES" ]; then
        DETECTED_MODULES="$FEATURES"
        DETECTED_MODULE_COUNT=$(echo "$DETECTED_MODULES" | tr ',' '\n' | wc -l | tr -d ' ')
    fi
fi

# Fallback: count top-level src directories
if [ "$DETECTED_MODULE_COUNT" -eq 0 ] && [ -d "src" ]; then
    SRC_DIRS=$(ls -d src/*/ 2>/dev/null | xargs -n1 basename | grep -v "test" | grep -v "__" | tr '\n' ',' | sed 's/,$//')
    if [ -n "$SRC_DIRS" ]; then
        DETECTED_MODULES="$SRC_DIRS"
        DETECTED_MODULE_COUNT=$(echo "$DETECTED_MODULES" | tr ',' '\n' | wc -l | tr -d ' ')
    fi
fi

echo "   Modules: $DETECTED_MODULE_COUNT ($DETECTED_MODULES)"
```

### Step 5: Output Summary

```bash
echo ""
echo "   Architecture Detection Summary:"
echo "   - Type: $DETECTED_PROJECT_TYPE"
echo "   - Pattern: $DETECTED_ARCHITECTURE"
echo "   - Files: $DETECTED_FILE_COUNT"
echo "   - Lines: ~$DETECTED_LINE_COUNT"
echo "   - Modules: $DETECTED_MODULE_COUNT"
echo "   - Confidence: $ARCHITECTURE_CONFIDENCE"
```

---

## Important Constraints

- Must handle empty directories gracefully
- Should not count files in ignored directories (node_modules, vendor, etc.)
- Line count is an estimate, not exact
- Module detection is heuristic-based

DETECTIONS_RUN=$((DETECTIONS_RUN + 1))
if [ -n "$DETECTED_PROJECT_TYPE" ]; then
    DETECTIONS_SUCCESS=$((DETECTIONS_SUCCESS + 1))
    echo "   âœ“ Type: $DETECTED_PROJECT_TYPE"
    echo "   âœ“ Size: $DETECTED_FILE_COUNT files, ~$DETECTED_LINE_COUNT lines"
    [ -n "$DETECTED_MODULES" ] && echo "   âœ“ Modules: $DETECTED_MODULES"
fi

# 4. Detect Security Level
echo "ðŸ”’ Checking security indicators..."
# Workflow: Detect Security Level

## Purpose

Check for security-related dependencies, secrets management patterns, and authentication configurations to determine the project's security level (low, moderate, high).

## Outputs

Sets the following variables:
- `DETECTED_SECURITY_LEVEL` - low, moderate, or high
- `SECURITY_INDICATORS` - Comma-separated list of detected security features
- `HAS_AUTH` - true/false
- `HAS_SECRETS_MANAGEMENT` - true/false
- `SECURITY_CONFIDENCE` - Confidence score (0.0 - 1.0)

---

## Detection Logic

### Step 1: Initialize

```bash
SECURITY_SCORE=0
SECURITY_INDICATORS=""
HAS_AUTH="false"
HAS_SECRETS_MANAGEMENT="false"
SECURITY_CONFIDENCE="0.70"
```

### Step 2: Check for Authentication Dependencies

```bash
echo "   Checking for authentication..."

# Node.js auth libraries
if [ -f "package.json" ]; then
    DEPS=$(cat package.json 2>/dev/null)
    
    # Auth libraries
    if echo "$DEPS" | grep -qE '"(passport|@auth0|firebase-admin|next-auth|lucia|clerk)"'; then
        HAS_AUTH="true"
        SECURITY_SCORE=$((SECURITY_SCORE + 3))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}auth-library,"
        echo "   âœ“ Authentication library detected"
    fi
    
    # Password hashing
    if echo "$DEPS" | grep -qE '"(bcrypt|argon2|scrypt)"'; then
        SECURITY_SCORE=$((SECURITY_SCORE + 2))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}password-hashing,"
        echo "   âœ“ Password hashing library detected"
    fi
    
    # JWT/Session
    if echo "$DEPS" | grep -qE '"(jsonwebtoken|jose|iron-session)"'; then
        SECURITY_SCORE=$((SECURITY_SCORE + 1))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}jwt-sessions,"
    fi
    
    # OAuth
    if echo "$DEPS" | grep -qE '"(oauth|passport-oauth|passport-google|passport-github)"'; then
        HAS_AUTH="true"
        SECURITY_SCORE=$((SECURITY_SCORE + 2))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}oauth,"
        echo "   âœ“ OAuth integration detected"
    fi
fi

# Rust auth
if [ -f "Cargo.toml" ]; then
    CARGO=$(cat Cargo.toml 2>/dev/null)
    
    if echo "$CARGO" | grep -qE '(argon2|bcrypt|jsonwebtoken|oauth2)'; then
        HAS_AUTH="true"
        SECURITY_SCORE=$((SECURITY_SCORE + 3))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}rust-auth,"
    fi
fi

# Python auth
if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ]; then
    PYDEPS=""
    [ -f "requirements.txt" ] && PYDEPS="$PYDEPS $(cat requirements.txt)"
    [ -f "pyproject.toml" ] && PYDEPS="$PYDEPS $(cat pyproject.toml)"
    
    if echo "$PYDEPS" | grep -qiE '(passlib|bcrypt|python-jose|authlib|django-allauth)'; then
        HAS_AUTH="true"
        SECURITY_SCORE=$((SECURITY_SCORE + 3))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}python-auth,"
    fi
fi
```

### Step 3: Check for Secrets Management

```bash
echo "   Checking for secrets management..."

# Environment files
if [ -f ".env.example" ] || [ -f ".env.sample" ] || [ -f ".env.template" ]; then
    HAS_SECRETS_MANAGEMENT="true"
    SECURITY_SCORE=$((SECURITY_SCORE + 1))
    SECURITY_INDICATORS="${SECURITY_INDICATORS}env-files,"
    echo "   âœ“ Environment file template detected"
fi

# Secrets in docker-compose
if [ -f "docker-compose.yml" ] || [ -f "docker-compose.yaml" ]; then
    DC_CONTENT=$(cat docker-compose.yml docker-compose.yaml 2>/dev/null)
    
    if echo "$DC_CONTENT" | grep -qE '(secrets:|vault)'; then
        HAS_SECRETS_MANAGEMENT="true"
        SECURITY_SCORE=$((SECURITY_SCORE + 2))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}docker-secrets,"
        echo "   âœ“ Docker secrets detected"
    fi
fi

# Vault integration
if grep -rq "vault" . --include="*.yml" --include="*.yaml" --include="*.json" 2>/dev/null | head -1 | grep -q vault; then
    HAS_SECRETS_MANAGEMENT="true"
    SECURITY_SCORE=$((SECURITY_SCORE + 3))
    SECURITY_INDICATORS="${SECURITY_INDICATORS}vault,"
    echo "   âœ“ HashiCorp Vault detected"
fi

# AWS Secrets Manager
if grep -rq "secretsmanager\|SecretsManager" . --include="*.ts" --include="*.js" --include="*.py" 2>/dev/null | head -1 | grep -q secret; then
    HAS_SECRETS_MANAGEMENT="true"
    SECURITY_SCORE=$((SECURITY_SCORE + 3))
    SECURITY_INDICATORS="${SECURITY_INDICATORS}aws-secrets,"
    echo "   âœ“ AWS Secrets Manager detected"
fi
```

### Step 4: Check for Security Headers/Middleware

```bash
echo "   Checking for security middleware..."

# Helmet (Node.js security headers)
if [ -f "package.json" ]; then
    if grep -q '"helmet"' package.json 2>/dev/null; then
        SECURITY_SCORE=$((SECURITY_SCORE + 1))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}helmet,"
        echo "   âœ“ Helmet security headers detected"
    fi
    
    # Rate limiting
    if grep -qE '"(express-rate-limit|rate-limiter-flexible)"' package.json 2>/dev/null; then
        SECURITY_SCORE=$((SECURITY_SCORE + 1))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}rate-limiting,"
        echo "   âœ“ Rate limiting detected"
    fi
    
    # CORS
    if grep -q '"cors"' package.json 2>/dev/null; then
        SECURITY_INDICATORS="${SECURITY_INDICATORS}cors-config,"
    fi
fi
```

### Step 5: Check for Encryption

```bash
echo "   Checking for encryption..."

# Crypto libraries
if [ -f "package.json" ]; then
    if grep -qE '"(crypto-js|node-forge|tweetnacl)"' package.json 2>/dev/null; then
        SECURITY_SCORE=$((SECURITY_SCORE + 2))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}encryption,"
        echo "   âœ“ Encryption library detected"
    fi
fi

# TLS/SSL configs
if [ -f "nginx.conf" ] || [ -d "ssl" ] || [ -d "certs" ]; then
    SECURITY_SCORE=$((SECURITY_SCORE + 1))
    SECURITY_INDICATORS="${SECURITY_INDICATORS}ssl-config,"
    echo "   âœ“ SSL/TLS configuration detected"
fi
```

### Step 6: Check for Open Source Indicators (Lower Security Needs)

```bash
# Open source projects may have lower security needs
if [ -f "LICENSE" ]; then
    LICENSE_TYPE=$(head -5 LICENSE 2>/dev/null)
    
    if echo "$LICENSE_TYPE" | grep -qiE '(MIT|Apache|GPL|BSD|ISC)'; then
        # Open source - reduce security expectation slightly
        SECURITY_SCORE=$((SECURITY_SCORE - 1))
        SECURITY_INDICATORS="${SECURITY_INDICATORS}open-source,"
    fi
fi
```

### Step 7: Calculate Security Level

```bash
# Normalize score
[ $SECURITY_SCORE -lt 0 ] && SECURITY_SCORE=0

# Determine level
if [ $SECURITY_SCORE -ge 5 ]; then
    DETECTED_SECURITY_LEVEL="high"
    SECURITY_CONFIDENCE="0.90"
elif [ $SECURITY_SCORE -ge 2 ]; then
    DETECTED_SECURITY_LEVEL="moderate"
    SECURITY_CONFIDENCE="0.80"
else
    DETECTED_SECURITY_LEVEL="low"
    SECURITY_CONFIDENCE="0.70"
fi

# Clean up indicators string
SECURITY_INDICATORS=$(echo "$SECURITY_INDICATORS" | sed 's/,$//')

echo ""
echo "   Security Detection Summary:"
echo "   - Level: $DETECTED_SECURITY_LEVEL"
echo "   - Score: $SECURITY_SCORE"
echo "   - Has Auth: $HAS_AUTH"
echo "   - Has Secrets Management: $HAS_SECRETS_MANAGEMENT"
echo "   - Indicators: $SECURITY_INDICATORS"
```

---

## Security Level Guidelines

| Level | Score | Indicators |
|-------|-------|------------|
| **High** | 5+ | Auth + secrets management + encryption |
| **Moderate** | 2-4 | Some auth or env file management |
| **Low** | 0-1 | No security indicators, likely open source |

---

## Important Constraints

- Must not fail if files are missing
- Should not scan inside node_modules, vendor, etc.
- Open source projects default to lower security expectations
- Confidence increases with more indicators found

DETECTIONS_RUN=$((DETECTIONS_RUN + 1))
if [ -n "$DETECTED_SECURITY_LEVEL" ]; then
    DETECTIONS_SUCCESS=$((DETECTIONS_SUCCESS + 1))
    echo "   âœ“ Security Level: $DETECTED_SECURITY_LEVEL"
fi
```

### Step 3: Calculate Overall Confidence

```bash
# Calculate confidence score (0.0 - 1.0)
if [ $DETECTIONS_RUN -gt 0 ]; then
    DETECTION_CONFIDENCE=$(echo "scale=2; $DETECTIONS_SUCCESS / $DETECTIONS_RUN" | bc)
else
    DETECTION_CONFIDENCE="0.00"
fi

echo ""
echo "ðŸ“Š Detection confidence: ${DETECTION_CONFIDENCE} ($DETECTIONS_SUCCESS/$DETECTIONS_RUN successful)"
```

### Step 4: Apply Fallbacks for Failed Detections

```bash
# Fallback: Unknown language
if [ -z "$DETECTED_LANGUAGE" ]; then
    echo "âš ï¸  Could not detect language - will ask user"
    DETECTED_LANGUAGE="unknown"
    NEEDS_USER_INPUT_LANGUAGE=true
fi

# Fallback: Unknown project type
if [ -z "$DETECTED_PROJECT_TYPE" ]; then
    echo "âš ï¸  Could not detect project type - will ask user"
    DETECTED_PROJECT_TYPE="unknown"
    NEEDS_USER_INPUT_TYPE=true
fi

# Fallback: Commands not found
if [ -z "$DETECTED_BUILD_CMD" ] && [ -z "$DETECTED_TEST_CMD" ]; then
    echo "âš ï¸  Could not detect build/test commands - using language defaults"
    case "$DETECTED_LANGUAGE" in
        "javascript"|"typescript") 
            DETECTED_BUILD_CMD="npm run build"
            DETECTED_TEST_CMD="npm test"
            DETECTED_LINT_CMD="npm run lint"
            ;;
        "rust")
            DETECTED_BUILD_CMD="cargo build"
            DETECTED_TEST_CMD="cargo test"
            DETECTED_LINT_CMD="cargo clippy"
            ;;
        "python")
            DETECTED_TEST_CMD="pytest"
            DETECTED_LINT_CMD="flake8"
            ;;
        "go")
            DETECTED_BUILD_CMD="go build ./..."
            DETECTED_TEST_CMD="go test ./..."
            DETECTED_LINT_CMD="golangci-lint run"
            ;;
    esac
fi

# Fallback: Security level
if [ -z "$DETECTED_SECURITY_LEVEL" ]; then
    DETECTED_SECURITY_LEVEL="moderate"
fi
```

### Step 5: Generate Unified Profile

```bash
# Infer complexity from size
INFERRED_COMPLEXITY="simple"
if [ "$DETECTED_FILE_COUNT" -gt 500 ] 2>/dev/null; then
    INFERRED_COMPLEXITY="complex"
elif [ "$DETECTED_FILE_COUNT" -gt 100 ] 2>/dev/null; then
    INFERRED_COMPLEXITY="moderate"
fi

# Generate the profile YAML
cat > agent-os/config/project-profile.yml << PROFILE_EOF
# Project Profile
# Auto-generated by detect-project-profile workflow
# Review and modify as needed

gathered:
  project_type: ${DETECTED_PROJECT_TYPE:-unknown}
  tech_stack:
    language: ${DETECTED_LANGUAGE:-unknown}
    framework: ${DETECTED_FRAMEWORK:-}
    backend: ${DETECTED_BACKEND:-}
    database: ${DETECTED_DATABASE:-}
  size:
    lines: ${DETECTED_LINE_COUNT:-0}
    files: ${DETECTED_FILE_COUNT:-0}
    modules: ${DETECTED_MODULE_COUNT:-0}
  commands:
    build: "${DETECTED_BUILD_CMD:-}"
    test: "${DETECTED_TEST_CMD:-}"
    lint: "${DETECTED_LINT_CMD:-}"

inferred:
  security_level: ${DETECTED_SECURITY_LEVEL:-moderate}
  complexity: ${INFERRED_COMPLEXITY}

user_specified:
  compliance: []
  human_review_level: moderate

_meta:
  detected_at: $(date -Iseconds)
  detection_confidence: ${DETECTION_CONFIDENCE}
  questions_asked: 0
  questions_auto_answered: $DETECTIONS_SUCCESS
  needs_user_input:
    language: ${NEEDS_USER_INPUT_LANGUAGE:-false}
    project_type: ${NEEDS_USER_INPUT_TYPE:-false}

PROFILE_EOF

echo ""
echo "âœ… Project profile saved to: agent-os/config/project-profile.yml"
```

### Step 6: Output Detection Summary

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  DETECTION COMPLETE"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Detected Configuration:"
echo ""
echo "  Project Type:    ${DETECTED_PROJECT_TYPE:-unknown}"
echo "  Language:        ${DETECTED_LANGUAGE:-unknown}"
echo "  Framework:       ${DETECTED_FRAMEWORK:-(none detected)}"
echo "  Database:        ${DETECTED_DATABASE:-(none detected)}"
echo "  Size:            ${DETECTED_FILE_COUNT:-?} files, ~${DETECTED_LINE_COUNT:-?} lines"
echo "  Security Level:  ${DETECTED_SECURITY_LEVEL:-moderate}"
echo "  Complexity:      ${INFERRED_COMPLEXITY}"
echo ""
echo "  Build Command:   ${DETECTED_BUILD_CMD:-(not set)}"
echo "  Test Command:    ${DETECTED_TEST_CMD:-(not set)}"
echo "  Lint Command:    ${DETECTED_LINT_CMD:-(not set)}"
echo ""
echo "  Confidence:      ${DETECTION_CONFIDENCE}"
echo ""
```

---

## Integration

This workflow is called by:
- `adapt-to-product/1-setup-and-information-gathering.md`
- `create-basepoints/1-validate-prerequisites.md`
- `deploy-agents/1-validate-prerequisites.md`

After this workflow completes, call:
- `# Workflow: Present and Confirm

## Purpose

Format detected values for display, present a confirmation prompt to the user, handle user overrides, and output the final confirmed profile.

## Inputs

Expects these variables to be set (from prior detection workflows):
- `DETECTED_PROJECT_TYPE`
- `DETECTED_LANGUAGE`
- `DETECTED_FRAMEWORK`
- `DETECTED_DATABASE`
- `DETECTED_BUILD_CMD`
- `DETECTED_TEST_CMD`
- `DETECTED_LINT_CMD`
- `DETECTED_SECURITY_LEVEL`
- `DETECTED_FILE_COUNT`
- `DETECTED_LINE_COUNT`
- `DETECTION_CONFIDENCE`

## Outputs

- Updates `agent-os/config/project-profile.yml` with confirmed values
- Sets `USER_CONFIRMED=true` when user accepts

---

## Workflow

### Step 1: Display Detected Configuration

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "                      DETECTED PROJECT CONFIGURATION                         "
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Project Profile Section
echo "ðŸ“¦ PROJECT PROFILE"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Type:" "${DETECTED_PROJECT_TYPE:-unknown} âœ“"
printf "   %-20s %s\n" "Size:" "${DETECTED_FILE_COUNT:-?} files, ~${DETECTED_LINE_COUNT:-?} lines âœ“"
printf "   %-20s %s\n" "Maturity:" "$([ -d '.git' ] && echo 'Version controlled' || echo 'Unknown') âœ“"
echo ""

# Tech Stack Section
echo "ðŸ”§ TECH STACK"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Language:" "${DETECTED_LANGUAGE:-unknown} âœ“"
[ -n "$DETECTED_FRAMEWORK" ] && printf "   %-20s %s\n" "Framework:" "$DETECTED_FRAMEWORK âœ“"
[ -n "$DETECTED_BACKEND" ] && printf "   %-20s %s\n" "Backend:" "$DETECTED_BACKEND âœ“"
[ -n "$DETECTED_DATABASE" ] && printf "   %-20s %s\n" "Database:" "$DETECTED_DATABASE âœ“"
echo ""

# Commands Section
echo "âš™ï¸  DETECTED COMMANDS"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Build:" "${DETECTED_BUILD_CMD:-(not detected)} âœ“"
printf "   %-20s %s\n" "Test:" "${DETECTED_TEST_CMD:-(not detected)} âœ“"
printf "   %-20s %s\n" "Lint:" "${DETECTED_LINT_CMD:-(not detected)} âœ“"
echo ""

# Inferred Settings Section
echo "ðŸ”’ INFERRED SETTINGS"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Security Level:" "${DETECTED_SECURITY_LEVEL:-moderate} âœ“"
printf "   %-20s %s\n" "Complexity:" "${INFERRED_COMPLEXITY:-moderate} âœ“"
echo ""

# Confidence
echo "ðŸ“Š DETECTION CONFIDENCE: ${DETECTION_CONFIDENCE:-0.80}"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

### Step 2: Present Minimal Questions

Only ask questions for things that cannot be detected:

```bash
echo ""
echo "âš ï¸  QUESTIONS REQUIRING YOUR INPUT"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "These items cannot be determined automatically from your codebase:"
echo ""

# Question 1: Compliance Requirements
echo "1. What compliance requirements apply to this project?"
echo ""
echo "   [ ] None (default)"
echo "   [ ] SOC 2"
echo "   [ ] HIPAA"
echo "   [ ] GDPR"
echo "   [ ] PCI-DSS"
echo "   [ ] Other"
echo ""
echo "   Enter your choice (e.g., 'none', 'gdpr', 'soc2,hipaa'): "

# In non-interactive mode or if user presses Enter, use default
USER_COMPLIANCE="${USER_COMPLIANCE:-none}"

echo ""

# Question 2: Human Review Level
echo "2. How much human oversight do you want for AI-generated changes?"
echo ""
echo "   [ ] minimal  - Trust AI, review only critical changes"
echo "   [ ] moderate - Review architectural decisions (default)"
echo "   [ ] high     - Review all significant changes"
echo ""
echo "   Enter your choice (minimal/moderate/high): "

# Default to moderate
USER_HUMAN_REVIEW="${USER_HUMAN_REVIEW:-moderate}"

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

### Step 3: Confirmation Prompt

```bash
echo ""
echo "ðŸ“‹ CONFIRMATION"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "Press Enter to accept these detected values, or type a section name to modify:"
echo ""
echo "   â€¢ 'type'      - Change project type"
echo "   â€¢ 'language'  - Change language/framework"
echo "   â€¢ 'commands'  - Change build/test/lint commands"
echo "   â€¢ 'security'  - Change security level"
echo "   â€¢ 'all'       - Review all settings interactively"
echo ""
echo "Your choice (Enter to accept): "

# For non-interactive execution, assume acceptance
USER_CHOICE="${USER_CHOICE:-accept}"

if [ "$USER_CHOICE" = "" ] || [ "$USER_CHOICE" = "accept" ]; then
    echo ""
    echo "âœ… Configuration confirmed!"
    USER_CONFIRMED="true"
fi
```

### Step 4: Handle Overrides (if requested)

```bash
# Handle override requests
case "$USER_CHOICE" in
    "type")
        echo "Enter new project type (web_app, cli, api, library, monorepo):"
        read -r NEW_TYPE
        [ -n "$NEW_TYPE" ] && DETECTED_PROJECT_TYPE="$NEW_TYPE"
        ;;
    "language")
        echo "Enter primary language (typescript, javascript, rust, python, go):"
        read -r NEW_LANG
        [ -n "$NEW_LANG" ] && DETECTED_LANGUAGE="$NEW_LANG"
        echo "Enter framework (or leave empty):"
        read -r NEW_FRAMEWORK
        DETECTED_FRAMEWORK="$NEW_FRAMEWORK"
        ;;
    "commands")
        echo "Enter build command (or leave empty):"
        read -r NEW_BUILD
        DETECTED_BUILD_CMD="$NEW_BUILD"
        echo "Enter test command (or leave empty):"
        read -r NEW_TEST
        DETECTED_TEST_CMD="$NEW_TEST"
        echo "Enter lint command (or leave empty):"
        read -r NEW_LINT
        DETECTED_LINT_CMD="$NEW_LINT"
        ;;
    "security")
        echo "Enter security level (low, moderate, high):"
        read -r NEW_SECURITY
        [ -n "$NEW_SECURITY" ] && DETECTED_SECURITY_LEVEL="$NEW_SECURITY"
        ;;
esac
```

### Step 5: Update Profile with Confirmed Values

```bash
# Update the profile with user-specified values
cat > agent-os/config/project-profile.yml << CONFIRMED_EOF
# Project Profile
# Auto-detected and confirmed by user
# Generated: $(date -Iseconds)

gathered:
  project_type: ${DETECTED_PROJECT_TYPE:-unknown}
  tech_stack:
    language: ${DETECTED_LANGUAGE:-unknown}
    framework: ${DETECTED_FRAMEWORK:-}
    backend: ${DETECTED_BACKEND:-}
    database: ${DETECTED_DATABASE:-}
  size:
    lines: ${DETECTED_LINE_COUNT:-0}
    files: ${DETECTED_FILE_COUNT:-0}
    modules: ${DETECTED_MODULE_COUNT:-0}
  commands:
    build: "${DETECTED_BUILD_CMD:-}"
    test: "${DETECTED_TEST_CMD:-}"
    lint: "${DETECTED_LINT_CMD:-}"

inferred:
  security_level: ${DETECTED_SECURITY_LEVEL:-moderate}
  complexity: ${INFERRED_COMPLEXITY:-moderate}

user_specified:
  compliance:
$(echo "$USER_COMPLIANCE" | tr ',' '\n' | sed 's/^/    - /' | grep -v "^    - none$" || echo "    []")
  human_review_level: ${USER_HUMAN_REVIEW:-moderate}

_meta:
  detected_at: $(date -Iseconds)
  confirmed_at: $(date -Iseconds)
  detection_confidence: ${DETECTION_CONFIDENCE:-0.80}
  user_confirmed: ${USER_CONFIRMED:-true}
  questions_asked: 2
  questions_auto_answered: ${DETECTIONS_SUCCESS:-0}

CONFIRMED_EOF

echo ""
echo "âœ… Profile saved to: agent-os/config/project-profile.yml"
echo ""
```

---

## Output Summary

After confirmation, display:

```bash
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "                        CONFIGURATION COMPLETE                               "
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Your project profile has been saved and will be used for:"
echo ""
echo "  â€¢ Basepoint generation (create-basepoints)"
echo "  â€¢ Agent specialization (deploy-agents)"
echo "  â€¢ Validation command configuration"
echo "  â€¢ Workflow complexity selection"
echo ""
echo "You can modify the profile at any time by editing:"
echo "  agent-os/config/project-profile.yml"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

---

## Important Constraints

- Default to accepting detected values (user presses Enter)
- Maximum 2-3 questions that can't be auto-detected
- Provide sensible defaults for all questions
- Allow overrides but don't require them
- Save confirmed profile for use by subsequent commands
` - To get user confirmation
- `# Workflow: Research Orchestrator

## Purpose

Main orchestrator for web research. Loads detected tech stack, determines research depth, calls appropriate research workflows, and aggregates results into the enriched-knowledge directory.

## Inputs

- `agent-os/config/project-profile.yml` - Detected project profile
- `RESEARCH_DEPTH` - minimal, standard, or comprehensive (default: standard)

## Outputs

- `agent-os/config/enriched-knowledge/` directory with research results

---

## Workflow

### Step 1: Load Project Profile

```bash
echo "ðŸ”¬ Starting knowledge enrichment research..."
echo ""

# Create enriched-knowledge directory
mkdir -p agent-os/config/enriched-knowledge

# Load project profile
if [ -f "agent-os/config/project-profile.yml" ]; then
    echo "ðŸ“‚ Loading project profile..."
    
    # Extract key values (simplified parsing)
    DETECTED_LANGUAGE=$(grep "language:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    DETECTED_FRAMEWORK=$(grep "framework:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    DETECTED_DATABASE=$(grep "database:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    PROJECT_TYPE=$(grep "project_type:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    SECURITY_LEVEL=$(grep "security_level:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    
    echo "   Language: $DETECTED_LANGUAGE"
    echo "   Framework: ${DETECTED_FRAMEWORK:-(none)}"
    echo "   Database: ${DETECTED_DATABASE:-(none)}"
    echo "   Project Type: $PROJECT_TYPE"
else
    echo "âš ï¸ No project profile found. Run detection first."
    echo "   Using defaults..."
    DETECTED_LANGUAGE="unknown"
fi
```

### Step 2: Determine Research Depth

```bash
# Set research depth (can be overridden)
RESEARCH_DEPTH="${RESEARCH_DEPTH:-standard}"

echo ""
echo "ðŸ“Š Research depth: $RESEARCH_DEPTH"
echo ""

# Define what each depth includes
case "$RESEARCH_DEPTH" in
    "minimal")
        echo "   â€¢ Latest versions"
        echo "   â€¢ Critical security issues"
        echo "   Estimated time: ~30 seconds"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=false
        DO_DOMAIN_RESEARCH=false
        ;;
    "standard")
        echo "   â€¢ Latest versions"
        echo "   â€¢ Security issues"
        echo "   â€¢ Best practices"
        echo "   â€¢ Common pitfalls"
        echo "   Estimated time: ~2 minutes"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=true
        DO_DOMAIN_RESEARCH=false
        ;;
    "comprehensive")
        echo "   â€¢ All standard research"
        echo "   â€¢ Architecture patterns"
        echo "   â€¢ Domain knowledge"
        echo "   â€¢ Migration guides"
        echo "   Estimated time: ~5 minutes"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=true
        DO_DOMAIN_RESEARCH=true
        ;;
esac

echo ""
```

### Step 3: Execute Research Workflows

```bash
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  EXECUTING RESEARCH WORKFLOWS"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Track what was researched
RESEARCH_COMPLETED=""

# 1. Library Research (always in minimal+)
if [ "$DO_LIBRARY_RESEARCH" = "true" ]; then
    echo "ðŸ“š Researching libraries and frameworks..."
    # Workflow: Research Library

## Purpose

Research best practices, known issues, security vulnerabilities, and latest versions for each detected library/framework. Compiles findings into structured markdown.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework (if any)
- `DETECTED_DATABASE` - Database (if any)

## Outputs

- `agent-os/config/enriched-knowledge/library-research.md`

---

## Web Search Queries

For each detected technology, perform the following searches:

### Query Templates

```
1. "[library] best practices [current_year]"
2. "[library] common mistakes to avoid"
3. "[library] known issues bugs"
4. "[library] latest stable version"
5. "[library] security vulnerabilities CVE"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/library-research.md"

cat > "$OUTPUT_FILE" << 'HEADER_EOF'
# Library Research

> Auto-generated knowledge enrichment from web research
> Generated: $(date -Iseconds)

This document contains best practices, known issues, and recommendations
for the libraries and frameworks detected in your project.

---

HEADER_EOF
```

### Step 2: Research Primary Language

```bash
if [ -n "$DETECTED_LANGUAGE" ] && [ "$DETECTED_LANGUAGE" != "unknown" ]; then
    echo "   Researching $DETECTED_LANGUAGE..."
    
    cat >> "$OUTPUT_FILE" << LANG_EOF

## $DETECTED_LANGUAGE

### Best Practices

<!-- Web search: "$DETECTED_LANGUAGE best practices $CURRENT_YEAR" -->

**Recommended practices for $DETECTED_LANGUAGE development:**

- Follow official style guides and conventions
- Use type annotations where available
- Implement proper error handling
- Write comprehensive tests
- Use linting and formatting tools

### Common Pitfalls

<!-- Web search: "$DETECTED_LANGUAGE common mistakes to avoid" -->

**Common mistakes to avoid:**

- Ignoring error handling
- Not using proper dependency management
- Skipping type safety features
- Insufficient testing
- Poor code organization

### Resources

- Official documentation
- Community style guides
- Popular learning resources

---

LANG_EOF
fi
```

### Step 3: Research Framework

```bash
if [ -n "$DETECTED_FRAMEWORK" ] && [ "$DETECTED_FRAMEWORK" != "" ]; then
    echo "   Researching $DETECTED_FRAMEWORK..."
    
    cat >> "$OUTPUT_FILE" << FRAMEWORK_EOF

## $DETECTED_FRAMEWORK

### Best Practices

<!-- Web search: "$DETECTED_FRAMEWORK best practices $CURRENT_YEAR" -->

**Recommended practices for $DETECTED_FRAMEWORK:**

- Follow the framework's recommended project structure
- Use built-in features before reaching for third-party solutions
- Implement proper state management patterns
- Optimize for performance from the start
- Follow security guidelines

### Architecture Recommendations

<!-- Web search: "$DETECTED_FRAMEWORK architecture patterns" -->

**Recommended architecture patterns:**

- Component-based architecture (for UI frameworks)
- Clean separation of concerns
- Proper routing and navigation patterns
- Effective data fetching strategies
- Error boundary implementation

### Known Issues

<!-- Web search: "$DETECTED_FRAMEWORK known issues bugs" -->

**Common issues to be aware of:**

- Check the framework's GitHub issues for current bugs
- Review migration guides for breaking changes
- Monitor security advisories

### Performance Tips

<!-- Web search: "$DETECTED_FRAMEWORK performance optimization" -->

**Performance optimization strategies:**

- Implement lazy loading
- Use memoization appropriately
- Optimize bundle size
- Monitor and profile regularly

---

FRAMEWORK_EOF
fi
```

### Step 4: Research Database

```bash
if [ -n "$DETECTED_DATABASE" ] && [ "$DETECTED_DATABASE" != "" ]; then
    echo "   Researching $DETECTED_DATABASE..."
    
    cat >> "$OUTPUT_FILE" << DB_EOF

## $DETECTED_DATABASE

### Best Practices

<!-- Web search: "$DETECTED_DATABASE best practices $CURRENT_YEAR" -->

**Database best practices:**

- Use proper indexing strategies
- Implement connection pooling
- Follow security best practices
- Regular backup and maintenance
- Monitor query performance

### Security Considerations

<!-- Web search: "$DETECTED_DATABASE security best practices" -->

**Security recommendations:**

- Use parameterized queries (prevent SQL injection)
- Implement proper authentication
- Encrypt sensitive data
- Regular security audits
- Keep database updated

### Performance Optimization

<!-- Web search: "$DETECTED_DATABASE performance tuning" -->

**Performance tips:**

- Optimize query patterns
- Use appropriate indexes
- Monitor slow queries
- Consider caching strategies
- Regular maintenance

---

DB_EOF
fi
```

### Step 5: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

---

## How to Use This Document

1. **Review recommendations** before implementing features
2. **Check known issues** when debugging problems
3. **Follow best practices** for code quality
4. **Monitor security advisories** for your dependencies

## Sources

Research compiled from:
- Official documentation
- GitHub issues and discussions
- Stack Overflow community
- Security advisory databases
- Community best practices

---

*Generated by Geist Adaptive Questionnaire System*
*For the most current information, verify with official sources*

FOOTER_EOF

echo "   âœ“ Library research saved to $OUTPUT_FILE"
```

---

## Web Search Integration

When executing this workflow, the AI agent should use the `web_search` tool to gather current information:

```markdown
### Example Web Search Calls

For React framework:
- web_search("React 18 best practices 2026")
- web_search("React common mistakes to avoid")
- web_search("React known issues bugs")
- web_search("React latest stable version")

For PostgreSQL:
- web_search("PostgreSQL best practices 2026")
- web_search("PostgreSQL security hardening")
- web_search("PostgreSQL performance tuning")
```

---

## Important Constraints

- Must attribute information sources
- Should include publication dates for time-sensitive info
- Must handle missing/unknown technologies gracefully
- Should prioritize official documentation over blog posts
- Results should be actionable, not just informational

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}library,"
fi

# 2. Security Research (always in minimal+)
if [ "$DO_SECURITY_RESEARCH" = "true" ]; then
    echo ""
    echo "ðŸ”’ Researching security vulnerabilities..."
    # Workflow: Research Security

## Purpose

Research security vulnerabilities (CVEs), security advisories, and security best practices for the detected dependencies.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework
- `DETECTED_DATABASE` - Database
- Dependencies from package.json, Cargo.toml, etc.

## Outputs

- `agent-os/config/enriched-knowledge/security-notes.md`

---

## Web Search Queries

### Query Templates

```
1. "[dependency] CVE vulnerabilities [year]"
2. "[dependency] security advisory"
3. "[framework] security best practices"
4. "[language] OWASP top 10 prevention"
5. "[dependency] security patch"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/security-notes.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Security Notes

> Security vulnerabilities and recommendations for your dependencies
> Generated: $(date -Iseconds)

âš ï¸ **Important**: Always verify security information with official sources.
Check npm audit, cargo audit, or pip-audit for real-time vulnerability scanning.

---

HEADER_EOF
```

### Step 2: Research Language Security

```bash
if [ -n "$DETECTED_LANGUAGE" ] && [ "$DETECTED_LANGUAGE" != "unknown" ]; then
    cat >> "$OUTPUT_FILE" << LANG_SEC_EOF

## $DETECTED_LANGUAGE Security

<!-- Web search: "$DETECTED_LANGUAGE security best practices $CURRENT_YEAR" -->

### Common Vulnerabilities

| Vulnerability | Risk | Prevention |
|---------------|------|------------|
| Injection attacks | High | Input validation, parameterized queries |
| XSS (if web) | High | Output encoding, CSP headers |
| Insecure dependencies | Medium | Regular audits, version pinning |
| Secrets exposure | Critical | Environment variables, secret managers |

### Security Tools

LANG_SEC_EOF

    case "$DETECTED_LANGUAGE" in
        "javascript"|"typescript")
            cat >> "$OUTPUT_FILE" << 'JSTOOLS_EOF'
- **npm audit** - Check for vulnerable dependencies
- **Snyk** - Continuous vulnerability monitoring
- **ESLint security plugins** - Static code analysis
- **Helmet** - Security headers for Express

```bash
# Run security audit
npm audit
npm audit fix

# Use Snyk
npx snyk test
```

JSTOOLS_EOF
            ;;
        "rust")
            cat >> "$OUTPUT_FILE" << 'RSTOOLS_EOF'
- **cargo audit** - Check for vulnerable dependencies
- **cargo deny** - Lint dependencies
- **clippy** - Catch common mistakes

```bash
# Run security audit
cargo audit

# Install if needed
cargo install cargo-audit
```

RSTOOLS_EOF
            ;;
        "python")
            cat >> "$OUTPUT_FILE" << 'PYTOOLS_EOF'
- **pip-audit** - Check for vulnerable dependencies
- **safety** - Check dependencies against safety db
- **bandit** - Security linter

```bash
# Run security audit
pip-audit

# Or use safety
safety check
```

PYTOOLS_EOF
            ;;
        "go")
            cat >> "$OUTPUT_FILE" << 'GOTOOLS_EOF'
- **govulncheck** - Official Go vulnerability scanner
- **gosec** - Security linter

```bash
# Run vulnerability check
govulncheck ./...

# Install if needed
go install golang.org/x/vuln/cmd/govulncheck@latest
```

GOTOOLS_EOF
            ;;
    esac
    
    echo "---" >> "$OUTPUT_FILE"
fi
```

### Step 3: Research Framework Security

```bash
if [ -n "$DETECTED_FRAMEWORK" ]; then
    cat >> "$OUTPUT_FILE" << FRAMEWORK_SEC_EOF

## $DETECTED_FRAMEWORK Security

<!-- Web search: "$DETECTED_FRAMEWORK security vulnerabilities CVE" -->

### Security Checklist

- [ ] Keep framework updated to latest stable version
- [ ] Review security advisories regularly
- [ ] Follow framework's security guidelines
- [ ] Implement recommended security middleware
- [ ] Use built-in security features

### Known Security Considerations

FRAMEWORK_SEC_EOF

    case "$DETECTED_FRAMEWORK" in
        "react"|"vue"|"angular")
            cat >> "$OUTPUT_FILE" << 'FRONTEND_SEC_EOF'
**Frontend Framework Security:**

1. **XSS Prevention**
   - Use framework's built-in escaping
   - Avoid `dangerouslySetInnerHTML` (React) or `v-html` (Vue)
   - Sanitize user input before display

2. **CSRF Protection**
   - Use anti-CSRF tokens
   - SameSite cookie attribute
   - Verify origin headers

3. **Secure Dependencies**
   - Regular `npm audit`
   - Lock file integrity
   - Review new dependencies

FRONTEND_SEC_EOF
            ;;
        "express"|"fastify"|"koa")
            cat >> "$OUTPUT_FILE" << 'BACKEND_SEC_EOF'
**Backend Framework Security:**

1. **Request Validation**
   - Validate all input
   - Sanitize data
   - Use schema validation (Zod, Joi)

2. **Authentication**
   - Secure session management
   - Rate limiting on auth endpoints
   - Brute force protection

3. **Headers & HTTPS**
   - Use Helmet.js for security headers
   - Force HTTPS in production
   - Set secure cookie flags

BACKEND_SEC_EOF
            ;;
    esac
    
    echo "---" >> "$OUTPUT_FILE"
fi
```

### Step 4: Research Database Security

```bash
if [ -n "$DETECTED_DATABASE" ]; then
    cat >> "$OUTPUT_FILE" << DB_SEC_EOF

## $DETECTED_DATABASE Security

<!-- Web search: "$DETECTED_DATABASE security best practices" -->

### Security Checklist

- [ ] Use strong, unique passwords
- [ ] Enable authentication
- [ ] Encrypt connections (TLS/SSL)
- [ ] Regular security patches
- [ ] Principle of least privilege for users

### Common Vulnerabilities

| Risk | Prevention |
|------|------------|
| SQL Injection | Parameterized queries, ORM |
| Unauthorized access | Authentication, firewall |
| Data exposure | Encryption, access control |
| Backup theft | Encrypted backups |

### Secure Configuration

- Disable default/public access
- Use connection pooling with limits
- Enable query logging (audit)
- Regular backup testing

---

DB_SEC_EOF
fi
```

### Step 5: Add OWASP Reference

```bash
cat >> "$OUTPUT_FILE" << 'OWASP_EOF'

## OWASP Top 10 Reference

The most critical web application security risks:

| # | Risk | Key Prevention |
|---|------|----------------|
| 1 | Broken Access Control | Deny by default, validate permissions |
| 2 | Cryptographic Failures | Encrypt sensitive data, strong algorithms |
| 3 | Injection | Input validation, parameterized queries |
| 4 | Insecure Design | Threat modeling, secure patterns |
| 5 | Security Misconfiguration | Hardened configs, remove defaults |
| 6 | Vulnerable Components | Regular updates, dependency scanning |
| 7 | Auth Failures | MFA, rate limiting, secure sessions |
| 8 | Data Integrity Failures | Digital signatures, CI/CD security |
| 9 | Logging Failures | Comprehensive logging, monitoring |
| 10 | SSRF | Validate URLs, network segmentation |

---

OWASP_EOF
```

### Step 6: Add Severity Legend and Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Severity Levels

| Level | Description | Action |
|-------|-------------|--------|
| ðŸ”´ **CRITICAL** | Actively exploited, patch immediately | Stop and fix now |
| ðŸŸ  **HIGH** | Significant risk, patch soon | Fix within 24-48 hours |
| ðŸŸ¡ **MEDIUM** | Moderate risk | Fix within 1 week |
| ðŸŸ¢ **LOW** | Minor risk | Fix in next release |

## Recommended Actions

1. **Immediate**: Run security audit tool for your language
2. **Weekly**: Review security advisories for your dependencies
3. **Monthly**: Update dependencies to latest stable versions
4. **Quarterly**: Perform security review/penetration testing

## Resources

- [OWASP](https://owasp.org)
- [CVE Database](https://cve.mitre.org)
- [GitHub Security Advisories](https://github.com/advisories)
- [Snyk Vulnerability Database](https://snyk.io/vuln/)

---

*Generated by Geist Adaptive Questionnaire System*
*Always verify security information with official sources*

FOOTER_EOF

echo "   âœ“ Security notes saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Must clearly indicate severity of issues
- Should provide actionable remediation steps
- Must recommend official security scanning tools
- Should link to authoritative sources
- Must emphasize verification with real-time tools

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}security,"
fi

# 3. Stack Patterns (standard+)
if [ "$DO_STACK_PATTERNS" = "true" ]; then
    echo ""
    echo "ðŸ—ï¸ Researching architecture patterns..."
    # Workflow: Research Stack Patterns

## Purpose

Research architecture patterns, project structure conventions, and testing strategies for the detected tech stack combination.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework
- `DETECTED_BACKEND` - Backend technology
- `DETECTED_DATABASE` - Database

## Outputs

- `agent-os/config/enriched-knowledge/stack-best-practices.md`

---

## Web Search Queries

### Query Templates

```
1. "[frontend] [backend] architecture patterns"
2. "[stack] project structure best practices"
3. "[stack] folder organization"
4. "[stack] testing strategy"
5. "[frontend] [backend] full stack patterns"
```

---

## Workflow

### Step 1: Build Stack String

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/stack-best-practices.md"

# Build a stack description string
STACK_PARTS=""
[ -n "$DETECTED_FRAMEWORK" ] && STACK_PARTS="$DETECTED_FRAMEWORK"
[ -n "$DETECTED_BACKEND" ] && STACK_PARTS="$STACK_PARTS $DETECTED_BACKEND"
[ -n "$DETECTED_DATABASE" ] && STACK_PARTS="$STACK_PARTS $DETECTED_DATABASE"
STACK_STRING=$(echo "$STACK_PARTS" | xargs)  # Trim whitespace

echo "   Researching patterns for: $STACK_STRING"
```

### Step 2: Initialize Output File

```bash
cat > "$OUTPUT_FILE" << HEADER_EOF
# Tech Stack Best Practices

> Architecture patterns and recommendations for your tech stack
> Generated: $(date -Iseconds)

**Detected Stack:** $STACK_STRING

---

HEADER_EOF
```

### Step 3: Research Architecture Patterns

```bash
cat >> "$OUTPUT_FILE" << 'ARCH_EOF'

## Recommended Architecture

<!-- Web search: "[stack] architecture patterns [year]" -->

### Overview

Based on your tech stack, consider these architectural approaches:

### Layer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Presentation Layer            â”‚
â”‚    (UI Components, Views, Templates)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Application Layer             â”‚
â”‚   (Controllers, Services, Use Cases)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚             Domain Layer                â”‚
â”‚    (Business Logic, Entities, Rules)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          Infrastructure Layer           â”‚
â”‚  (Database, External APIs, File System) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Principles

1. **Separation of Concerns** - Each layer has a distinct responsibility
2. **Dependency Inversion** - High-level modules don't depend on low-level modules
3. **Single Responsibility** - Each component does one thing well
4. **Open/Closed** - Open for extension, closed for modification

---

ARCH_EOF
```

### Step 4: Research Project Structure

```bash
cat >> "$OUTPUT_FILE" << 'STRUCTURE_EOF'

## Recommended Project Structure

<!-- Web search: "[stack] project structure best practices" -->

### General Structure

```
project-root/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/     # UI components (if applicable)
â”‚   â”œâ”€â”€ services/       # Business logic services
â”‚   â”œâ”€â”€ models/         # Data models/entities
â”‚   â”œâ”€â”€ utils/          # Utility functions
â”‚   â”œâ”€â”€ config/         # Configuration files
â”‚   â””â”€â”€ index.ts        # Entry point
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/           # Unit tests
â”‚   â”œâ”€â”€ integration/    # Integration tests
â”‚   â””â”€â”€ e2e/            # End-to-end tests
â”œâ”€â”€ docs/               # Documentation
â”œâ”€â”€ scripts/            # Build/deploy scripts
â””â”€â”€ config files        # package.json, tsconfig, etc.
```

### Naming Conventions

- **Files**: `kebab-case.ts` or `PascalCase.tsx` for components
- **Directories**: `kebab-case/`
- **Tests**: `*.test.ts` or `*.spec.ts`
- **Types/Interfaces**: `PascalCase`

### Module Organization

- Group by feature, not by type (feature-first)
- Keep related files close together
- Limit folder nesting (max 3-4 levels)
- Use barrel exports (index.ts) for clean imports

---

STRUCTURE_EOF
```

### Step 5: Research Testing Strategy

```bash
cat >> "$OUTPUT_FILE" << 'TESTING_EOF'

## Testing Strategy

<!-- Web search: "[stack] testing strategy" -->

### Test Pyramid

```
         /\
        /  \
       / E2E\        <- Few, slow, expensive
      /â”€â”€â”€â”€â”€â”€\
     /  Int.  \      <- Some, medium speed
    /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
   /    Unit    \    <- Many, fast, cheap
  /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
```

### Recommended Coverage

| Layer | Coverage Target | Focus |
|-------|----------------|-------|
| Unit | 80%+ | Business logic, utilities |
| Integration | 60%+ | API endpoints, services |
| E2E | Critical paths | User journeys |

### Testing Best Practices

1. **Write tests first** (TDD) for complex logic
2. **Test behavior, not implementation**
3. **Use meaningful test names** that describe the scenario
4. **Keep tests independent** - no shared state
5. **Mock external dependencies** in unit tests
6. **Use factories** for test data creation

### Recommended Tools

- **Unit Testing**: Jest, Vitest, pytest, cargo test
- **Integration Testing**: Supertest, pytest, integration frameworks
- **E2E Testing**: Playwright, Cypress, Selenium
- **Mocking**: MSW, unittest.mock, mockall

---

TESTING_EOF
```

### Step 6: Research Data Flow Patterns

```bash
cat >> "$OUTPUT_FILE" << 'DATAFLOW_EOF'

## Data Flow Patterns

<!-- Web search: "[stack] state management patterns" -->

### Frontend State Management

For complex applications, consider:

1. **Local State** - Component-level state for UI
2. **Server State** - Data from API (use React Query, SWR, etc.)
3. **Global State** - Shared across components (Context, Redux, Zustand)

### API Design Patterns

- **REST** - Resource-based, stateless, cacheable
- **GraphQL** - Query language, single endpoint, typed
- **tRPC** - End-to-end type safety for TypeScript

### Data Fetching Strategies

1. **Server-Side Rendering (SSR)** - SEO, initial load
2. **Static Site Generation (SSG)** - Performance, caching
3. **Client-Side Rendering (CSR)** - Interactivity
4. **Incremental Static Regeneration (ISR)** - Best of both

---

DATAFLOW_EOF
```

### Step 7: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Implementation Checklist

Before implementing, verify:

- [ ] Architecture aligns with team expertise
- [ ] Project structure supports scalability
- [ ] Testing strategy covers critical paths
- [ ] Data flow patterns match requirements
- [ ] Security considerations addressed

## Sources

Patterns compiled from:
- Official framework documentation
- Community best practices
- Industry standards (Clean Architecture, DDD)
- Real-world project examples

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Stack patterns saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Patterns should be practical, not theoretical
- Should adapt recommendations to detected stack
- Must provide concrete examples where possible
- Should highlight trade-offs for different approaches

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}patterns,"
fi

# 4. Domain Research (comprehensive only)
if [ "$DO_DOMAIN_RESEARCH" = "true" ]; then
    echo ""
    echo "ðŸŽ¯ Researching domain-specific knowledge..."
    # Workflow: Research Domain

## Purpose

Research domain-specific patterns, compliance requirements, and industry best practices based on the detected project type.

## Inputs

- `PROJECT_TYPE` - web_app, cli, api, library, monorepo
- `DETECTED_FRAMEWORK` - For context
- `SECURITY_LEVEL` - Informs compliance research depth

## Outputs

- `agent-os/config/enriched-knowledge/domain-knowledge.md`

---

## Web Search Queries

### Query Templates

```
1. "[project_type] software architecture patterns"
2. "[project_type] industry best practices"
3. "[project_type] compliance requirements"
4. "[project_type] common challenges"
5. "[project_type] scalability patterns"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/domain-knowledge.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Domain Knowledge

> Industry-specific patterns and considerations
> Generated: $(date -Iseconds)

**Project Type:** ${PROJECT_TYPE:-unknown}

---

HEADER_EOF
```

### Step 2: Research Based on Project Type

```bash
case "${PROJECT_TYPE:-unknown}" in
    "web_app")
        cat >> "$OUTPUT_FILE" << 'WEBAPP_EOF'

## Web Application Patterns

<!-- Web search: "web application architecture patterns [year]" -->

### Common Architectures

1. **Single Page Application (SPA)**
   - Rich client-side interactivity
   - API-driven data fetching
   - Client-side routing
   
2. **Server-Side Rendered (SSR)**
   - Better SEO
   - Faster initial load
   - Server load considerations

3. **Hybrid (SSR + SPA)**
   - Best of both worlds
   - Complexity trade-off
   - Popular in modern frameworks

### User Experience Considerations

- **Performance**: First Contentful Paint < 1.8s
- **Accessibility**: WCAG 2.1 compliance
- **Responsive Design**: Mobile-first approach
- **Progressive Enhancement**: Works without JS

### Security Requirements

- **Authentication**: Secure login flows
- **Authorization**: Role-based access control
- **Data Protection**: HTTPS, CSP headers
- **Input Validation**: Client and server-side

---

WEBAPP_EOF
        ;;
        
    "api")
        cat >> "$OUTPUT_FILE" << 'API_EOF'

## API Service Patterns

<!-- Web search: "API design best practices [year]" -->

### API Design Principles

1. **RESTful Design**
   - Resource-oriented URLs
   - HTTP methods for operations
   - Stateless communication
   
2. **Versioning Strategy**
   - URL versioning: `/api/v1/`
   - Header versioning
   - Query parameter versioning

3. **Error Handling**
   - Consistent error format
   - Meaningful status codes
   - Detailed error messages (dev only)

### Performance Patterns

- **Pagination**: Cursor-based for large datasets
- **Caching**: ETags, Cache-Control headers
- **Rate Limiting**: Protect against abuse
- **Compression**: gzip/brotli responses

### Documentation

- OpenAPI/Swagger specification
- Interactive documentation
- Code examples
- Versioned docs

---

API_EOF
        ;;
        
    "cli")
        cat >> "$OUTPUT_FILE" << 'CLI_EOF'

## CLI Tool Patterns

<!-- Web search: "CLI application best practices" -->

### CLI Design Principles

1. **User Experience**
   - Clear help text (`--help`)
   - Intuitive command structure
   - Meaningful error messages
   - Progress indicators for long operations

2. **Command Structure**
   ```
   tool <command> [subcommand] [options] [arguments]
   ```

3. **Configuration**
   - Config file support
   - Environment variables
   - Command-line flags (highest priority)

### Best Practices

- Follow POSIX conventions
- Support piping and redirection
- Provide quiet (`-q`) and verbose (`-v`) modes
- Exit with appropriate codes
- Support both short (`-h`) and long (`--help`) flags

### Distribution

- Single binary if possible
- Package manager support (npm, cargo, brew)
- Auto-update mechanism
- Cross-platform builds

---

CLI_EOF
        ;;
        
    "library")
        cat >> "$OUTPUT_FILE" << 'LIB_EOF'

## Library Design Patterns

<!-- Web search: "library design best practices" -->

### API Design

1. **Minimal Surface Area**
   - Expose only what's needed
   - Internal vs. public APIs
   - Deprecation strategy

2. **Consistency**
   - Naming conventions
   - Error handling patterns
   - Return value patterns

3. **Extensibility**
   - Plugin/middleware support
   - Configuration options
   - Hooks for customization

### Documentation

- Comprehensive README
- API documentation
- Usage examples
- Migration guides
- Changelog

### Versioning

- Semantic versioning (SemVer)
- Clear breaking change policy
- Deprecation warnings
- LTS versions for stability

---

LIB_EOF
        ;;
        
    *)
        cat >> "$OUTPUT_FILE" << 'DEFAULT_EOF'

## General Software Patterns

### Universal Best Practices

1. **Code Quality**
   - Consistent formatting
   - Meaningful naming
   - Documentation
   - Testing

2. **Architecture**
   - Separation of concerns
   - Dependency management
   - Configuration management
   - Error handling

3. **Operations**
   - Logging and monitoring
   - Health checks
   - Graceful shutdown
   - Configuration via environment

---

DEFAULT_EOF
        ;;
esac
```

### Step 3: Research Compliance Requirements

```bash
if [ "$SECURITY_LEVEL" = "high" ]; then
    cat >> "$OUTPUT_FILE" << 'COMPLIANCE_EOF'

## Compliance Considerations

<!-- Web search: "software compliance requirements [year]" -->

### Common Compliance Frameworks

| Framework | Focus | Key Requirements |
|-----------|-------|------------------|
| **GDPR** | Data Privacy (EU) | Consent, data rights, breach notification |
| **SOC 2** | Security Controls | Access control, encryption, monitoring |
| **HIPAA** | Healthcare Data | PHI protection, access logs, encryption |
| **PCI-DSS** | Payment Data | Cardholder data protection, network security |

### General Compliance Checklist

- [ ] Data encryption at rest and in transit
- [ ] Access control and authentication
- [ ] Audit logging
- [ ] Data retention policies
- [ ] Incident response plan
- [ ] Regular security assessments
- [ ] Privacy policy and terms of service

### Security Controls

1. **Authentication**
   - Multi-factor authentication
   - Session management
   - Password policies

2. **Authorization**
   - Principle of least privilege
   - Role-based access control
   - Resource-level permissions

3. **Data Protection**
   - Encryption standards (AES-256)
   - Key management
   - Secure deletion

---

COMPLIANCE_EOF
fi
```

### Step 4: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Implementation Priority

Based on your project type, prioritize:

1. **Core Functionality** - Get the basics right first
2. **Security** - Build security in from the start
3. **Performance** - Optimize critical paths
4. **Scalability** - Design for growth
5. **Maintainability** - Think long-term

## Sources

Domain knowledge compiled from:
- Industry standards and frameworks
- Community best practices
- Regulatory requirements
- Real-world case studies

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Domain knowledge saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Research should be specific to detected project type
- Compliance info should match security level
- Should provide actionable recommendations
- Must include implementation priorities

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}domain,"
fi

# 5. Version Analysis (always)
echo ""
echo "ðŸ“¦ Analyzing dependency versions..."
# Workflow: Version Analysis

## Purpose

Compare detected dependency versions against latest stable versions, flag outdated dependencies, and note breaking changes in newer versions.

## Inputs

- `package.json` (Node.js)
- `Cargo.toml` (Rust)
- `go.mod` (Go)
- `requirements.txt` / `pyproject.toml` (Python)

## Outputs

- `agent-os/config/enriched-knowledge/version-analysis.md`

---

## Web Search Queries

### Query Templates

```
1. "[package] latest version"
2. "[package] changelog breaking changes"
3. "[package] migration guide v[old] to v[new]"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/version-analysis.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Version Analysis

> Dependency version status and update recommendations
> Generated: $(date -Iseconds)

This analysis compares your current dependency versions against the latest
stable releases and flags potential updates.

---

## Summary

HEADER_EOF
```

### Step 2: Analyze Node.js Dependencies

```bash
if [ -f "package.json" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Node.js Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`npm outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Package | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|---------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Extract key dependencies (simplified - in practice, use npm outdated)
    # This is a template showing the expected output format
    
    # Check for common frameworks and their typical update status
    if grep -q '"react"' package.json 2>/dev/null; then
        REACT_VER=$(grep '"react"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        if [ "${REACT_VER%%.*}" -lt "18" ] 2>/dev/null; then
            echo "| react | ${REACT_VER:-unknown} | âš ï¸ OUTDATED | Consider upgrading to React 18 |" >> "$OUTPUT_FILE"
        else
            echo "| react | ${REACT_VER:-unknown} | âœ… Current | |" >> "$OUTPUT_FILE"
        fi
    fi
    
    if grep -q '"next"' package.json 2>/dev/null; then
        NEXT_VER=$(grep '"next"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| next | ${NEXT_VER:-unknown} | â„¹ï¸ Check | Major versions may have breaking changes |" >> "$OUTPUT_FILE"
    fi
    
    if grep -q '"typescript"' package.json 2>/dev/null; then
        TS_VER=$(grep '"typescript"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| typescript | ${TS_VER:-unknown} | â„¹ï¸ Check | TypeScript 5.x has new features |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check all outdated packages" >> "$OUTPUT_FILE"
    echo "npm outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all packages (minor/patch)" >> "$OUTPUT_FILE"
    echo "npm update" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Interactive update (recommended)" >> "$OUTPUT_FILE"
    echo "npx npm-check-updates -i" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 3: Analyze Rust Dependencies

```bash
if [ -f "Cargo.toml" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Rust Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`cargo outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Crate | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|-------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check for common crates
    if grep -q 'tokio' Cargo.toml 2>/dev/null; then
        TOKIO_VER=$(grep 'tokio' Cargo.toml | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| tokio | ${TOKIO_VER:-unknown} | â„¹ï¸ Check | Async runtime |" >> "$OUTPUT_FILE"
    fi
    
    if grep -q 'serde' Cargo.toml 2>/dev/null; then
        SERDE_VER=$(grep 'serde' Cargo.toml | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| serde | ${SERDE_VER:-unknown} | â„¹ï¸ Check | Serialization |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Install cargo-outdated" >> "$OUTPUT_FILE"
    echo "cargo install cargo-outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Check outdated dependencies" >> "$OUTPUT_FILE"
    echo "cargo outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update dependencies" >> "$OUTPUT_FILE"
    echo "cargo update" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 4: Analyze Python Dependencies

```bash
if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Python Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`pip list --outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Package | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|---------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check for common packages
    PYDEPS=""
    [ -f "requirements.txt" ] && PYDEPS=$(cat requirements.txt)
    
    if echo "$PYDEPS" | grep -qi 'django'; then
        DJANGO_VER=$(echo "$PYDEPS" | grep -i 'django' | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| django | ${DJANGO_VER:-unknown} | â„¹ï¸ Check | Web framework |" >> "$OUTPUT_FILE"
    fi
    
    if echo "$PYDEPS" | grep -qi 'fastapi'; then
        FASTAPI_VER=$(echo "$PYDEPS" | grep -i 'fastapi' | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| fastapi | ${FASTAPI_VER:-unknown} | â„¹ï¸ Check | API framework |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check outdated packages" >> "$OUTPUT_FILE"
    echo "pip list --outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all packages" >> "$OUTPUT_FILE"
    echo "pip install --upgrade -r requirements.txt" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Use pip-tools for better dependency management" >> "$OUTPUT_FILE"
    echo "pip install pip-tools" >> "$OUTPUT_FILE"
    echo "pip-compile --upgrade" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 5: Analyze Go Dependencies

```bash
if [ -f "go.mod" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Go Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`go list -m -u all\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Module | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|--------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check Go version
    GO_VER=$(grep "^go " go.mod | awk '{print $2}')
    echo "| go (runtime) | ${GO_VER:-unknown} | â„¹ï¸ Check | Go version |" >> "$OUTPUT_FILE"
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check for available updates" >> "$OUTPUT_FILE"
    echo "go list -m -u all" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all dependencies" >> "$OUTPUT_FILE"
    echo "go get -u ./..." >> "$OUTPUT_FILE"
    echo "go mod tidy" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 6: Add Update Strategy

```bash
cat >> "$OUTPUT_FILE" << 'STRATEGY_EOF'

---

## Update Strategy

### Version Status Legend

| Status | Meaning | Action |
|--------|---------|--------|
| âœ… Current | Latest stable version | No action needed |
| â„¹ï¸ Check | Unknown/needs verification | Run version check tool |
| âš ï¸ OUTDATED | Behind latest stable | Plan update |
| ðŸ”´ CRITICAL | Security patch available | Update immediately |

### Safe Update Process

1. **Check current versions**
   ```bash
   # Use appropriate tool for your language
   npm outdated / cargo outdated / pip list --outdated
   ```

2. **Review changelogs**
   - Check for breaking changes
   - Review migration guides
   - Note deprecated features

3. **Update in stages**
   - Patch versions first (x.x.PATCH)
   - Then minor versions (x.MINOR.x)
   - Major versions last (MAJOR.x.x)

4. **Test thoroughly**
   - Run test suite after each update
   - Check critical paths manually
   - Monitor for regressions

### When to Update

| Situation | Recommended Action |
|-----------|-------------------|
| Security vulnerability | Update immediately |
| Major version behind | Plan migration sprint |
| Minor version behind | Update during maintenance |
| Patch version behind | Update with next release |

---

STRATEGY_EOF
```

### Step 7: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Automated Version Checking

For real-time, accurate version information, use these tools:

| Language | Tool | Command |
|----------|------|---------|
| Node.js | npm | `npm outdated` |
| Node.js | npx | `npx npm-check-updates` |
| Rust | cargo | `cargo outdated` |
| Python | pip | `pip list --outdated` |
| Go | go | `go list -m -u all` |

### CI Integration

Consider adding version checking to your CI pipeline:

```yaml
# Example GitHub Action
- name: Check for outdated dependencies
  run: npm outdated || true  # Don't fail build
```

---

*Generated by Geist Adaptive Questionnaire System*
*Run the appropriate version check tool for accurate, real-time information*

FOOTER_EOF

echo "   âœ“ Version analysis saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Must recommend using real-time tools for accuracy
- Should not make definitive version claims without verification
- Must provide safe update strategy
- Should distinguish between critical and non-critical updates
- Must include commands for each language ecosystem

RESEARCH_COMPLETED="${RESEARCH_COMPLETED}versions,"
```

### Step 4: Synthesize Knowledge

```bash
echo ""
echo "ðŸ”„ Synthesizing research findings..."
# Workflow: Synthesize Knowledge

## Purpose

Combine all research outputs, remove duplicates, prioritize actionable insights, and create a unified summary for easy consumption.

## Inputs

Research files from:
- `agent-os/config/enriched-knowledge/library-research.md`
- `agent-os/config/enriched-knowledge/stack-best-practices.md`
- `agent-os/config/enriched-knowledge/domain-knowledge.md`
- `agent-os/config/enriched-knowledge/version-analysis.md`
- `agent-os/config/enriched-knowledge/security-notes.md`

## Outputs

- Updates to individual research files (deduplication)
- `agent-os/config/enriched-knowledge/README.md` - Summary index

---

## Workflow

### Step 1: Create Summary Index

```bash
KNOWLEDGE_DIR="agent-os/config/enriched-knowledge"
SUMMARY_FILE="$KNOWLEDGE_DIR/README.md"

cat > "$SUMMARY_FILE" << HEADER_EOF
# Enriched Knowledge Index

> Consolidated research findings for your project
> Generated: $(date -Iseconds)

This directory contains research gathered during the adaptive questionnaire
process. Use this knowledge to inform your development decisions.

---

## Available Research

HEADER_EOF
```

### Step 2: Index Available Files

```bash
echo "### Research Files" >> "$SUMMARY_FILE"
echo "" >> "$SUMMARY_FILE"

# Check each expected file and add to index
if [ -f "$KNOWLEDGE_DIR/library-research.md" ]; then
    echo "- ðŸ“š [Library Research](library-research.md) - Best practices for your dependencies" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/stack-best-practices.md" ]; then
    echo "- ðŸ—ï¸ [Stack Best Practices](stack-best-practices.md) - Architecture patterns for your tech stack" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/domain-knowledge.md" ]; then
    echo "- ðŸŽ¯ [Domain Knowledge](domain-knowledge.md) - Industry-specific patterns" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/version-analysis.md" ]; then
    echo "- ðŸ“¦ [Version Analysis](version-analysis.md) - Dependency version status" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/security-notes.md" ]; then
    echo "- ðŸ”’ [Security Notes](security-notes.md) - Security considerations and CVEs" >> "$SUMMARY_FILE"
fi

echo "" >> "$SUMMARY_FILE"
```

### Step 3: Extract Key Insights

```bash
cat >> "$SUMMARY_FILE" << 'INSIGHTS_HEADER'

---

## Key Insights Summary

### ðŸ”´ Critical Items

Items requiring immediate attention:

INSIGHTS_HEADER

# Check for critical security issues
if [ -f "$KNOWLEDGE_DIR/security-notes.md" ]; then
    if grep -q "CRITICAL\|ðŸ”´" "$KNOWLEDGE_DIR/security-notes.md" 2>/dev/null; then
        echo "- âš ï¸ Critical security issues found - see [Security Notes](security-notes.md)" >> "$SUMMARY_FILE"
    else
        echo "- âœ… No critical security issues detected" >> "$SUMMARY_FILE"
    fi
fi

# Check for outdated dependencies
if [ -f "$KNOWLEDGE_DIR/version-analysis.md" ]; then
    if grep -q "OUTDATED\|Major update" "$KNOWLEDGE_DIR/version-analysis.md" 2>/dev/null; then
        echo "- âš ï¸ Outdated dependencies found - see [Version Analysis](version-analysis.md)" >> "$SUMMARY_FILE"
    else
        echo "- âœ… Dependencies are up to date" >> "$SUMMARY_FILE"
    fi
fi

echo "" >> "$SUMMARY_FILE"
```

### Step 4: Add Quick Reference

```bash
cat >> "$SUMMARY_FILE" << 'QUICKREF_EOF'

### ðŸ“‹ Quick Reference

| Area | Document | When to Use |
|------|----------|-------------|
| Starting a new feature | Stack Best Practices | Architecture decisions |
| Adding dependencies | Library Research | Evaluate libraries |
| Security review | Security Notes | Before deployment |
| Updating packages | Version Analysis | Maintenance cycles |
| Domain questions | Domain Knowledge | Business logic |

---

QUICKREF_EOF
```

### Step 5: Add Usage Instructions

```bash
cat >> "$SUMMARY_FILE" << 'USAGE_EOF'

## How to Use This Knowledge

### During Development

1. **Before implementing a feature**: Check Stack Best Practices for patterns
2. **When choosing libraries**: Review Library Research for recommendations
3. **During code review**: Reference Security Notes for secure coding
4. **When debugging**: Check Library Research for known issues

### During Maintenance

1. **Regular updates**: Use Version Analysis to prioritize updates
2. **Security patches**: Follow Security Notes recommendations
3. **Refactoring**: Reference Stack Best Practices for patterns

### During Planning

1. **Architecture decisions**: Stack Best Practices + Domain Knowledge
2. **Compliance requirements**: Domain Knowledge + Security Notes
3. **Technology choices**: Library Research + Version Analysis

---

USAGE_EOF
```

### Step 6: Add Refresh Instructions

```bash
cat >> "$SUMMARY_FILE" << 'REFRESH_EOF'

## Keeping Knowledge Fresh

This research was generated at a point in time. To refresh:

1. **Re-run detection**: This will update research with latest information
2. **Manual update**: Edit files directly for project-specific notes
3. **Research depth**: Adjust depth for more/less detail

### Research Depth Levels

| Level | Time | Best For |
|-------|------|----------|
| `minimal` | ~30s | Quick updates |
| `standard` | ~2m | Regular use |
| `comprehensive` | ~5m | Major decisions |

To change depth:
```bash
RESEARCH_DEPTH=comprehensive
# Then re-run adapt-to-product or the research orchestrator
```

---

## Files in This Directory

REFRESH_EOF

# List all files with sizes
ls -lh "$KNOWLEDGE_DIR"/*.md 2>/dev/null | awk '{print "- `" $NF "` (" $5 ")"}' >> "$SUMMARY_FILE" || echo "- No files found" >> "$SUMMARY_FILE"

cat >> "$SUMMARY_FILE" << 'FOOTER_EOF'

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Knowledge synthesis complete"
echo "   âœ“ Summary index saved to $SUMMARY_FILE"
```

---

## Deduplication Logic

When synthesizing, the workflow identifies and consolidates:

1. **Repeated recommendations** across files
2. **Conflicting advice** (flags for human review)
3. **Version-specific information** (keeps most recent)
4. **Source attribution** (maintains for verification)

---

## Important Constraints

- Must maintain source attribution
- Should flag conflicting recommendations
- Must create navigable index
- Should highlight critical items prominently
- Must provide clear usage guidance

```

### Step 5: Generate Research Summary

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  RESEARCH COMPLETE"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ðŸ“ Enriched knowledge saved to: agent-os/config/enriched-knowledge/"
echo ""
echo "Files generated:"
ls -la agent-os/config/enriched-knowledge/ 2>/dev/null | grep ".md" | awk '{print "   â€¢ " $NF}'
echo ""
echo "Research areas completed: $(echo $RESEARCH_COMPLETED | sed 's/,$//' | tr ',' ', ')"
echo ""
```

---

## Integration

This workflow is called by:
- `adapt-to-product/1-setup-and-information-gathering.md` (after detection)
- `create-basepoints/1-validate-prerequisites.md` (for architecture research)

The enriched knowledge is used by:
- `deploy-agents` for specialization
- Validation workflows for security checks
- Human review for flagging issues

---

## Research Depth Guidelines

| Depth | Time | Use Case |
|-------|------|----------|
| `minimal` | ~30s | Quick setup, simple projects |
| `standard` | ~2m | Most projects (default) |
| `comprehensive` | ~5m | Enterprise, complex projects |

---

## Important Constraints

- Must handle web search failures gracefully
- Should cache results to avoid redundant searches
- Must attribute sources in output
- Should prioritize actionable insights over raw data
` - To enrich with web research

---

## Important Constraints

- Must handle missing files gracefully (no errors)
- Must provide sensible defaults when detection fails
- Must flag low-confidence detections for user questions
- Must be idempotent (can run multiple times safely)

```

This will:
- Detect language, framework, and database from config files
- Extract build/test/lint commands from package.json, Makefile, etc.
- Analyze directory structure for project type and architecture
- Check for security indicators (auth, secrets management)
- Calculate overall detection confidence

### 0.2: Enrich with Web Research

Research best practices and patterns for the detected tech stack:

```bash
# Set research depth (minimal for quick setup, standard for most projects)
RESEARCH_DEPTH="${RESEARCH_DEPTH:-standard}"

# Workflow: Research Orchestrator

## Purpose

Main orchestrator for web research. Loads detected tech stack, determines research depth, calls appropriate research workflows, and aggregates results into the enriched-knowledge directory.

## Inputs

- `agent-os/config/project-profile.yml` - Detected project profile
- `RESEARCH_DEPTH` - minimal, standard, or comprehensive (default: standard)

## Outputs

- `agent-os/config/enriched-knowledge/` directory with research results

---

## Workflow

### Step 1: Load Project Profile

```bash
echo "ðŸ”¬ Starting knowledge enrichment research..."
echo ""

# Create enriched-knowledge directory
mkdir -p agent-os/config/enriched-knowledge

# Load project profile
if [ -f "agent-os/config/project-profile.yml" ]; then
    echo "ðŸ“‚ Loading project profile..."
    
    # Extract key values (simplified parsing)
    DETECTED_LANGUAGE=$(grep "language:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    DETECTED_FRAMEWORK=$(grep "framework:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    DETECTED_DATABASE=$(grep "database:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    PROJECT_TYPE=$(grep "project_type:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    SECURITY_LEVEL=$(grep "security_level:" agent-os/config/project-profile.yml | head -1 | awk '{print $2}')
    
    echo "   Language: $DETECTED_LANGUAGE"
    echo "   Framework: ${DETECTED_FRAMEWORK:-(none)}"
    echo "   Database: ${DETECTED_DATABASE:-(none)}"
    echo "   Project Type: $PROJECT_TYPE"
else
    echo "âš ï¸ No project profile found. Run detection first."
    echo "   Using defaults..."
    DETECTED_LANGUAGE="unknown"
fi
```

### Step 2: Determine Research Depth

```bash
# Set research depth (can be overridden)
RESEARCH_DEPTH="${RESEARCH_DEPTH:-standard}"

echo ""
echo "ðŸ“Š Research depth: $RESEARCH_DEPTH"
echo ""

# Define what each depth includes
case "$RESEARCH_DEPTH" in
    "minimal")
        echo "   â€¢ Latest versions"
        echo "   â€¢ Critical security issues"
        echo "   Estimated time: ~30 seconds"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=false
        DO_DOMAIN_RESEARCH=false
        ;;
    "standard")
        echo "   â€¢ Latest versions"
        echo "   â€¢ Security issues"
        echo "   â€¢ Best practices"
        echo "   â€¢ Common pitfalls"
        echo "   Estimated time: ~2 minutes"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=true
        DO_DOMAIN_RESEARCH=false
        ;;
    "comprehensive")
        echo "   â€¢ All standard research"
        echo "   â€¢ Architecture patterns"
        echo "   â€¢ Domain knowledge"
        echo "   â€¢ Migration guides"
        echo "   Estimated time: ~5 minutes"
        DO_LIBRARY_RESEARCH=true
        DO_SECURITY_RESEARCH=true
        DO_STACK_PATTERNS=true
        DO_DOMAIN_RESEARCH=true
        ;;
esac

echo ""
```

### Step 3: Execute Research Workflows

```bash
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  EXECUTING RESEARCH WORKFLOWS"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Track what was researched
RESEARCH_COMPLETED=""

# 1. Library Research (always in minimal+)
if [ "$DO_LIBRARY_RESEARCH" = "true" ]; then
    echo "ðŸ“š Researching libraries and frameworks..."
    # Workflow: Research Library

## Purpose

Research best practices, known issues, security vulnerabilities, and latest versions for each detected library/framework. Compiles findings into structured markdown.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework (if any)
- `DETECTED_DATABASE` - Database (if any)

## Outputs

- `agent-os/config/enriched-knowledge/library-research.md`

---

## Web Search Queries

For each detected technology, perform the following searches:

### Query Templates

```
1. "[library] best practices [current_year]"
2. "[library] common mistakes to avoid"
3. "[library] known issues bugs"
4. "[library] latest stable version"
5. "[library] security vulnerabilities CVE"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/library-research.md"

cat > "$OUTPUT_FILE" << 'HEADER_EOF'
# Library Research

> Auto-generated knowledge enrichment from web research
> Generated: $(date -Iseconds)

This document contains best practices, known issues, and recommendations
for the libraries and frameworks detected in your project.

---

HEADER_EOF
```

### Step 2: Research Primary Language

```bash
if [ -n "$DETECTED_LANGUAGE" ] && [ "$DETECTED_LANGUAGE" != "unknown" ]; then
    echo "   Researching $DETECTED_LANGUAGE..."
    
    cat >> "$OUTPUT_FILE" << LANG_EOF

## $DETECTED_LANGUAGE

### Best Practices

<!-- Web search: "$DETECTED_LANGUAGE best practices $CURRENT_YEAR" -->

**Recommended practices for $DETECTED_LANGUAGE development:**

- Follow official style guides and conventions
- Use type annotations where available
- Implement proper error handling
- Write comprehensive tests
- Use linting and formatting tools

### Common Pitfalls

<!-- Web search: "$DETECTED_LANGUAGE common mistakes to avoid" -->

**Common mistakes to avoid:**

- Ignoring error handling
- Not using proper dependency management
- Skipping type safety features
- Insufficient testing
- Poor code organization

### Resources

- Official documentation
- Community style guides
- Popular learning resources

---

LANG_EOF
fi
```

### Step 3: Research Framework

```bash
if [ -n "$DETECTED_FRAMEWORK" ] && [ "$DETECTED_FRAMEWORK" != "" ]; then
    echo "   Researching $DETECTED_FRAMEWORK..."
    
    cat >> "$OUTPUT_FILE" << FRAMEWORK_EOF

## $DETECTED_FRAMEWORK

### Best Practices

<!-- Web search: "$DETECTED_FRAMEWORK best practices $CURRENT_YEAR" -->

**Recommended practices for $DETECTED_FRAMEWORK:**

- Follow the framework's recommended project structure
- Use built-in features before reaching for third-party solutions
- Implement proper state management patterns
- Optimize for performance from the start
- Follow security guidelines

### Architecture Recommendations

<!-- Web search: "$DETECTED_FRAMEWORK architecture patterns" -->

**Recommended architecture patterns:**

- Component-based architecture (for UI frameworks)
- Clean separation of concerns
- Proper routing and navigation patterns
- Effective data fetching strategies
- Error boundary implementation

### Known Issues

<!-- Web search: "$DETECTED_FRAMEWORK known issues bugs" -->

**Common issues to be aware of:**

- Check the framework's GitHub issues for current bugs
- Review migration guides for breaking changes
- Monitor security advisories

### Performance Tips

<!-- Web search: "$DETECTED_FRAMEWORK performance optimization" -->

**Performance optimization strategies:**

- Implement lazy loading
- Use memoization appropriately
- Optimize bundle size
- Monitor and profile regularly

---

FRAMEWORK_EOF
fi
```

### Step 4: Research Database

```bash
if [ -n "$DETECTED_DATABASE" ] && [ "$DETECTED_DATABASE" != "" ]; then
    echo "   Researching $DETECTED_DATABASE..."
    
    cat >> "$OUTPUT_FILE" << DB_EOF

## $DETECTED_DATABASE

### Best Practices

<!-- Web search: "$DETECTED_DATABASE best practices $CURRENT_YEAR" -->

**Database best practices:**

- Use proper indexing strategies
- Implement connection pooling
- Follow security best practices
- Regular backup and maintenance
- Monitor query performance

### Security Considerations

<!-- Web search: "$DETECTED_DATABASE security best practices" -->

**Security recommendations:**

- Use parameterized queries (prevent SQL injection)
- Implement proper authentication
- Encrypt sensitive data
- Regular security audits
- Keep database updated

### Performance Optimization

<!-- Web search: "$DETECTED_DATABASE performance tuning" -->

**Performance tips:**

- Optimize query patterns
- Use appropriate indexes
- Monitor slow queries
- Consider caching strategies
- Regular maintenance

---

DB_EOF
fi
```

### Step 5: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

---

## How to Use This Document

1. **Review recommendations** before implementing features
2. **Check known issues** when debugging problems
3. **Follow best practices** for code quality
4. **Monitor security advisories** for your dependencies

## Sources

Research compiled from:
- Official documentation
- GitHub issues and discussions
- Stack Overflow community
- Security advisory databases
- Community best practices

---

*Generated by Geist Adaptive Questionnaire System*
*For the most current information, verify with official sources*

FOOTER_EOF

echo "   âœ“ Library research saved to $OUTPUT_FILE"
```

---

## Web Search Integration

When executing this workflow, the AI agent should use the `web_search` tool to gather current information:

```markdown
### Example Web Search Calls

For React framework:
- web_search("React 18 best practices 2026")
- web_search("React common mistakes to avoid")
- web_search("React known issues bugs")
- web_search("React latest stable version")

For PostgreSQL:
- web_search("PostgreSQL best practices 2026")
- web_search("PostgreSQL security hardening")
- web_search("PostgreSQL performance tuning")
```

---

## Important Constraints

- Must attribute information sources
- Should include publication dates for time-sensitive info
- Must handle missing/unknown technologies gracefully
- Should prioritize official documentation over blog posts
- Results should be actionable, not just informational

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}library,"
fi

# 2. Security Research (always in minimal+)
if [ "$DO_SECURITY_RESEARCH" = "true" ]; then
    echo ""
    echo "ðŸ”’ Researching security vulnerabilities..."
    # Workflow: Research Security

## Purpose

Research security vulnerabilities (CVEs), security advisories, and security best practices for the detected dependencies.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework
- `DETECTED_DATABASE` - Database
- Dependencies from package.json, Cargo.toml, etc.

## Outputs

- `agent-os/config/enriched-knowledge/security-notes.md`

---

## Web Search Queries

### Query Templates

```
1. "[dependency] CVE vulnerabilities [year]"
2. "[dependency] security advisory"
3. "[framework] security best practices"
4. "[language] OWASP top 10 prevention"
5. "[dependency] security patch"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/security-notes.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Security Notes

> Security vulnerabilities and recommendations for your dependencies
> Generated: $(date -Iseconds)

âš ï¸ **Important**: Always verify security information with official sources.
Check npm audit, cargo audit, or pip-audit for real-time vulnerability scanning.

---

HEADER_EOF
```

### Step 2: Research Language Security

```bash
if [ -n "$DETECTED_LANGUAGE" ] && [ "$DETECTED_LANGUAGE" != "unknown" ]; then
    cat >> "$OUTPUT_FILE" << LANG_SEC_EOF

## $DETECTED_LANGUAGE Security

<!-- Web search: "$DETECTED_LANGUAGE security best practices $CURRENT_YEAR" -->

### Common Vulnerabilities

| Vulnerability | Risk | Prevention |
|---------------|------|------------|
| Injection attacks | High | Input validation, parameterized queries |
| XSS (if web) | High | Output encoding, CSP headers |
| Insecure dependencies | Medium | Regular audits, version pinning |
| Secrets exposure | Critical | Environment variables, secret managers |

### Security Tools

LANG_SEC_EOF

    case "$DETECTED_LANGUAGE" in
        "javascript"|"typescript")
            cat >> "$OUTPUT_FILE" << 'JSTOOLS_EOF'
- **npm audit** - Check for vulnerable dependencies
- **Snyk** - Continuous vulnerability monitoring
- **ESLint security plugins** - Static code analysis
- **Helmet** - Security headers for Express

```bash
# Run security audit
npm audit
npm audit fix

# Use Snyk
npx snyk test
```

JSTOOLS_EOF
            ;;
        "rust")
            cat >> "$OUTPUT_FILE" << 'RSTOOLS_EOF'
- **cargo audit** - Check for vulnerable dependencies
- **cargo deny** - Lint dependencies
- **clippy** - Catch common mistakes

```bash
# Run security audit
cargo audit

# Install if needed
cargo install cargo-audit
```

RSTOOLS_EOF
            ;;
        "python")
            cat >> "$OUTPUT_FILE" << 'PYTOOLS_EOF'
- **pip-audit** - Check for vulnerable dependencies
- **safety** - Check dependencies against safety db
- **bandit** - Security linter

```bash
# Run security audit
pip-audit

# Or use safety
safety check
```

PYTOOLS_EOF
            ;;
        "go")
            cat >> "$OUTPUT_FILE" << 'GOTOOLS_EOF'
- **govulncheck** - Official Go vulnerability scanner
- **gosec** - Security linter

```bash
# Run vulnerability check
govulncheck ./...

# Install if needed
go install golang.org/x/vuln/cmd/govulncheck@latest
```

GOTOOLS_EOF
            ;;
    esac
    
    echo "---" >> "$OUTPUT_FILE"
fi
```

### Step 3: Research Framework Security

```bash
if [ -n "$DETECTED_FRAMEWORK" ]; then
    cat >> "$OUTPUT_FILE" << FRAMEWORK_SEC_EOF

## $DETECTED_FRAMEWORK Security

<!-- Web search: "$DETECTED_FRAMEWORK security vulnerabilities CVE" -->

### Security Checklist

- [ ] Keep framework updated to latest stable version
- [ ] Review security advisories regularly
- [ ] Follow framework's security guidelines
- [ ] Implement recommended security middleware
- [ ] Use built-in security features

### Known Security Considerations

FRAMEWORK_SEC_EOF

    case "$DETECTED_FRAMEWORK" in
        "react"|"vue"|"angular")
            cat >> "$OUTPUT_FILE" << 'FRONTEND_SEC_EOF'
**Frontend Framework Security:**

1. **XSS Prevention**
   - Use framework's built-in escaping
   - Avoid `dangerouslySetInnerHTML` (React) or `v-html` (Vue)
   - Sanitize user input before display

2. **CSRF Protection**
   - Use anti-CSRF tokens
   - SameSite cookie attribute
   - Verify origin headers

3. **Secure Dependencies**
   - Regular `npm audit`
   - Lock file integrity
   - Review new dependencies

FRONTEND_SEC_EOF
            ;;
        "express"|"fastify"|"koa")
            cat >> "$OUTPUT_FILE" << 'BACKEND_SEC_EOF'
**Backend Framework Security:**

1. **Request Validation**
   - Validate all input
   - Sanitize data
   - Use schema validation (Zod, Joi)

2. **Authentication**
   - Secure session management
   - Rate limiting on auth endpoints
   - Brute force protection

3. **Headers & HTTPS**
   - Use Helmet.js for security headers
   - Force HTTPS in production
   - Set secure cookie flags

BACKEND_SEC_EOF
            ;;
    esac
    
    echo "---" >> "$OUTPUT_FILE"
fi
```

### Step 4: Research Database Security

```bash
if [ -n "$DETECTED_DATABASE" ]; then
    cat >> "$OUTPUT_FILE" << DB_SEC_EOF

## $DETECTED_DATABASE Security

<!-- Web search: "$DETECTED_DATABASE security best practices" -->

### Security Checklist

- [ ] Use strong, unique passwords
- [ ] Enable authentication
- [ ] Encrypt connections (TLS/SSL)
- [ ] Regular security patches
- [ ] Principle of least privilege for users

### Common Vulnerabilities

| Risk | Prevention |
|------|------------|
| SQL Injection | Parameterized queries, ORM |
| Unauthorized access | Authentication, firewall |
| Data exposure | Encryption, access control |
| Backup theft | Encrypted backups |

### Secure Configuration

- Disable default/public access
- Use connection pooling with limits
- Enable query logging (audit)
- Regular backup testing

---

DB_SEC_EOF
fi
```

### Step 5: Add OWASP Reference

```bash
cat >> "$OUTPUT_FILE" << 'OWASP_EOF'

## OWASP Top 10 Reference

The most critical web application security risks:

| # | Risk | Key Prevention |
|---|------|----------------|
| 1 | Broken Access Control | Deny by default, validate permissions |
| 2 | Cryptographic Failures | Encrypt sensitive data, strong algorithms |
| 3 | Injection | Input validation, parameterized queries |
| 4 | Insecure Design | Threat modeling, secure patterns |
| 5 | Security Misconfiguration | Hardened configs, remove defaults |
| 6 | Vulnerable Components | Regular updates, dependency scanning |
| 7 | Auth Failures | MFA, rate limiting, secure sessions |
| 8 | Data Integrity Failures | Digital signatures, CI/CD security |
| 9 | Logging Failures | Comprehensive logging, monitoring |
| 10 | SSRF | Validate URLs, network segmentation |

---

OWASP_EOF
```

### Step 6: Add Severity Legend and Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Severity Levels

| Level | Description | Action |
|-------|-------------|--------|
| ðŸ”´ **CRITICAL** | Actively exploited, patch immediately | Stop and fix now |
| ðŸŸ  **HIGH** | Significant risk, patch soon | Fix within 24-48 hours |
| ðŸŸ¡ **MEDIUM** | Moderate risk | Fix within 1 week |
| ðŸŸ¢ **LOW** | Minor risk | Fix in next release |

## Recommended Actions

1. **Immediate**: Run security audit tool for your language
2. **Weekly**: Review security advisories for your dependencies
3. **Monthly**: Update dependencies to latest stable versions
4. **Quarterly**: Perform security review/penetration testing

## Resources

- [OWASP](https://owasp.org)
- [CVE Database](https://cve.mitre.org)
- [GitHub Security Advisories](https://github.com/advisories)
- [Snyk Vulnerability Database](https://snyk.io/vuln/)

---

*Generated by Geist Adaptive Questionnaire System*
*Always verify security information with official sources*

FOOTER_EOF

echo "   âœ“ Security notes saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Must clearly indicate severity of issues
- Should provide actionable remediation steps
- Must recommend official security scanning tools
- Should link to authoritative sources
- Must emphasize verification with real-time tools

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}security,"
fi

# 3. Stack Patterns (standard+)
if [ "$DO_STACK_PATTERNS" = "true" ]; then
    echo ""
    echo "ðŸ—ï¸ Researching architecture patterns..."
    # Workflow: Research Stack Patterns

## Purpose

Research architecture patterns, project structure conventions, and testing strategies for the detected tech stack combination.

## Inputs

- `DETECTED_LANGUAGE` - Primary language
- `DETECTED_FRAMEWORK` - Web framework
- `DETECTED_BACKEND` - Backend technology
- `DETECTED_DATABASE` - Database

## Outputs

- `agent-os/config/enriched-knowledge/stack-best-practices.md`

---

## Web Search Queries

### Query Templates

```
1. "[frontend] [backend] architecture patterns"
2. "[stack] project structure best practices"
3. "[stack] folder organization"
4. "[stack] testing strategy"
5. "[frontend] [backend] full stack patterns"
```

---

## Workflow

### Step 1: Build Stack String

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/stack-best-practices.md"

# Build a stack description string
STACK_PARTS=""
[ -n "$DETECTED_FRAMEWORK" ] && STACK_PARTS="$DETECTED_FRAMEWORK"
[ -n "$DETECTED_BACKEND" ] && STACK_PARTS="$STACK_PARTS $DETECTED_BACKEND"
[ -n "$DETECTED_DATABASE" ] && STACK_PARTS="$STACK_PARTS $DETECTED_DATABASE"
STACK_STRING=$(echo "$STACK_PARTS" | xargs)  # Trim whitespace

echo "   Researching patterns for: $STACK_STRING"
```

### Step 2: Initialize Output File

```bash
cat > "$OUTPUT_FILE" << HEADER_EOF
# Tech Stack Best Practices

> Architecture patterns and recommendations for your tech stack
> Generated: $(date -Iseconds)

**Detected Stack:** $STACK_STRING

---

HEADER_EOF
```

### Step 3: Research Architecture Patterns

```bash
cat >> "$OUTPUT_FILE" << 'ARCH_EOF'

## Recommended Architecture

<!-- Web search: "[stack] architecture patterns [year]" -->

### Overview

Based on your tech stack, consider these architectural approaches:

### Layer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Presentation Layer            â”‚
â”‚    (UI Components, Views, Templates)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Application Layer             â”‚
â”‚   (Controllers, Services, Use Cases)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚             Domain Layer                â”‚
â”‚    (Business Logic, Entities, Rules)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          Infrastructure Layer           â”‚
â”‚  (Database, External APIs, File System) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Principles

1. **Separation of Concerns** - Each layer has a distinct responsibility
2. **Dependency Inversion** - High-level modules don't depend on low-level modules
3. **Single Responsibility** - Each component does one thing well
4. **Open/Closed** - Open for extension, closed for modification

---

ARCH_EOF
```

### Step 4: Research Project Structure

```bash
cat >> "$OUTPUT_FILE" << 'STRUCTURE_EOF'

## Recommended Project Structure

<!-- Web search: "[stack] project structure best practices" -->

### General Structure

```
project-root/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/     # UI components (if applicable)
â”‚   â”œâ”€â”€ services/       # Business logic services
â”‚   â”œâ”€â”€ models/         # Data models/entities
â”‚   â”œâ”€â”€ utils/          # Utility functions
â”‚   â”œâ”€â”€ config/         # Configuration files
â”‚   â””â”€â”€ index.ts        # Entry point
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/           # Unit tests
â”‚   â”œâ”€â”€ integration/    # Integration tests
â”‚   â””â”€â”€ e2e/            # End-to-end tests
â”œâ”€â”€ docs/               # Documentation
â”œâ”€â”€ scripts/            # Build/deploy scripts
â””â”€â”€ config files        # package.json, tsconfig, etc.
```

### Naming Conventions

- **Files**: `kebab-case.ts` or `PascalCase.tsx` for components
- **Directories**: `kebab-case/`
- **Tests**: `*.test.ts` or `*.spec.ts`
- **Types/Interfaces**: `PascalCase`

### Module Organization

- Group by feature, not by type (feature-first)
- Keep related files close together
- Limit folder nesting (max 3-4 levels)
- Use barrel exports (index.ts) for clean imports

---

STRUCTURE_EOF
```

### Step 5: Research Testing Strategy

```bash
cat >> "$OUTPUT_FILE" << 'TESTING_EOF'

## Testing Strategy

<!-- Web search: "[stack] testing strategy" -->

### Test Pyramid

```
         /\
        /  \
       / E2E\        <- Few, slow, expensive
      /â”€â”€â”€â”€â”€â”€\
     /  Int.  \      <- Some, medium speed
    /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
   /    Unit    \    <- Many, fast, cheap
  /â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\
```

### Recommended Coverage

| Layer | Coverage Target | Focus |
|-------|----------------|-------|
| Unit | 80%+ | Business logic, utilities |
| Integration | 60%+ | API endpoints, services |
| E2E | Critical paths | User journeys |

### Testing Best Practices

1. **Write tests first** (TDD) for complex logic
2. **Test behavior, not implementation**
3. **Use meaningful test names** that describe the scenario
4. **Keep tests independent** - no shared state
5. **Mock external dependencies** in unit tests
6. **Use factories** for test data creation

### Recommended Tools

- **Unit Testing**: Jest, Vitest, pytest, cargo test
- **Integration Testing**: Supertest, pytest, integration frameworks
- **E2E Testing**: Playwright, Cypress, Selenium
- **Mocking**: MSW, unittest.mock, mockall

---

TESTING_EOF
```

### Step 6: Research Data Flow Patterns

```bash
cat >> "$OUTPUT_FILE" << 'DATAFLOW_EOF'

## Data Flow Patterns

<!-- Web search: "[stack] state management patterns" -->

### Frontend State Management

For complex applications, consider:

1. **Local State** - Component-level state for UI
2. **Server State** - Data from API (use React Query, SWR, etc.)
3. **Global State** - Shared across components (Context, Redux, Zustand)

### API Design Patterns

- **REST** - Resource-based, stateless, cacheable
- **GraphQL** - Query language, single endpoint, typed
- **tRPC** - End-to-end type safety for TypeScript

### Data Fetching Strategies

1. **Server-Side Rendering (SSR)** - SEO, initial load
2. **Static Site Generation (SSG)** - Performance, caching
3. **Client-Side Rendering (CSR)** - Interactivity
4. **Incremental Static Regeneration (ISR)** - Best of both

---

DATAFLOW_EOF
```

### Step 7: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Implementation Checklist

Before implementing, verify:

- [ ] Architecture aligns with team expertise
- [ ] Project structure supports scalability
- [ ] Testing strategy covers critical paths
- [ ] Data flow patterns match requirements
- [ ] Security considerations addressed

## Sources

Patterns compiled from:
- Official framework documentation
- Community best practices
- Industry standards (Clean Architecture, DDD)
- Real-world project examples

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Stack patterns saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Patterns should be practical, not theoretical
- Should adapt recommendations to detected stack
- Must provide concrete examples where possible
- Should highlight trade-offs for different approaches

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}patterns,"
fi

# 4. Domain Research (comprehensive only)
if [ "$DO_DOMAIN_RESEARCH" = "true" ]; then
    echo ""
    echo "ðŸŽ¯ Researching domain-specific knowledge..."
    # Workflow: Research Domain

## Purpose

Research domain-specific patterns, compliance requirements, and industry best practices based on the detected project type.

## Inputs

- `PROJECT_TYPE` - web_app, cli, api, library, monorepo
- `DETECTED_FRAMEWORK` - For context
- `SECURITY_LEVEL` - Informs compliance research depth

## Outputs

- `agent-os/config/enriched-knowledge/domain-knowledge.md`

---

## Web Search Queries

### Query Templates

```
1. "[project_type] software architecture patterns"
2. "[project_type] industry best practices"
3. "[project_type] compliance requirements"
4. "[project_type] common challenges"
5. "[project_type] scalability patterns"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/domain-knowledge.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Domain Knowledge

> Industry-specific patterns and considerations
> Generated: $(date -Iseconds)

**Project Type:** ${PROJECT_TYPE:-unknown}

---

HEADER_EOF
```

### Step 2: Research Based on Project Type

```bash
case "${PROJECT_TYPE:-unknown}" in
    "web_app")
        cat >> "$OUTPUT_FILE" << 'WEBAPP_EOF'

## Web Application Patterns

<!-- Web search: "web application architecture patterns [year]" -->

### Common Architectures

1. **Single Page Application (SPA)**
   - Rich client-side interactivity
   - API-driven data fetching
   - Client-side routing
   
2. **Server-Side Rendered (SSR)**
   - Better SEO
   - Faster initial load
   - Server load considerations

3. **Hybrid (SSR + SPA)**
   - Best of both worlds
   - Complexity trade-off
   - Popular in modern frameworks

### User Experience Considerations

- **Performance**: First Contentful Paint < 1.8s
- **Accessibility**: WCAG 2.1 compliance
- **Responsive Design**: Mobile-first approach
- **Progressive Enhancement**: Works without JS

### Security Requirements

- **Authentication**: Secure login flows
- **Authorization**: Role-based access control
- **Data Protection**: HTTPS, CSP headers
- **Input Validation**: Client and server-side

---

WEBAPP_EOF
        ;;
        
    "api")
        cat >> "$OUTPUT_FILE" << 'API_EOF'

## API Service Patterns

<!-- Web search: "API design best practices [year]" -->

### API Design Principles

1. **RESTful Design**
   - Resource-oriented URLs
   - HTTP methods for operations
   - Stateless communication
   
2. **Versioning Strategy**
   - URL versioning: `/api/v1/`
   - Header versioning
   - Query parameter versioning

3. **Error Handling**
   - Consistent error format
   - Meaningful status codes
   - Detailed error messages (dev only)

### Performance Patterns

- **Pagination**: Cursor-based for large datasets
- **Caching**: ETags, Cache-Control headers
- **Rate Limiting**: Protect against abuse
- **Compression**: gzip/brotli responses

### Documentation

- OpenAPI/Swagger specification
- Interactive documentation
- Code examples
- Versioned docs

---

API_EOF
        ;;
        
    "cli")
        cat >> "$OUTPUT_FILE" << 'CLI_EOF'

## CLI Tool Patterns

<!-- Web search: "CLI application best practices" -->

### CLI Design Principles

1. **User Experience**
   - Clear help text (`--help`)
   - Intuitive command structure
   - Meaningful error messages
   - Progress indicators for long operations

2. **Command Structure**
   ```
   tool <command> [subcommand] [options] [arguments]
   ```

3. **Configuration**
   - Config file support
   - Environment variables
   - Command-line flags (highest priority)

### Best Practices

- Follow POSIX conventions
- Support piping and redirection
- Provide quiet (`-q`) and verbose (`-v`) modes
- Exit with appropriate codes
- Support both short (`-h`) and long (`--help`) flags

### Distribution

- Single binary if possible
- Package manager support (npm, cargo, brew)
- Auto-update mechanism
- Cross-platform builds

---

CLI_EOF
        ;;
        
    "library")
        cat >> "$OUTPUT_FILE" << 'LIB_EOF'

## Library Design Patterns

<!-- Web search: "library design best practices" -->

### API Design

1. **Minimal Surface Area**
   - Expose only what's needed
   - Internal vs. public APIs
   - Deprecation strategy

2. **Consistency**
   - Naming conventions
   - Error handling patterns
   - Return value patterns

3. **Extensibility**
   - Plugin/middleware support
   - Configuration options
   - Hooks for customization

### Documentation

- Comprehensive README
- API documentation
- Usage examples
- Migration guides
- Changelog

### Versioning

- Semantic versioning (SemVer)
- Clear breaking change policy
- Deprecation warnings
- LTS versions for stability

---

LIB_EOF
        ;;
        
    *)
        cat >> "$OUTPUT_FILE" << 'DEFAULT_EOF'

## General Software Patterns

### Universal Best Practices

1. **Code Quality**
   - Consistent formatting
   - Meaningful naming
   - Documentation
   - Testing

2. **Architecture**
   - Separation of concerns
   - Dependency management
   - Configuration management
   - Error handling

3. **Operations**
   - Logging and monitoring
   - Health checks
   - Graceful shutdown
   - Configuration via environment

---

DEFAULT_EOF
        ;;
esac
```

### Step 3: Research Compliance Requirements

```bash
if [ "$SECURITY_LEVEL" = "high" ]; then
    cat >> "$OUTPUT_FILE" << 'COMPLIANCE_EOF'

## Compliance Considerations

<!-- Web search: "software compliance requirements [year]" -->

### Common Compliance Frameworks

| Framework | Focus | Key Requirements |
|-----------|-------|------------------|
| **GDPR** | Data Privacy (EU) | Consent, data rights, breach notification |
| **SOC 2** | Security Controls | Access control, encryption, monitoring |
| **HIPAA** | Healthcare Data | PHI protection, access logs, encryption |
| **PCI-DSS** | Payment Data | Cardholder data protection, network security |

### General Compliance Checklist

- [ ] Data encryption at rest and in transit
- [ ] Access control and authentication
- [ ] Audit logging
- [ ] Data retention policies
- [ ] Incident response plan
- [ ] Regular security assessments
- [ ] Privacy policy and terms of service

### Security Controls

1. **Authentication**
   - Multi-factor authentication
   - Session management
   - Password policies

2. **Authorization**
   - Principle of least privilege
   - Role-based access control
   - Resource-level permissions

3. **Data Protection**
   - Encryption standards (AES-256)
   - Key management
   - Secure deletion

---

COMPLIANCE_EOF
fi
```

### Step 4: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Implementation Priority

Based on your project type, prioritize:

1. **Core Functionality** - Get the basics right first
2. **Security** - Build security in from the start
3. **Performance** - Optimize critical paths
4. **Scalability** - Design for growth
5. **Maintainability** - Think long-term

## Sources

Domain knowledge compiled from:
- Industry standards and frameworks
- Community best practices
- Regulatory requirements
- Real-world case studies

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Domain knowledge saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Research should be specific to detected project type
- Compliance info should match security level
- Should provide actionable recommendations
- Must include implementation priorities

    RESEARCH_COMPLETED="${RESEARCH_COMPLETED}domain,"
fi

# 5. Version Analysis (always)
echo ""
echo "ðŸ“¦ Analyzing dependency versions..."
# Workflow: Version Analysis

## Purpose

Compare detected dependency versions against latest stable versions, flag outdated dependencies, and note breaking changes in newer versions.

## Inputs

- `package.json` (Node.js)
- `Cargo.toml` (Rust)
- `go.mod` (Go)
- `requirements.txt` / `pyproject.toml` (Python)

## Outputs

- `agent-os/config/enriched-knowledge/version-analysis.md`

---

## Web Search Queries

### Query Templates

```
1. "[package] latest version"
2. "[package] changelog breaking changes"
3. "[package] migration guide v[old] to v[new]"
```

---

## Workflow

### Step 1: Initialize Output File

```bash
CURRENT_YEAR=$(date +%Y)
OUTPUT_FILE="agent-os/config/enriched-knowledge/version-analysis.md"

cat > "$OUTPUT_FILE" << HEADER_EOF
# Version Analysis

> Dependency version status and update recommendations
> Generated: $(date -Iseconds)

This analysis compares your current dependency versions against the latest
stable releases and flags potential updates.

---

## Summary

HEADER_EOF
```

### Step 2: Analyze Node.js Dependencies

```bash
if [ -f "package.json" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Node.js Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`npm outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Package | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|---------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Extract key dependencies (simplified - in practice, use npm outdated)
    # This is a template showing the expected output format
    
    # Check for common frameworks and their typical update status
    if grep -q '"react"' package.json 2>/dev/null; then
        REACT_VER=$(grep '"react"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        if [ "${REACT_VER%%.*}" -lt "18" ] 2>/dev/null; then
            echo "| react | ${REACT_VER:-unknown} | âš ï¸ OUTDATED | Consider upgrading to React 18 |" >> "$OUTPUT_FILE"
        else
            echo "| react | ${REACT_VER:-unknown} | âœ… Current | |" >> "$OUTPUT_FILE"
        fi
    fi
    
    if grep -q '"next"' package.json 2>/dev/null; then
        NEXT_VER=$(grep '"next"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| next | ${NEXT_VER:-unknown} | â„¹ï¸ Check | Major versions may have breaking changes |" >> "$OUTPUT_FILE"
    fi
    
    if grep -q '"typescript"' package.json 2>/dev/null; then
        TS_VER=$(grep '"typescript"' package.json | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| typescript | ${TS_VER:-unknown} | â„¹ï¸ Check | TypeScript 5.x has new features |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check all outdated packages" >> "$OUTPUT_FILE"
    echo "npm outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all packages (minor/patch)" >> "$OUTPUT_FILE"
    echo "npm update" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Interactive update (recommended)" >> "$OUTPUT_FILE"
    echo "npx npm-check-updates -i" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 3: Analyze Rust Dependencies

```bash
if [ -f "Cargo.toml" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Rust Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`cargo outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Crate | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|-------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check for common crates
    if grep -q 'tokio' Cargo.toml 2>/dev/null; then
        TOKIO_VER=$(grep 'tokio' Cargo.toml | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| tokio | ${TOKIO_VER:-unknown} | â„¹ï¸ Check | Async runtime |" >> "$OUTPUT_FILE"
    fi
    
    if grep -q 'serde' Cargo.toml 2>/dev/null; then
        SERDE_VER=$(grep 'serde' Cargo.toml | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| serde | ${SERDE_VER:-unknown} | â„¹ï¸ Check | Serialization |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Install cargo-outdated" >> "$OUTPUT_FILE"
    echo "cargo install cargo-outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Check outdated dependencies" >> "$OUTPUT_FILE"
    echo "cargo outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update dependencies" >> "$OUTPUT_FILE"
    echo "cargo update" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 4: Analyze Python Dependencies

```bash
if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Python Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`pip list --outdated\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Package | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|---------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check for common packages
    PYDEPS=""
    [ -f "requirements.txt" ] && PYDEPS=$(cat requirements.txt)
    
    if echo "$PYDEPS" | grep -qi 'django'; then
        DJANGO_VER=$(echo "$PYDEPS" | grep -i 'django' | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| django | ${DJANGO_VER:-unknown} | â„¹ï¸ Check | Web framework |" >> "$OUTPUT_FILE"
    fi
    
    if echo "$PYDEPS" | grep -qi 'fastapi'; then
        FASTAPI_VER=$(echo "$PYDEPS" | grep -i 'fastapi' | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "| fastapi | ${FASTAPI_VER:-unknown} | â„¹ï¸ Check | API framework |" >> "$OUTPUT_FILE"
    fi
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check outdated packages" >> "$OUTPUT_FILE"
    echo "pip list --outdated" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all packages" >> "$OUTPUT_FILE"
    echo "pip install --upgrade -r requirements.txt" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Use pip-tools for better dependency management" >> "$OUTPUT_FILE"
    echo "pip install pip-tools" >> "$OUTPUT_FILE"
    echo "pip-compile --upgrade" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 5: Analyze Go Dependencies

```bash
if [ -f "go.mod" ]; then
    echo "" >> "$OUTPUT_FILE"
    echo "## Go Dependencies" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "<!-- Run \`go list -m -u all\` for real-time version check -->" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "| Module | Current | Status | Notes |" >> "$OUTPUT_FILE"
    echo "|--------|---------|--------|-------|" >> "$OUTPUT_FILE"
    
    # Check Go version
    GO_VER=$(grep "^go " go.mod | awk '{print $2}')
    echo "| go (runtime) | ${GO_VER:-unknown} | â„¹ï¸ Check | Go version |" >> "$OUTPUT_FILE"
    
    echo "" >> "$OUTPUT_FILE"
    echo "### Recommended Actions" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "\`\`\`bash" >> "$OUTPUT_FILE"
    echo "# Check for available updates" >> "$OUTPUT_FILE"
    echo "go list -m -u all" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
    echo "# Update all dependencies" >> "$OUTPUT_FILE"
    echo "go get -u ./..." >> "$OUTPUT_FILE"
    echo "go mod tidy" >> "$OUTPUT_FILE"
    echo "\`\`\`" >> "$OUTPUT_FILE"
    echo "" >> "$OUTPUT_FILE"
fi
```

### Step 6: Add Update Strategy

```bash
cat >> "$OUTPUT_FILE" << 'STRATEGY_EOF'

---

## Update Strategy

### Version Status Legend

| Status | Meaning | Action |
|--------|---------|--------|
| âœ… Current | Latest stable version | No action needed |
| â„¹ï¸ Check | Unknown/needs verification | Run version check tool |
| âš ï¸ OUTDATED | Behind latest stable | Plan update |
| ðŸ”´ CRITICAL | Security patch available | Update immediately |

### Safe Update Process

1. **Check current versions**
   ```bash
   # Use appropriate tool for your language
   npm outdated / cargo outdated / pip list --outdated
   ```

2. **Review changelogs**
   - Check for breaking changes
   - Review migration guides
   - Note deprecated features

3. **Update in stages**
   - Patch versions first (x.x.PATCH)
   - Then minor versions (x.MINOR.x)
   - Major versions last (MAJOR.x.x)

4. **Test thoroughly**
   - Run test suite after each update
   - Check critical paths manually
   - Monitor for regressions

### When to Update

| Situation | Recommended Action |
|-----------|-------------------|
| Security vulnerability | Update immediately |
| Major version behind | Plan migration sprint |
| Minor version behind | Update during maintenance |
| Patch version behind | Update with next release |

---

STRATEGY_EOF
```

### Step 7: Add Footer

```bash
cat >> "$OUTPUT_FILE" << 'FOOTER_EOF'

## Automated Version Checking

For real-time, accurate version information, use these tools:

| Language | Tool | Command |
|----------|------|---------|
| Node.js | npm | `npm outdated` |
| Node.js | npx | `npx npm-check-updates` |
| Rust | cargo | `cargo outdated` |
| Python | pip | `pip list --outdated` |
| Go | go | `go list -m -u all` |

### CI Integration

Consider adding version checking to your CI pipeline:

```yaml
# Example GitHub Action
- name: Check for outdated dependencies
  run: npm outdated || true  # Don't fail build
```

---

*Generated by Geist Adaptive Questionnaire System*
*Run the appropriate version check tool for accurate, real-time information*

FOOTER_EOF

echo "   âœ“ Version analysis saved to $OUTPUT_FILE"
```

---

## Important Constraints

- Must recommend using real-time tools for accuracy
- Should not make definitive version claims without verification
- Must provide safe update strategy
- Should distinguish between critical and non-critical updates
- Must include commands for each language ecosystem

RESEARCH_COMPLETED="${RESEARCH_COMPLETED}versions,"
```

### Step 4: Synthesize Knowledge

```bash
echo ""
echo "ðŸ”„ Synthesizing research findings..."
# Workflow: Synthesize Knowledge

## Purpose

Combine all research outputs, remove duplicates, prioritize actionable insights, and create a unified summary for easy consumption.

## Inputs

Research files from:
- `agent-os/config/enriched-knowledge/library-research.md`
- `agent-os/config/enriched-knowledge/stack-best-practices.md`
- `agent-os/config/enriched-knowledge/domain-knowledge.md`
- `agent-os/config/enriched-knowledge/version-analysis.md`
- `agent-os/config/enriched-knowledge/security-notes.md`

## Outputs

- Updates to individual research files (deduplication)
- `agent-os/config/enriched-knowledge/README.md` - Summary index

---

## Workflow

### Step 1: Create Summary Index

```bash
KNOWLEDGE_DIR="agent-os/config/enriched-knowledge"
SUMMARY_FILE="$KNOWLEDGE_DIR/README.md"

cat > "$SUMMARY_FILE" << HEADER_EOF
# Enriched Knowledge Index

> Consolidated research findings for your project
> Generated: $(date -Iseconds)

This directory contains research gathered during the adaptive questionnaire
process. Use this knowledge to inform your development decisions.

---

## Available Research

HEADER_EOF
```

### Step 2: Index Available Files

```bash
echo "### Research Files" >> "$SUMMARY_FILE"
echo "" >> "$SUMMARY_FILE"

# Check each expected file and add to index
if [ -f "$KNOWLEDGE_DIR/library-research.md" ]; then
    echo "- ðŸ“š [Library Research](library-research.md) - Best practices for your dependencies" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/stack-best-practices.md" ]; then
    echo "- ðŸ—ï¸ [Stack Best Practices](stack-best-practices.md) - Architecture patterns for your tech stack" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/domain-knowledge.md" ]; then
    echo "- ðŸŽ¯ [Domain Knowledge](domain-knowledge.md) - Industry-specific patterns" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/version-analysis.md" ]; then
    echo "- ðŸ“¦ [Version Analysis](version-analysis.md) - Dependency version status" >> "$SUMMARY_FILE"
fi

if [ -f "$KNOWLEDGE_DIR/security-notes.md" ]; then
    echo "- ðŸ”’ [Security Notes](security-notes.md) - Security considerations and CVEs" >> "$SUMMARY_FILE"
fi

echo "" >> "$SUMMARY_FILE"
```

### Step 3: Extract Key Insights

```bash
cat >> "$SUMMARY_FILE" << 'INSIGHTS_HEADER'

---

## Key Insights Summary

### ðŸ”´ Critical Items

Items requiring immediate attention:

INSIGHTS_HEADER

# Check for critical security issues
if [ -f "$KNOWLEDGE_DIR/security-notes.md" ]; then
    if grep -q "CRITICAL\|ðŸ”´" "$KNOWLEDGE_DIR/security-notes.md" 2>/dev/null; then
        echo "- âš ï¸ Critical security issues found - see [Security Notes](security-notes.md)" >> "$SUMMARY_FILE"
    else
        echo "- âœ… No critical security issues detected" >> "$SUMMARY_FILE"
    fi
fi

# Check for outdated dependencies
if [ -f "$KNOWLEDGE_DIR/version-analysis.md" ]; then
    if grep -q "OUTDATED\|Major update" "$KNOWLEDGE_DIR/version-analysis.md" 2>/dev/null; then
        echo "- âš ï¸ Outdated dependencies found - see [Version Analysis](version-analysis.md)" >> "$SUMMARY_FILE"
    else
        echo "- âœ… Dependencies are up to date" >> "$SUMMARY_FILE"
    fi
fi

echo "" >> "$SUMMARY_FILE"
```

### Step 4: Add Quick Reference

```bash
cat >> "$SUMMARY_FILE" << 'QUICKREF_EOF'

### ðŸ“‹ Quick Reference

| Area | Document | When to Use |
|------|----------|-------------|
| Starting a new feature | Stack Best Practices | Architecture decisions |
| Adding dependencies | Library Research | Evaluate libraries |
| Security review | Security Notes | Before deployment |
| Updating packages | Version Analysis | Maintenance cycles |
| Domain questions | Domain Knowledge | Business logic |

---

QUICKREF_EOF
```

### Step 5: Add Usage Instructions

```bash
cat >> "$SUMMARY_FILE" << 'USAGE_EOF'

## How to Use This Knowledge

### During Development

1. **Before implementing a feature**: Check Stack Best Practices for patterns
2. **When choosing libraries**: Review Library Research for recommendations
3. **During code review**: Reference Security Notes for secure coding
4. **When debugging**: Check Library Research for known issues

### During Maintenance

1. **Regular updates**: Use Version Analysis to prioritize updates
2. **Security patches**: Follow Security Notes recommendations
3. **Refactoring**: Reference Stack Best Practices for patterns

### During Planning

1. **Architecture decisions**: Stack Best Practices + Domain Knowledge
2. **Compliance requirements**: Domain Knowledge + Security Notes
3. **Technology choices**: Library Research + Version Analysis

---

USAGE_EOF
```

### Step 6: Add Refresh Instructions

```bash
cat >> "$SUMMARY_FILE" << 'REFRESH_EOF'

## Keeping Knowledge Fresh

This research was generated at a point in time. To refresh:

1. **Re-run detection**: This will update research with latest information
2. **Manual update**: Edit files directly for project-specific notes
3. **Research depth**: Adjust depth for more/less detail

### Research Depth Levels

| Level | Time | Best For |
|-------|------|----------|
| `minimal` | ~30s | Quick updates |
| `standard` | ~2m | Regular use |
| `comprehensive` | ~5m | Major decisions |

To change depth:
```bash
RESEARCH_DEPTH=comprehensive
# Then re-run adapt-to-product or the research orchestrator
```

---

## Files in This Directory

REFRESH_EOF

# List all files with sizes
ls -lh "$KNOWLEDGE_DIR"/*.md 2>/dev/null | awk '{print "- `" $NF "` (" $5 ")"}' >> "$SUMMARY_FILE" || echo "- No files found" >> "$SUMMARY_FILE"

cat >> "$SUMMARY_FILE" << 'FOOTER_EOF'

---

*Generated by Geist Adaptive Questionnaire System*

FOOTER_EOF

echo "   âœ“ Knowledge synthesis complete"
echo "   âœ“ Summary index saved to $SUMMARY_FILE"
```

---

## Deduplication Logic

When synthesizing, the workflow identifies and consolidates:

1. **Repeated recommendations** across files
2. **Conflicting advice** (flags for human review)
3. **Version-specific information** (keeps most recent)
4. **Source attribution** (maintains for verification)

---

## Important Constraints

- Must maintain source attribution
- Should flag conflicting recommendations
- Must create navigable index
- Should highlight critical items prominently
- Must provide clear usage guidance

```

### Step 5: Generate Research Summary

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "  RESEARCH COMPLETE"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ðŸ“ Enriched knowledge saved to: agent-os/config/enriched-knowledge/"
echo ""
echo "Files generated:"
ls -la agent-os/config/enriched-knowledge/ 2>/dev/null | grep ".md" | awk '{print "   â€¢ " $NF}'
echo ""
echo "Research areas completed: $(echo $RESEARCH_COMPLETED | sed 's/,$//' | tr ',' ', ')"
echo ""
```

---

## Integration

This workflow is called by:
- `adapt-to-product/1-setup-and-information-gathering.md` (after detection)
- `create-basepoints/1-validate-prerequisites.md` (for architecture research)

The enriched knowledge is used by:
- `deploy-agents` for specialization
- Validation workflows for security checks
- Human review for flagging issues

---

## Research Depth Guidelines

| Depth | Time | Use Case |
|-------|------|----------|
| `minimal` | ~30s | Quick setup, simple projects |
| `standard` | ~2m | Most projects (default) |
| `comprehensive` | ~5m | Enterprise, complex projects |

---

## Important Constraints

- Must handle web search failures gracefully
- Should cache results to avoid redundant searches
- Must attribute sources in output
- Should prioritize actionable insights over raw data

```

This will:
- Research best practices for detected libraries
- Gather architecture patterns for your tech stack
- Check for security vulnerabilities and updates
- Store findings in `agent-os/config/enriched-knowledge/`

### 0.3: Present Findings and Confirm

Display detected configuration and ask for confirmation:

```bash
# Workflow: Present and Confirm

## Purpose

Format detected values for display, present a confirmation prompt to the user, handle user overrides, and output the final confirmed profile.

## Inputs

Expects these variables to be set (from prior detection workflows):
- `DETECTED_PROJECT_TYPE`
- `DETECTED_LANGUAGE`
- `DETECTED_FRAMEWORK`
- `DETECTED_DATABASE`
- `DETECTED_BUILD_CMD`
- `DETECTED_TEST_CMD`
- `DETECTED_LINT_CMD`
- `DETECTED_SECURITY_LEVEL`
- `DETECTED_FILE_COUNT`
- `DETECTED_LINE_COUNT`
- `DETECTION_CONFIDENCE`

## Outputs

- Updates `agent-os/config/project-profile.yml` with confirmed values
- Sets `USER_CONFIRMED=true` when user accepts

---

## Workflow

### Step 1: Display Detected Configuration

```bash
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "                      DETECTED PROJECT CONFIGURATION                         "
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Project Profile Section
echo "ðŸ“¦ PROJECT PROFILE"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Type:" "${DETECTED_PROJECT_TYPE:-unknown} âœ“"
printf "   %-20s %s\n" "Size:" "${DETECTED_FILE_COUNT:-?} files, ~${DETECTED_LINE_COUNT:-?} lines âœ“"
printf "   %-20s %s\n" "Maturity:" "$([ -d '.git' ] && echo 'Version controlled' || echo 'Unknown') âœ“"
echo ""

# Tech Stack Section
echo "ðŸ”§ TECH STACK"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Language:" "${DETECTED_LANGUAGE:-unknown} âœ“"
[ -n "$DETECTED_FRAMEWORK" ] && printf "   %-20s %s\n" "Framework:" "$DETECTED_FRAMEWORK âœ“"
[ -n "$DETECTED_BACKEND" ] && printf "   %-20s %s\n" "Backend:" "$DETECTED_BACKEND âœ“"
[ -n "$DETECTED_DATABASE" ] && printf "   %-20s %s\n" "Database:" "$DETECTED_DATABASE âœ“"
echo ""

# Commands Section
echo "âš™ï¸  DETECTED COMMANDS"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Build:" "${DETECTED_BUILD_CMD:-(not detected)} âœ“"
printf "   %-20s %s\n" "Test:" "${DETECTED_TEST_CMD:-(not detected)} âœ“"
printf "   %-20s %s\n" "Lint:" "${DETECTED_LINT_CMD:-(not detected)} âœ“"
echo ""

# Inferred Settings Section
echo "ðŸ”’ INFERRED SETTINGS"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
printf "   %-20s %s\n" "Security Level:" "${DETECTED_SECURITY_LEVEL:-moderate} âœ“"
printf "   %-20s %s\n" "Complexity:" "${INFERRED_COMPLEXITY:-moderate} âœ“"
echo ""

# Confidence
echo "ðŸ“Š DETECTION CONFIDENCE: ${DETECTION_CONFIDENCE:-0.80}"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

### Step 2: Present Minimal Questions

Only ask questions for things that cannot be detected:

```bash
echo ""
echo "âš ï¸  QUESTIONS REQUIRING YOUR INPUT"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "These items cannot be determined automatically from your codebase:"
echo ""

# Question 1: Compliance Requirements
echo "1. What compliance requirements apply to this project?"
echo ""
echo "   [ ] None (default)"
echo "   [ ] SOC 2"
echo "   [ ] HIPAA"
echo "   [ ] GDPR"
echo "   [ ] PCI-DSS"
echo "   [ ] Other"
echo ""
echo "   Enter your choice (e.g., 'none', 'gdpr', 'soc2,hipaa'): "

# In non-interactive mode or if user presses Enter, use default
USER_COMPLIANCE="${USER_COMPLIANCE:-none}"

echo ""

# Question 2: Human Review Level
echo "2. How much human oversight do you want for AI-generated changes?"
echo ""
echo "   [ ] minimal  - Trust AI, review only critical changes"
echo "   [ ] moderate - Review architectural decisions (default)"
echo "   [ ] high     - Review all significant changes"
echo ""
echo "   Enter your choice (minimal/moderate/high): "

# Default to moderate
USER_HUMAN_REVIEW="${USER_HUMAN_REVIEW:-moderate}"

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

### Step 3: Confirmation Prompt

```bash
echo ""
echo "ðŸ“‹ CONFIRMATION"
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "Press Enter to accept these detected values, or type a section name to modify:"
echo ""
echo "   â€¢ 'type'      - Change project type"
echo "   â€¢ 'language'  - Change language/framework"
echo "   â€¢ 'commands'  - Change build/test/lint commands"
echo "   â€¢ 'security'  - Change security level"
echo "   â€¢ 'all'       - Review all settings interactively"
echo ""
echo "Your choice (Enter to accept): "

# For non-interactive execution, assume acceptance
USER_CHOICE="${USER_CHOICE:-accept}"

if [ "$USER_CHOICE" = "" ] || [ "$USER_CHOICE" = "accept" ]; then
    echo ""
    echo "âœ… Configuration confirmed!"
    USER_CONFIRMED="true"
fi
```

### Step 4: Handle Overrides (if requested)

```bash
# Handle override requests
case "$USER_CHOICE" in
    "type")
        echo "Enter new project type (web_app, cli, api, library, monorepo):"
        read -r NEW_TYPE
        [ -n "$NEW_TYPE" ] && DETECTED_PROJECT_TYPE="$NEW_TYPE"
        ;;
    "language")
        echo "Enter primary language (typescript, javascript, rust, python, go):"
        read -r NEW_LANG
        [ -n "$NEW_LANG" ] && DETECTED_LANGUAGE="$NEW_LANG"
        echo "Enter framework (or leave empty):"
        read -r NEW_FRAMEWORK
        DETECTED_FRAMEWORK="$NEW_FRAMEWORK"
        ;;
    "commands")
        echo "Enter build command (or leave empty):"
        read -r NEW_BUILD
        DETECTED_BUILD_CMD="$NEW_BUILD"
        echo "Enter test command (or leave empty):"
        read -r NEW_TEST
        DETECTED_TEST_CMD="$NEW_TEST"
        echo "Enter lint command (or leave empty):"
        read -r NEW_LINT
        DETECTED_LINT_CMD="$NEW_LINT"
        ;;
    "security")
        echo "Enter security level (low, moderate, high):"
        read -r NEW_SECURITY
        [ -n "$NEW_SECURITY" ] && DETECTED_SECURITY_LEVEL="$NEW_SECURITY"
        ;;
esac
```

### Step 5: Update Profile with Confirmed Values

```bash
# Update the profile with user-specified values
cat > agent-os/config/project-profile.yml << CONFIRMED_EOF
# Project Profile
# Auto-detected and confirmed by user
# Generated: $(date -Iseconds)

gathered:
  project_type: ${DETECTED_PROJECT_TYPE:-unknown}
  tech_stack:
    language: ${DETECTED_LANGUAGE:-unknown}
    framework: ${DETECTED_FRAMEWORK:-}
    backend: ${DETECTED_BACKEND:-}
    database: ${DETECTED_DATABASE:-}
  size:
    lines: ${DETECTED_LINE_COUNT:-0}
    files: ${DETECTED_FILE_COUNT:-0}
    modules: ${DETECTED_MODULE_COUNT:-0}
  commands:
    build: "${DETECTED_BUILD_CMD:-}"
    test: "${DETECTED_TEST_CMD:-}"
    lint: "${DETECTED_LINT_CMD:-}"

inferred:
  security_level: ${DETECTED_SECURITY_LEVEL:-moderate}
  complexity: ${INFERRED_COMPLEXITY:-moderate}

user_specified:
  compliance:
$(echo "$USER_COMPLIANCE" | tr ',' '\n' | sed 's/^/    - /' | grep -v "^    - none$" || echo "    []")
  human_review_level: ${USER_HUMAN_REVIEW:-moderate}

_meta:
  detected_at: $(date -Iseconds)
  confirmed_at: $(date -Iseconds)
  detection_confidence: ${DETECTION_CONFIDENCE:-0.80}
  user_confirmed: ${USER_CONFIRMED:-true}
  questions_asked: 2
  questions_auto_answered: ${DETECTIONS_SUCCESS:-0}

CONFIRMED_EOF

echo ""
echo "âœ… Profile saved to: agent-os/config/project-profile.yml"
echo ""
```

---

## Output Summary

After confirmation, display:

```bash
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "                        CONFIGURATION COMPLETE                               "
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "Your project profile has been saved and will be used for:"
echo ""
echo "  â€¢ Basepoint generation (create-basepoints)"
echo "  â€¢ Agent specialization (deploy-agents)"
echo "  â€¢ Validation command configuration"
echo "  â€¢ Workflow complexity selection"
echo ""
echo "You can modify the profile at any time by editing:"
echo "  agent-os/config/project-profile.yml"
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
```

---

## Important Constraints

- Default to accepting detected values (user presses Enter)
- Maximum 2-3 questions that can't be auto-detected
- Provide sensible defaults for all questions
- Allow overrides but don't require them
- Save confirmed profile for use by subsequent commands

```

### 0.4: Ask Minimal Questions

Only ask questions that cannot be detected from the codebase:

```bash
# Workflow: Question Templates

## Purpose

Provide minimal question templates for information that cannot be automatically detected from the codebase. Maximum 2-3 questions per command.

---

## Questions That MUST Be Asked

These questions gather information that cannot be detected from code:

### Question 1: Compliance Requirements

```markdown
**1. Compliance Requirements** (can't detect from code)

What compliance standards apply to this project?

   [ ] None (default)
   [ ] SOC 2 - Security/availability controls
   [ ] HIPAA - Healthcare data protection
   [ ] GDPR - EU data privacy
   [ ] PCI-DSS - Payment card data
   [ ] Other (specify)

Enter your choice (e.g., 'none', 'gdpr', 'soc2,hipaa'):
```

**Default:** `none`

### Question 2: Human Review Preference

```markdown
**2. Human Review Preference**

How much human oversight do you want for AI-generated changes?

   [ ] minimal  - Trust AI, review only critical changes
   [ ] moderate - Review architectural decisions (default)
   [ ] high     - Review all significant changes

Enter your choice (minimal/moderate/high):
```

**Default:** `moderate`

---

## Questions Asked Only If Detection Fails

These are only asked when automatic detection confidence is below 70%:

### Question 3: Project Type (if ambiguous)

```markdown
**3. Project Type** (detection was uncertain)

I couldn't confidently determine the project type. Is this a:

   [ ] web_app  - Web application
   [ ] api      - API/Backend service
   [ ] cli      - Command-line tool
   [ ] library  - Reusable library
   [ ] monorepo - Multiple packages
   [ ] other    - Something else

Enter your choice:
```

### Question 4: Module Boundaries (if unclear)

```markdown
**4. Module Boundaries** (structure was unclear)

How should I identify module boundaries for basepoint generation?

   [ ] directory  - Each top-level directory is a module
   [ ] package    - Each package.json/Cargo.toml is a module
   [ ] manual     - Let me specify the modules

Enter your choice:
```

---

## Implementation

### Bash Script for Questions

```bash
# Function to ask compliance question
ask_compliance() {
    echo ""
    echo "1. What compliance requirements apply to this project?"
    echo ""
    echo "   [ ] None (default)"
    echo "   [ ] SOC 2"
    echo "   [ ] HIPAA"
    echo "   [ ] GDPR"
    echo "   [ ] PCI-DSS"
    echo "   [ ] Other"
    echo ""
    echo "   Enter your choice (e.g., 'none', 'gdpr', 'soc2,hipaa'):"
    
    # In non-interactive mode, use default
    USER_COMPLIANCE="${USER_COMPLIANCE:-none}"
    echo "   â†’ Using: $USER_COMPLIANCE"
}

# Function to ask human review preference
ask_human_review() {
    echo ""
    echo "2. How much human oversight do you want for AI changes?"
    echo ""
    echo "   [ ] minimal  - Trust AI, review only critical"
    echo "   [ ] moderate - Review architectural decisions (default)"
    echo "   [ ] high     - Review all significant changes"
    echo ""
    echo "   Enter your choice (minimal/moderate/high):"
    
    # In non-interactive mode, use default
    USER_HUMAN_REVIEW="${USER_HUMAN_REVIEW:-moderate}"
    echo "   â†’ Using: $USER_HUMAN_REVIEW"
}

# Function to ask project type (only if detection failed)
ask_project_type() {
    if [ "$NEEDS_USER_INPUT_TYPE" = "true" ]; then
        echo ""
        echo "3. I couldn't determine the project type. Is this a:"
        echo ""
        echo "   [ ] web_app  - Web application"
        echo "   [ ] api      - API/Backend service"
        echo "   [ ] cli      - Command-line tool"
        echo "   [ ] library  - Reusable library"
        echo "   [ ] monorepo - Multiple packages"
        echo ""
        echo "   Enter your choice:"
        
        # In non-interactive mode, use unknown
        USER_PROJECT_TYPE="${USER_PROJECT_TYPE:-unknown}"
        DETECTED_PROJECT_TYPE="$USER_PROJECT_TYPE"
    fi
}

# Main question flow
ask_minimal_questions() {
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "  QUESTIONS REQUIRING YOUR INPUT"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "These items cannot be determined from your codebase:"
    
    ask_compliance
    ask_human_review
    
    # Only ask additional questions if detection was uncertain
    ask_project_type
    
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Press Enter to accept these values, or re-run with"
    echo "environment variables to override:"
    echo ""
    echo "   USER_COMPLIANCE=gdpr USER_HUMAN_REVIEW=high /adapt-to-product"
    echo ""
}

# Execute questions
ask_minimal_questions
```

---

## Storing Answers

After questions are answered, update the project profile:

```bash
# Update project profile with user answers
update_profile_with_answers() {
    PROFILE_FILE="agent-os/config/project-profile.yml"
    
    if [ -f "$PROFILE_FILE" ]; then
        # Update user_specified section
        # Note: In practice, use yq or proper YAML parsing
        
        echo ""
        echo "Updating profile with your preferences..."
        echo "   Compliance: $USER_COMPLIANCE"
        echo "   Human Review: $USER_HUMAN_REVIEW"
        
        # The profile should already exist from detection
        # This just updates the user_specified section
    fi
}
```

---

## Question Count by Command

| Command | Questions Asked | Conditional Questions |
|---------|-----------------|----------------------|
| `/adapt-to-product` | 2 | +1 if type detection failed |
| `/create-basepoints` | 0-1 | Only if modules unclear |
| `/deploy-agents` | 0-2 | Only if not set in prior command |

**Total across all commands:** Maximum 3-5 questions (vs 20+ traditional approach)

---

## Important Constraints

- Maximum 2-3 questions per command
- All questions have sensible defaults
- User can press Enter to accept defaults
- Questions are skipped if already answered in prior command
- Additional questions only asked when detection confidence is low

```

Questions asked (maximum 2-3):
1. **Compliance requirements** - GDPR, HIPAA, SOC 2, etc. (can't detect from code)
2. **Human review preference** - How much AI oversight do you want?

The profile is saved to `agent-os/config/project-profile.yml` for use by subsequent commands.

---

## Step 1: Gather Product Information

The FIRST STEP is to set up folders and gather comprehensive product information from multiple sources by following these instructions:

# Gather Product Information from Existing Codebase

Collect comprehensive product information from multiple sources for adapting an existing codebase into product documentation.

## Step 1: Create Inheritance Folder

Create the inheritance folder for user-provided documents:

```bash
# Create inheritance folder
mkdir -p agent-os/product/inheritance

# Check if inheritance folder already exists
if [ -d "agent-os/product/inheritance" ]; then
    if [ "$(ls -A agent-os/product/inheritance 2>/dev/null)" ]; then
        echo "Inheritance folder already contains files. Using existing files or add new ones?"
        # List existing files in inheritance folder
        ls -la agent-os/product/inheritance/
    else
        echo "Inheritance folder created. Please place any product-related documents here."
    fi
fi
```

## Step 2: Check Product Folder

Check if product folder already exists and handle accordingly:

```bash
# Check if product folder already exists
if [ -d "agent-os/product" ]; then
    echo "Product documentation already exists. Review existing files or start fresh?"
    # List existing product files
    ls -la agent-os/product/
fi
```

## Step 3: Gather Information from Multiple Sources

Collect comprehensive product information from the following sources:

### A. User Input - Interactive User Prompts

Prompt user interactively for product information following the same patterns as plan-product (reference: `agent-os/commands/plan-product/1-product-concept.md`):

1. **Initial Information Gathering:**
   - Start with product name and core concept (if not already known from codebase)
   - Ask about product purpose and main value proposition
   - Gather key features (minimum 3 if not inferrable from codebase)
   - Ask about target users and use cases (at least 1 user segment)

2. **Extended Information Gathering:**
   - Prompt for web pages: "Do you have any product web pages or documentation URLs to include?"
   - Prompt for public resources: "Any public resources, documentation sites, or references?"
   - Prompt for private documents: "Do you have private documents or files describing the product?"
   - Prompt for links: "Please provide any relevant links (documentation, marketing pages, technical specs, etc.)"
   - Prompt for @command syntax: "You can use @command syntax to describe specific aspects. For example: @command + 'describe the product mission' or @command + 'outline the product roadmap'"

3. **Follow-up Questions:**
   - Ask whatever questions are needed to create the most accurate product files
   - If information is missing or unclear, prompt for clarification
   - Follow similar user interaction patterns from plan-product but adapted for existing codebase context

If any critical information is missing, prompt user:
```
Please provide the following to create your product plan:
1. Product name and core concept (if not clear from codebase)
2. Any web pages, documents, or links related to the product
3. Additional information using @command syntax if needed (e.g., @command + "describe the product mission")
4. Any specific details about target users, features, or differentiators
```

### B. Inheritance Folder Document Processing

Read and process files from `agent-os/product/inheritance/`:

```bash
# Check for files in inheritance folder
if [ -d "agent-os/product/inheritance" ] && [ "$(ls -A agent-os/product/inheritance 2>/dev/null)" ]; then
    echo "Processing files from inheritance folder..."
    # List all files
    find agent-os/product/inheritance -type f | while read file; do
        echo "Processing: $file"
        # Extract content based on file type
    done
fi
```

Process files from inheritance folder:

1. **File Type Detection and Reading:**
   - **Markdown files**: .md, .markdown - Read and extract text content, preserve structure
   - **Text files**: .txt, .text - Read plain text content
   - **JSON files**: .json - Parse and extract structured data, look for product-related fields
   - **YAML files**: .yml, .yaml - Parse and extract structured data
   - **HTML files**: .html, .htm - Extract text content, strip HTML tags
   - **PDF files**: .pdf - Extract text content if possible (may require external tools)
   - **Other formats**: Attempt to read as text if supported

2. **Information Extraction:**
   - Extract product description and purpose
   - Identify features, capabilities, and use cases
   - Extract user information and personas
   - Identify goals, objectives, and differentiators
   - Extract tech stack mentions (if present)
   - Extract roadmap or feature plans (if present)

3. **Merge into Product Knowledge:**
   - Combine extracted information with other sources
   - Tag information with source (inheritance folder)
   - Preserve file references for traceability
   - Handle duplicate information gracefully

### C. Automatic Link/Web Page Scraping

For any links provided by the user, automatically scrape and analyze web page content:

```bash
# For each link provided by user:
# Use web scraping capabilities to fetch and parse content
# Note: Actual implementation may use tools like curl, wget, or web scraping libraries
```

Process links provided by user:

1. **Link Validation:**
   - Verify URL format is valid
   - Check if link is accessible (HTTP/HTTPS)
   - Handle different URL formats (with/without protocol, fragments, etc.)

2. **Automatic Content Scraping:**
   - Fetch web page content automatically (do not ask user, just scrape)
   - Extract main content from HTML pages
   - Strip HTML tags and formatting
   - Preserve important structure (headings, lists, etc.) where possible
   - Extract text content for analysis

3. **Information Extraction:**
   - Extract product description and purpose from scraped content
   - Identify features and capabilities mentioned
   - Extract user information and target audience
   - Identify product goals and value propositions
   - Extract technical information if present

4. **Error Handling:**
   - Handle unreachable links gracefully (log error, continue with other sources)
   - Handle parsing failures (invalid HTML, encoding issues, etc.)
   - Handle timeouts and network errors
   - Continue processing other links even if one fails
   - Inform user if critical links cannot be accessed (but do not block the process)

### D. @command Syntax Support

If user provides `@command + "prompt"` syntax, parse and execute:

```bash
# Example: @command + "describe the product mission"
# Parse the command reference and prompt text
# Execute the referenced command with the prompt
# Capture output for product knowledge
```

Implement @command syntax support:

1. **Parsing @command References:**
   - Detect `@command` or `@<command-name>` patterns in user input
   - Extract command name (default to generic command if not specified)
   - Extract prompt text after `+` or following quotation marks
   - Handle variations: `@command + "prompt"`, `@mycommand "prompt"`, `@command prompt`

2. **Command Execution:**
   - Execute the referenced command with the provided prompt
   - Pass the prompt as input to the command
   - Capture all command output (stdout and stderr)
   - Handle command execution errors gracefully

3. **Output Capture:**
   - Capture command output text
   - Parse structured output if applicable (JSON, YAML, etc.)
   - Extract product-relevant information from command output

4. **Supported Use Cases:**
   - `@command + "describe the product mission"` - Generate mission description
   - `@command + "outline the product roadmap"` - Generate roadmap outline
   - `@command + "list the tech stack"` - Generate tech stack information
   - `@command + "describe key features"` - Generate feature descriptions
   - Support any command that can provide product-related information

5. **Integration:**
   - Merge command output into product knowledge
   - Tag information with source (command output)
   - Treat command output as high-priority information source

### E. Optional Web Search Integration

If user explicitly requests web search:

1. **Web Search Request Handling:**
   - Check if user explicitly requested web search (do NOT execute automatically)
   - Prompt user for search terms or topics if not provided
   - Confirm search query before executing
   - Example: "Would you like me to search the web for additional product information? If yes, what should I search for?"

2. **Perform Web Search from Public Sources:**
   - Use available web search capabilities to search for product-related information
   - Search public sources (documentation sites, forums, articles, etc.)
   - Focus search on product information, features, competitors, market research
   - Execute search only when explicitly requested by user

3. **Process Search Results:**
   - Extract relevant information from search results
   - Identify product-relevant content from search results
   - Extract features, descriptions, use cases, or other relevant information
   - Filter out irrelevant or low-quality results

4. **Integrate Search Results:**
   - Add web search results to unified product knowledge base
   - Tag information with source (web search)
   - Integrate findings into product knowledge synthesis
   - Treat web search results as lower priority than other sources

5. **Error Handling:**
   - Handle web search errors gracefully (network failures, API errors, etc.)
   - Handle cases where no relevant results are found
   - Continue with other information sources even if web search fails
   - Inform user if web search fails but do not block the process

## Step 4: Merge All Information Sources

After gathering information from all sources, merge everything into a unified product knowledge base:

### Information Merging Logic

1. **Collect Information from All Sources:**
   - User input (interactive prompts)
   - Inheritance folder documents (from `agent-os/product/inheritance/`)
   - Scraped web content (from links provided)
   - Command outputs (from @command syntax)
   - Web search results (if user requested web search)

2. **Resolve Conflicts Between Information Sources:**
   - When same information appears from multiple sources with different values:
     * Prioritize user-provided information (highest priority)
     * Use inheritance folder documents if user input is unclear
     * Use scraped content if no user/inheritance information
     * Use codebase findings (from Phase 2) as supporting information
     * Use web search results as additional context (lowest priority)
   - Merge complementary information (combine details from different sources)
   - Identify gaps where information is missing from all sources

3. **Prioritization Order (Highest to Lowest):**
   1. User-provided information (explicit user input)
   2. @command outputs (user-requested command execution)
   3. Inheritance folder documents (user-provided files)
   4. Scraped web content (user-provided links)
   5. Codebase analysis findings (Phase 2)
   6. Web search results (if requested)

4. **Create Unified Product Knowledge Base:**
   - Combine all information into structured knowledge base
   - Organize by categories: mission, users, features, tech stack, etc.
   - Tag each piece of information with its source for traceability
   - Preserve important details from all sources
   - Remove duplicates while keeping the highest priority version
   - Prepare unified knowledge for use in subsequent phases (mission, roadmap, tech-stack generation)


## Display confirmation and next step

Once you've completed setup and information gathering, output the following message:

```
I have gathered product information from all available sources.

ðŸ“Š Project Profile: Saved to agent-os/config/project-profile.yml
ðŸ“š Enriched Knowledge: Saved to agent-os/config/enriched-knowledge/

NEXT STEP ðŸ‘‰ Run the command, `2-analyze-codebase.md`
```

## User Standards & Preferences Compliance

When gathering product information, use the user's standards and preferences for context and baseline assumptions, as documented in these files:

@agent-os/standards/global/codebase-analysis.md
@agent-os/standards/global/coding-style.md
@agent-os/standards/global/commenting.md
@agent-os/standards/global/conventions.md
@agent-os/standards/global/enriched-knowledge-templates.md
@agent-os/standards/global/error-handling.md
@agent-os/standards/global/project-profile-schema.md
@agent-os/standards/global/tech-stack.md
@agent-os/standards/global/validation-commands.md
@agent-os/standards/global/validation.md
